# Presentations

A collection of presentations on various ML subjects/papers.
Mostly for internal lab meetings.

- [HMMs (Bootcamp)](<22-09-07 HMMs (Bootcamp)>)
    - An introductory presentation on Hidden Markov Models and the forward-backward and Baum-Welch algorithms.
    - _Presentation for [Compass CDT](https://www.bristol.ac.uk/cdt/compass/) first-year Bootcamp module_
- [Audio Diffusion](<23-05-02 Audio Diffusion>)
    - Diffusion-based neural vocoders for high-quality audio generation, with a focus on [WaveGrad](https://arxiv.org/abs/2009.00713) and [DiffWave](https://arxiv.org/abs/2009.09761).
    - _Lab meeting presentation_
- [Adaptive MCMC (mini)](<23-06-15 Adaptive MCMC (mini)>)
    - Introduction to Adaptive Markov Chain Monte Carlo methods for efficient sampling.
    - _Internal presentation_
- [AMP-IS Exploration](<23-08-02 AMP-IS Exploration>)
    - Discusses the AMP-IS algorithm (later renamed QEM) for efficient massively-parallel Bayesian inference with [alan](https://github.com/alan-ppl/alan).
    - _Internal presentation_
- [VOD (Variational Open-Domain Q&A)](<23-08-08 VOD (Variational Open-Domain Q&A)>)
    - Presents the paper [Variational Open-Domain Question Answering](https://arxiv.org/pdf/2210.06345) which uses RÃ©nyi divergence variational inference for RAG in open-domain question answering.
- [ML vs RWS](<23-08-09 ML vs RWS>)
    - Compares machine learning techniques with reward-weighted sampling.
- [SBI](<23-11-07 SBI>)
    - Covers Simulation-Based Inference and its applications in machine learning.
- [Sequential Bayes for Continual Learning](<24-02-20 Sequential Bayes for Continual Learning>)
    - Discusses the use of sequential Bayesian methods in continual learning scenarios.
- [KANs (Kolmogorov-Arnold Networks)](<24-05-28 KANs (Kolmogorov-Arnold Networks)>)
    - Presents KANs and their application in solving complex functional equations.
- [MP-VI](<24-07-05 MP-VI>)
    - Introduces MP-VI, a novel variational inference method.
- [Transformer Neural Processes](<24-07-30 Transformer Neural Processes>)
    - Explores the integration of transformer architectures in neural process models.
- [Why Linearised Laplace is Better](<24-10-08 Why Linearised Laplace is Better>)
    - Discusses the advantages of using linearised Laplace approximation in machine learning models.

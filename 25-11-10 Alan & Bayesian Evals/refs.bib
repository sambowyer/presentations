
@article{chatterjeeSampleSizeRequired2018,
	title = {The sample size required in importance sampling},
	volume = {28},
	issn = {1050-5164},
	url = {https://projecteuclid.org/journals/annals-of-applied-probability/volume-28/issue-2/The-sample-size-required-in-importance-sampling/10.1214/17-AAP1326.full},
	doi = {10.1214/17-AAP1326},
	language = {en},
	number = {2},
	urldate = {2023-02-03},
	journal = {The Annals of Applied Probability},
	author = {Chatterjee, Sourav and Diaconis, Persi},
	month = apr,
	year = {2018},
	file = {Chatterjee and Diaconis - 2018 - The sample size required in importance sampling.pdf:/home/dg22309/Zotero/storage/PPZTY3I3/Chatterjee and Diaconis - 2018 - The sample size required in importance sampling.pdf:application/pdf},
}

@misc{millerAddingErrorBars2024,
	title = {Adding {Error} {Bars} to {Evals}: {A} {Statistical} {Approach} to {Language} {Model} {Evaluations}},
	shorttitle = {Adding {Error} {Bars} to {Evals}},
	url = {http://arxiv.org/abs/2411.00640},
	doi = {10.48550/arXiv.2411.00640},
	abstract = {Evaluations are critical for understanding the capabilities of large language models (LLMs). Fundamentally, evaluations are experiments; but the literature on evaluations has largely ignored the literature from other sciences on experiment analysis and planning. This article shows researchers with some training in statistics how to think about and analyze data from language model evaluations. Conceptualizing evaluation questions as having been drawn from an unseen super-population, we present formulas for analyzing evaluation data, measuring differences between two models, and planning an evaluation experiment. We make a number of specific recommendations for running language model evaluations and reporting experiment results in a way that minimizes statistical noise and maximizes informativeness.},
	language = {en},
	urldate = {2024-12-17},
	publisher = {arXiv},
	author = {Miller, Evan},
	month = nov,
	year = {2024},
	note = {arXiv:2411.00640 [stat]},
	keywords = {Computer Science - Computation and Language, Statistics - Applications},
	file = {Miller - 2024 - Adding Error Bars to Evals A Statistical Approach.pdf:/home/dg22309/Zotero/storage/NFT3MERJ/Miller - 2024 - Adding Error Bars to Evals A Statistical Approach.pdf:application/pdf},
}

@inproceedings{bowyerPositionDontUse2025a,
	title = {Position: {Don}’t {Use} the {CLT} in {LLM} {Evals} {With} {Fewer} {Than} a {Few} {Hundred} {Datapoints}},
	shorttitle = {Position},
	url = {https://proceedings.mlr.press/v267/bowyer25a.html},
	abstract = {Rigorous statistical evaluations of large language models (LLMs), including valid error bars and significance testing, are essential for meaningful and reliable performance assessment. Currently, when such statistical measures are reported, they typically rely on the Central Limit Theorem (CLT). In this position paper, we argue that while CLT-based methods for uncertainty quantification are appropriate when benchmarks consist of thousands of examples, they fail to provide adequate uncertainty estimates for LLM evaluations that rely on smaller, highly specialized benchmarks. In these small-data settings, we demonstrate that CLT-based methods perform very poorly, usually dramatically underestimating uncertainty (i.e. producing error bars that are too small). We give recommendations for alternative frequentist and Bayesian methods that are both easy to implement and more appropriate in these increasingly common scenarios.},
	language = {en},
	urldate = {2025-11-09},
	booktitle = {Proceedings of the 42nd {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Bowyer, Sam and Aitchison, Laurence and Ivanova, Desi R.},
	month = oct,
	year = {2025},
	note = {ISSN: 2640-3498},
	pages = {81143--81184},
	file = {Full Text PDF:/home/dg22309/Zotero/storage/32REFA97/Bowyer et al. - 2025 - Position Don’t Use the CLT in LLM Evals With Fewer Than a Few Hundred Datapoints.pdf:application/pdf},
}

@inproceedings{heapMassivelyParallelExpectation2025b,
	title = {Massively {Parallel} {Expectation} {Maximization} {For} {Approximate} {Posteriors}},
	url = {https://proceedings.mlr.press/v289/heap25a.html},
	abstract = {Bayesian inference for hierarchical models can be very challenging. MCMC methods have difficulty scaling to large models with many observations and latent variables. While variational inference (VI) and reweighted wake-sleep (RWS) can be more scalable, they are gradient-based methods and so often require many iterations to converge. Our key insight was that modern massively parallel importance weighting methods (Bowyer et al., 2024) give fast and accurate posterior moment estimates, and we can use these moment estimates to rapidly learn an approximate posterior. Specifically, we propose using expectation maximization to fit the approximate posterior, which we call QEM. The expectation step involves computing the posterior moments using high-quality massively parallel estimates from Bowyer et al. (2024). The maximization step involves fitting the approximate posterior using these moments, which can be done straightforwardly for simple approximate posteriors such as Gaussian, Gamma, Beta, Dirichlet, Binomial, Multinomial, Categorical, etc. (or combinations thereof). We show that QEM is faster than state-of-the-art, massively parallel variants of RWS and VI, and is invariant to reparameterizations of the model that dramatically slow down gradient based methods.},
	language = {en},
	urldate = {2025-11-09},
	booktitle = {Proceedings of the 7th {Symposium} on {Advances} in {Approximate} {Bayesian} {Inference}},
	publisher = {PMLR},
	author = {Heap, Thomas and Bowyer, Sam and Aitchison, Laurence},
	month = jul,
	year = {2025},
	note = {ISSN: 2640-3498},
	pages = {25--66},
	file = {Full Text PDF:/home/dg22309/Zotero/storage/N6GEJV9C/Heap et al. - 2025 - Massively Parallel Expectation Maximization For Approximate Posteriors.pdf:application/pdf},
}

@inproceedings{bowyerUsingAutodiffEstimate2024c,
	title = {Using {Autodiff} to {Estimate} {Posterior} {Moments}, {Marginals} and {Samples}},
	url = {https://proceedings.mlr.press/v244/bowyer24a.html},
	abstract = {Importance sampling is a popular technique in Bayesian inference: by reweighting samples drawn from a proposal distribution we are able to obtain samples and moment estimates from a Bayesian posterior over latent variables. Recent work, however, indicates that importance sampling scales poorly — in order to accurately approximate the true posterior, the required number of importance samples grows is exponential in the number of latent variables [Chatterjee and Diaconis, 2018]. Massively parallel importance sampling works around this issue by drawing KKK samples for each of the nnn latent variables and reasoning about all KnKnK{\textasciicircum}n combinations of latent samples. In principle, we can reason efficiently over KnKnK{\textasciicircum}n combinations of samples by exploiting conditional independencies in the generative model. However, in practice this requires complex algorithms that traverse backwards through the graphical model, and we need separate backward traversals for each computation (posterior expectations, marginals and samples). Our contribution is to exploit the source term trick from physics to entirely avoid the need to hand-write backward traversals. Instead, we demonstrate how to simply and easily compute all the required quantities — posterior expectations, marginals and samples — by differentiating through a slightly modified marginal likelihood estimator.},
	language = {en},
	urldate = {2025-11-09},
	booktitle = {Proceedings of the {Fortieth} {Conference} on {Uncertainty} in {Artificial} {Intelligence}},
	publisher = {PMLR},
	author = {Bowyer, Sam and Heap, Thomas and Aitchison, Laurence},
	month = sep,
	year = {2024},
	note = {ISSN: 2640-3498},
	pages = {394--417},
	file = {Full Text PDF:/home/dg22309/Zotero/storage/WXBW5T6W/Bowyer et al. - 2024 - Using Autodiff to Estimate Posterior Moments, Marginals and Samples.pdf:application/pdf},
}

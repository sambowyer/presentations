
@article{brehmer_mining_2020,
	title = {Mining gold from implicit models to improve likelihood-free inference},
	volume = {117},
	issn = {0027-8424, 1091-6490},
	url = {http://arxiv.org/abs/1805.12244},
	doi = {10.1073/pnas.1915980117},
	abstract = {Simulators often provide the best description of real-world phenomena. However, they also lead to challenging inverse problems because the density they implicitly define is often intractable. We present a new suite of simulation-based inference techniques that go beyond the traditional Approximate Bayesian Computation approach, which struggles in a high-dimensional setting, and extend methods that use surrogate models based on neural networks. We show that additional information, such as the joint likelihood ratio and the joint score, can often be extracted from simulators and used to augment the training data for these surrogate models. Finally, we demonstrate that these new techniques are more sample efficient and provide higher-fidelity inference than traditional methods.},
	number = {10},
	urldate = {2023-11-06},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Brehmer, Johann and Louppe, Gilles and Pavez, Juan and Cranmer, Kyle},
	month = mar,
	year = {2020},
	note = {arXiv:1805.12244 [hep-ph, physics:physics, stat]},
	keywords = {Computer Science - Machine Learning, High Energy Physics - Phenomenology, Physics - Data Analysis, Statistics and Probability, Statistics - Machine Learning},
	pages = {5242--5249},
}

@article{atlas_collaboration_observation_2012,
	title = {Observation of a new particle in the search for the {Standard} {Model} {Higgs} boson with the {ATLAS} detector at the {LHC}},
	volume = {716},
	issn = {03702693},
	url = {http://arxiv.org/abs/1207.7214},
	doi = {10.1016/j.physletb.2012.08.020},
	abstract = {A search for the Standard Model Higgs boson in proton-proton collisions with the ATLAS detector at the LHC is presented. The datasets used correspond to integrated luminosities of approximately 4.8 fb{\textasciicircum}-1 collected at sqrt(s) = 7 TeV in 2011 and 5.8 fb{\textasciicircum}-1 at sqrt(s) = 8 TeV in 2012. Individual searches in the channels H-{\textgreater}ZZ{\textasciicircum}(*)-{\textgreater}llll, H-{\textgreater}gamma gamma and H-{\textgreater}WW-{\textgreater}e nu mu nu in the 8 TeV data are combined with previously published results of searches for H-{\textgreater}ZZ{\textasciicircum}(*), WW{\textasciicircum}(*), bbbar and tau{\textasciicircum}+tau{\textasciicircum}- in the 7 TeV data and results from improved analyses of the H-{\textgreater}ZZ{\textasciicircum}(*)-{\textgreater}llll and H-{\textgreater}gamma gamma channels in the 7 TeV data. Clear evidence for the production of a neutral boson with a measured mass of 126.0 +/- 0.4(stat) +/- 0.4(sys) GeV is presented. This observation, which has a significance of 5.9 standard deviations, corresponding to a background fluctuation probability of 1.7x10{\textasciicircum}-9, is compatible with the production and decay of the Standard Model Higgs boson.},
	number = {1},
	urldate = {2023-11-05},
	journal = {Physics Letters B},
	author = {{ATLAS Collaboration}},
	month = sep,
	year = {2012},
	note = {arXiv:1207.7214 [hep-ex]},
	keywords = {High Energy Physics - Experiment},
	pages = {1--29},
}

@misc{germain_made_2015,
	title = {{MADE}: {Masked} {Autoencoder} for {Distribution} {Estimation}},
	shorttitle = {{MADE}},
	url = {http://arxiv.org/abs/1502.03509},
	doi = {10.48550/arXiv.1502.03509},
	abstract = {There has been a lot of recent interest in designing neural network models to estimate a distribution from a set of examples. We introduce a simple modification for autoencoder neural networks that yields powerful generative models. Our method masks the autoencoder's parameters to respect autoregressive constraints: each input is reconstructed only from previous inputs in a given ordering. Constrained this way, the autoencoder outputs can be interpreted as a set of conditional probabilities, and their product, the full joint probability. We can also train a single network that can decompose the joint probability in multiple different orderings. Our simple framework can be applied to multiple architectures, including deep ones. Vectorized implementations, such as on GPUs, are simple and fast. Experiments demonstrate that this approach is competitive with state-of-the-art tractable distribution estimators. At test time, the method is significantly faster and scales better than other autoregressive estimators.},
	urldate = {2023-11-06},
	publisher = {arXiv},
	author = {Germain, Mathieu and Gregor, Karol and Murray, Iain and Larochelle, Hugo},
	month = jun,
	year = {2015},
	note = {arXiv:1502.03509 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@misc{papamakarios_masked_2018,
	title = {Masked {Autoregressive} {Flow} for {Density} {Estimation}},
	url = {http://arxiv.org/abs/1705.07057},
	doi = {10.48550/arXiv.1705.07057},
	abstract = {Autoregressive models are among the best performing neural density estimators. We describe an approach for increasing the flexibility of an autoregressive model, based on modelling the random numbers that the model uses internally when generating data. By constructing a stack of autoregressive models, each modelling the random numbers of the next model in the stack, we obtain a type of normalizing flow suitable for density estimation, which we call Masked Autoregressive Flow. This type of flow is closely related to Inverse Autoregressive Flow and is a generalization of Real NVP. Masked Autoregressive Flow achieves state-of-the-art performance in a range of general-purpose density estimation tasks.},
	urldate = {2023-11-06},
	publisher = {arXiv},
	author = {Papamakarios, George and Pavlakou, Theo and Murray, Iain},
	month = jun,
	year = {2018},
	note = {arXiv:1705.07057 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{cern_geant4_nodate,
	title = {Geant4},
	url = {https://geant4.web.cern.ch//},
	abstract = {Toolkit for the simulation of the passage of particles through matter. Its areas of application include high energy, nuclear and accelerator physics, as well as studies in medical and space science.},
	language = {en},
	urldate = {2023-11-06},
	journal = {Geant4},
	author = {CERN},
}

@misc{brehmer_slides2022simulation_based_inference_rodem_sinergia_2022pdf_2022,
	title = {slides/2022/simulation\_based\_inference\_rodem\_sinergia\_2022.pdf at master Â· johannbrehmer/slides},
	url = {https://github.com/johannbrehmer/slides/blob/master/2022/simulation_based_inference_rodem_sinergia_2022.pdf},
	abstract = {Collection of slides. Contribute to johannbrehmer/slides development by creating an account on GitHub.},
	language = {en},
	urldate = {2023-11-06},
	author = {Brehmer, Johann},
	month = jul,
	year = {2022},
}

@misc{papamakarios_fast_2018,
	title = {Fast \${\textbackslash}epsilon\$-free {Inference} of {Simulation} {Models} with {Bayesian} {Conditional} {Density} {Estimation}},
	url = {http://arxiv.org/abs/1605.06376},
	doi = {10.48550/arXiv.1605.06376},
	abstract = {Many statistical models can be simulated forwards but have intractable likelihoods. Approximate Bayesian Computation (ABC) methods are used to infer properties of these models from data. Traditionally these methods approximate the posterior over parameters by conditioning on data being inside an \${\textbackslash}epsilon\$-ball around the observed data, which is only correct in the limit \${\textbackslash}epsilon{\textbackslash}!{\textbackslash}rightarrow{\textbackslash}!0\$. Monte Carlo methods can then draw samples from the approximate posterior to approximate predictions or error bars on parameters. These algorithms critically slow down as \${\textbackslash}epsilon{\textbackslash}!{\textbackslash}rightarrow{\textbackslash}!0\$, and in practice draw samples from a broader distribution than the posterior. We propose a new approach to likelihood-free inference based on Bayesian conditional density estimation. Preliminary inferences based on limited simulation data are used to guide later simulations. In some cases, learning an accurate parametric representation of the entire true posterior distribution requires fewer model simulations than Monte Carlo ABC methods need to produce a single sample from an approximate posterior.},
	urldate = {2023-11-06},
	publisher = {arXiv},
	author = {Papamakarios, George and Murray, Iain},
	month = apr,
	year = {2018},
	note = {arXiv:1605.06376 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Computation, Statistics - Machine Learning},
}

@misc{lueckmann_flexible_2017,
	title = {Flexible statistical inference for mechanistic models of neural dynamics},
	url = {http://arxiv.org/abs/1711.01861},
	doi = {10.48550/arXiv.1711.01861},
	abstract = {Mechanistic models of single-neuron dynamics have been extensively studied in computational neuroscience. However, identifying which models can quantitatively reproduce empirically measured data has been challenging. We propose to overcome this limitation by using likelihood-free inference approaches (also known as Approximate Bayesian Computation, ABC) to perform full Bayesian inference on single-neuron models. Our approach builds on recent advances in ABC by learning a neural network which maps features of the observed data to the posterior distribution over parameters. We learn a Bayesian mixture-density network approximating the posterior over multiple rounds of adaptively chosen simulations. Furthermore, we propose an efficient approach for handling missing features and parameter settings for which the simulator fails, as well as a strategy for automatically learning relevant features using recurrent neural networks. On synthetic data, our approach efficiently estimates posterior distributions and recovers ground-truth parameters. On in-vitro recordings of membrane voltages, we recover multivariate posteriors over biophysical parameters, which yield model-predicted voltage traces that accurately match empirical data. Our approach will enable neuroscientists to perform Bayesian inference on complex neuron models without having to design model-specific algorithms, closing the gap between mechanistic and statistical approaches to single-neuron modelling.},
	urldate = {2023-11-06},
	publisher = {arXiv},
	author = {Lueckmann, Jan-Matthis and Goncalves, Pedro J. and Bassetto, Giacomo and Ãcal, Kaan and Nonnenmacher, Marcel and Macke, Jakob H.},
	month = nov,
	year = {2017},
	note = {arXiv:1711.01861 [stat]},
	keywords = {Statistics - Machine Learning},
}

@misc{cranmer_approximating_2016,
	title = {Approximating {Likelihood} {Ratios} with {Calibrated} {Discriminative} {Classifiers}},
	url = {http://arxiv.org/abs/1506.02169},
	doi = {10.48550/arXiv.1506.02169},
	abstract = {In many fields of science, generalized likelihood ratio tests are established tools for statistical inference. At the same time, it has become increasingly common that a simulator (or generative model) is used to describe complex processes that tie parameters \${\textbackslash}theta\$ of an underlying theory and measurement apparatus to high-dimensional observations \${\textbackslash}mathbf\{x\}{\textbackslash}in {\textbackslash}mathbb\{R\}{\textasciicircum}p\$. However, simulator often do not provide a way to evaluate the likelihood function for a given observation \${\textbackslash}mathbf\{x\}\$, which motivates a new class of likelihood-free inference algorithms. In this paper, we show that likelihood ratios are invariant under a specific class of dimensionality reduction maps \${\textbackslash}mathbb\{R\}{\textasciicircum}p {\textbackslash}mapsto {\textbackslash}mathbb\{R\}\$. As a direct consequence, we show that discriminative classifiers can be used to approximate the generalized likelihood ratio statistic when only a generative model for the data is available. This leads to a new machine learning-based approach to likelihood-free inference that is complementary to Approximate Bayesian Computation, and which does not require a prior on the model parameters. Experimental results on artificial problems with known exact likelihoods illustrate the potential of the proposed method.},
	urldate = {2023-11-06},
	publisher = {arXiv},
	author = {Cranmer, Kyle and Pavez, Juan and Louppe, Gilles},
	month = mar,
	year = {2016},
	note = {arXiv:1506.02169 [physics, stat]},
	keywords = {62P35, 62F99, 62H30, Physics - Data Analysis, Statistics and Probability, Statistics - Applications, Statistics - Machine Learning},
}

@article{lueckmann_benchmarking_2021,
	title = {Benchmarking {Simulation}-{Based} {Inference}},
	url = {http://proceedings.mlr.press/v130/lueckmann21a/lueckmann21a.pdf},
	abstract = {Recent advances in probabilistic modelling have led to a large number of simulation-based inference algorithms which do not require numerical evaluation of likelihoods. However, a public benchmark with appropriate performance metrics for such âlikelihood-freeâ algorithms has been lacking. This has made it diï¬cult to compare algorithms and identify their strengths and weaknesses. We set out to ï¬ll this gap: We provide a benchmark with inference tasks and suitable performance metrics, with an initial selection of algorithms including recent approaches employing neural networks and classical Approximate Bayesian Computation methods. We found that the choice of performance metric is critical, that even state-of-the-art algorithms have substantial room for improvement, and that sequential estimation improves sample eï¬ciency. Neural network-based approaches generally exhibit better performance, but there is no uniformly best algorithm. We provide practical advice and highlight the potential of the benchmark to diagnose problems and improve algorithms. The results can be explored interactively on a companion website. All code is open source, making it possible to contribute further benchmark tasks and inference algorithms.},
	language = {en},
	author = {Lueckmann, Jan-Matthis and Boelts, Jan and Greenberg, David S and GonÃ§alves, Pedro J and Macke, Jakob H},
	year = {2021},
}

@article{durkan_contrastive_2020,
	title = {On {Contrastive} {Learning} for {Likelihood}-free {Inference}},
	url = {https://arxiv.org/pdf/2002.03712.pdf},
	abstract = {Likelihood-free methods perform parameter inference in stochastic simulator models where evaluating the likelihood is intractable but sampling synthetic data is possible. One class of methods for this likelihood-free problem uses a classiï¬er to distinguish between pairs of parameterobservation samples generated using the simulator and pairs sampled from some reference distribution, which implicitly learns a density ratio proportional to the likelihood. Another popular class of methods ï¬ts a conditional distribution to the parameter posterior directly, and a particular recent variant allows for the use of ï¬exible neural density estimators for this task. In this work, we show that both of these approaches can be uniï¬ed under a general contrastive learning scheme, and clarify how they should be run and compared.},
	language = {en},
	author = {Durkan, Conor and Murray, Iain and Papamakarios, George},
	year = {2020},
}

@article{hermans_likelihood-free_2020,
	title = {Likelihood-free {MCMC} with {Amortized} {Approximate} {Ratio} {Estimators}},
	volume = {37},
	url = {http://proceedings.mlr.press/v119/hermans20a/hermans20a.pdf},
	language = {en},
	journal = {ICML},
	author = {Hermans, Joeri and Begy, Volodimir and Louppe, Gilles},
	year = {2020},
}

@article{noauthor_notitle_nodate,
}

@article{neyman_ix_1933,
	title = {{IX}. {On} the problem of the most efficient tests of statistical hypotheses},
	volume = {231},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsta.1933.0009},
	doi = {10.1098/rsta.1933.0009},
	abstract = {The problem of testing statistical hypotheses is an old one. Its origin is usually connected with the name of Thomas Bayes, who gave the well-known theorem on the probabilities a posteriori of the possible âcauses" of a given event. Since then it has been discussed by many writers of whom we shall here mention two only, Bertrand and Borel, whose differing views serve well to illustrate the point from which we shall approach the subject. Bertrand put into statistical form a variety of hypotheses, as for example the hypothesis that a given group of stars with relatively small angular distances between them as seen from the earth, form a âsystemâ or group in space. His method of attack, which is that in common use, consisted essentially in calculating the probability, P, that a certain character, x, of the observed facts would arise if the hypothesis tested were true. If P were very small, this would generally be considered as an indication that the hypothesis, H, was probably false, and vice versa. Bertrand expressed the pessimistic view that no test of this kind could give reliable results. Borel, however, in a later discussion, considered that the method described could be applied with success provided that the character, x, of the observed facts were properly chosenâwere, in fact, a character which he terms âen quelque sorte remarquable.â},
	number = {694-706},
	urldate = {2023-11-05},
	journal = {Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character},
	author = {Neyman, Jerzy and Pearson, Egon Sharpe and Pearson, Karl},
	month = feb,
	year = {1933},
	note = {Publisher: Royal Society},
	pages = {289--337},
}

@article{marjoram_markov_2003,
	title = {Markov chain {Monte} {Carlo} without likelihoods},
	volume = {100},
	issn = {0027-8424},
	doi = {10.1073/pnas.0306899100},
	abstract = {Many stochastic simulation approaches for generating observations from a posterior distribution depend on knowing a likelihood function. However, for many complex probability models, such likelihoods are either impossible or computationally prohibitive to obtain. Here we present a Markov chain Monte Carlo method for generating observations from a posterior distribution without the use of likelihoods. It can also be used in frequentist applications, in particular for maximum-likelihood estimation. The approach is illustrated by an example of ancestral inference in population genetics. A number of open problems are highlighted in the discussion.},
	language = {eng},
	number = {26},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Marjoram, Paul and Molitor, John and Plagnol, Vincent and Tavare, Simon},
	month = dec,
	year = {2003},
	pmid = {14663152},
	pmcid = {PMC307566},
	keywords = {Algorithms, Biological Evolution, Computer Simulation, DNA, DNA, Mitochondrial, Genetics, Population, Humans, Likelihood Functions, Markov Chains, Models, Biological, Monte Carlo Method, Stochastic Processes},
	pages = {15324--15328},
}

@article{diggle_monte_1984,
	title = {Monte {Carlo} {Methods} of {Inference} for {Implicit} {Statistical} {Models}},
	volume = {46},
	issn = {0035-9246, 2517-6161},
	url = {https://rss.onlinelibrary.wiley.com/doi/10.1111/j.2517-6161.1984.tb01290.x},
	doi = {10.1111/j.2517-6161.1984.tb01290.x},
	abstract = {SUMMARY
            
              A prescribed statistical model is a parametric specification of the distribution of a random vector, whilst an implicit statistical model is one defined at a more fundamental level in terms of a generating stochastic mechanism. This paper develops methods of inference which can be used for implicit statistical models whose distribution theory is intractable. The kernel method of probability density estimation is advocated for estimating a logâlikelihood from simulations of such a model. The development and testing of an algorithm for maximizing this estimated logâlikelihood function is described. An illustrative example involving a stochastic model for quantal response assays is given. Possible applications of the maximization algorithm to
              ad hoc
              methods of parameter estimation are noted briefly, and illustrated by an example involving a model for the spatial pattern of displaced amacrine cells in the retina of a rabbit.},
	language = {en},
	number = {2},
	urldate = {2023-11-05},
	journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
	author = {Diggle, Peter J. and Gratton, Richard J.},
	month = jan,
	year = {1984},
	pages = {193--212},
}

@misc{louppe_adversarial_2020,
	title = {Adversarial {Variational} {Optimization} of {Non}-{Differentiable} {Simulators}},
	url = {http://arxiv.org/abs/1707.07113},
	doi = {10.48550/arXiv.1707.07113},
	abstract = {Complex computer simulators are increasingly used across fields of science as generative models tying parameters of an underlying theory to experimental observations. Inference in this setup is often difficult, as simulators rarely admit a tractable density or likelihood function. We introduce Adversarial Variational Optimization (AVO), a likelihood-free inference algorithm for fitting a non-differentiable generative model incorporating ideas from generative adversarial networks, variational optimization and empirical Bayes. We adapt the training procedure of generative adversarial networks by replacing the differentiable generative network with a domain-specific simulator. We solve the resulting non-differentiable minimax problem by minimizing variational upper bounds of the two adversarial objectives. Effectively, the procedure results in learning a proposal distribution over simulator parameters, such that the JS divergence between the marginal distribution of the synthetic data and the empirical distribution of observed data is minimized. We evaluate and compare the method with simulators producing both discrete and continuous data.},
	urldate = {2023-11-05},
	publisher = {arXiv},
	author = {Louppe, Gilles and Hermans, Joeri and Cranmer, Kyle},
	month = apr,
	year = {2020},
	note = {arXiv:1707.07113 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{rubin_bayesianly_1984,
	title = {Bayesianly {Justifiable} and {Relevant} {Frequency} {Calculations} for the {Applied} {Statistician}},
	volume = {12},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-12/issue-4/Bayesianly-Justifiable-and-Relevant-Frequency-Calculations-for-the-Applied-Statistician/10.1214/aos/1176346785.full},
	doi = {10.1214/aos/1176346785},
	abstract = {A common reaction among applied statisticians is that the Bayesian statistician's energies in an applied problem must be directed at the a priori elicitation of one model specification from which an optimal design and all inferences follow automatically by applying Bayes's theorem to calculate conditional distributions of unknowns given knowns. I feel, however, that the applied Bayesian statistician's tool-kit should be more extensive and include tools that may be usefully labeled frequency calculations. Three types of Bayesianly justifiable and relevant frequency calculations are presented using examples to convey their use for the applied statistician.},
	number = {4},
	urldate = {2023-11-05},
	journal = {The Annals of Statistics},
	author = {Rubin, Donald B.},
	month = dec,
	year = {1984},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {62-07, 62A15, 62F15, 62L10, 62P99, Calibration, Empirical Bayes, Stopping rules, inference, model monitoring, operating characteristics, posterior predictive checks},
	pages = {1151--1172},
}

@misc{noauthor_orbi_nodate,
	title = {{ORBi}: {Detailled} {Reference}},
	url = {https://orbi.uliege.be/handle/2268/265148},
	urldate = {2023-11-05},
}

@techreport{hull_using_2022,
	type = {preprint},
	title = {Using simulation-based inference to determine the parameters of an integrated hydrologic model: a case study from the upper {Colorado} {River} basin},
	shorttitle = {Using simulation-based inference to determine the parameters of an integrated hydrologic model},
	url = {https://hess.copernicus.org/preprints/hess-2022-345/},
	abstract = {High-resolution, spatially distributed process-based models are a well-established tool to explore complex watershed processes and how they may evolve under a changing climate. While these models are powerful, calibrating them can be 20 difficult because they are costly to run and have many unknown parameters. To solve this problem, we need a state-of-the-art, data- driven approach to model calibration that can scale to the high-compute, high-dimensional hydrologic simulators that drive innovation in our field today. Simulation- Based Inference (SBI) uses deep learning methods to learn a probability distribution of simulation parameters by comparing simulator outputs to observed data. The inferred parameters can then be used to run calibrated model simulations. This approach has pushed boundaries in simulator-intensive research from 25 cosmology, particle physics, and neuroscience, but is less familiar to hydrology. The goal of this paper is to introduce SBI to the field of watershed modeling by benchmarking and exploring its performance in a set of synthetic experiments. We use SBI to infer two common physical parameters of hydrologic process-based models, Manningâs Coefficient and Hydraulic Conductivity, in a snowmelt-dominated catchment in Colorado, USA. We employ a process-based simulator (ParFlow), streamflow observations, and several deep learning components to confront two recalcitrant issues related to calibrating 30 watershed models: 1) the high cost of running enough simulations to do a calibration; 2) finding âcorrectâ parameters when our understanding of the system is uncertain or incomplete. In a series of experiments, we demonstrate the power of SBI to conduct rapid and precise parameter inference for model calibration. The workflow we present is general-purpose, and we discuss how this can be adapted to other hydrology-related problems.},
	language = {en},
	urldate = {2023-11-05},
	institution = {Groundwater hydrology/Modelling approaches},
	author = {Hull, Robert and Leonarduzzi, Elena and De La Fuente, Luis and Tran, Hoang Viet and Bennett, Andrew and Melchior, Peter and Maxwell, Reed M. and Condon, Laura E.},
	month = nov,
	year = {2022},
	doi = {10.5194/hess-2022-345},
}

@misc{brehmer_simulation-based_2020,
	title = {Simulation-based inference methods for particle physics},
	url = {http://arxiv.org/abs/2010.06439},
	doi = {10.48550/arXiv.2010.06439},
	abstract = {Our predictions for particle physics processes are realized in a chain of complex simulators. They allow us to generate high-fidelity simulated data, but they are not well-suited for inference on the theory parameters with observed data. We explain why the likelihood function of high-dimensional LHC data cannot be explicitly evaluated, why this matters for data analysis, and reframe what the field has traditionally done to circumvent this problem. We then review new simulation-based inference methods that let us directly analyze high-dimensional data by combining machine learning techniques and information from the simulator. Initial studies indicate that these techniques have the potential to substantially improve the precision of LHC measurements. Finally, we discuss probabilistic programming, an emerging paradigm that lets us extend inference to the latent process of the simulator.},
	urldate = {2023-11-05},
	publisher = {arXiv},
	author = {Brehmer, Johann and Cranmer, Kyle},
	month = nov,
	year = {2020},
	note = {arXiv:2010.06439 [hep-ex, physics:hep-ph, physics:physics, stat]},
	keywords = {High Energy Physics - Experiment, High Energy Physics - Phenomenology, Physics - Data Analysis, Statistics and Probability, Statistics - Machine Learning},
}

@article{hermans_towards_2021,
	title = {Towards constraining warm dark matter with stellar streams through neural simulation-based inference},
	volume = {507},
	issn = {0035-8711, 1365-2966},
	url = {http://arxiv.org/abs/2011.14923},
	doi = {10.1093/mnras/stab2181},
	abstract = {A statistical analysis of the observed perturbations in the density of stellar streams can in principle set stringent contraints on the mass function of dark matter subhaloes, which in turn can be used to constrain the mass of the dark matter particle. However, the likelihood of a stellar density with respect to the stream and subhaloes parameters involves solving an intractable inverse problem which rests on the integration of all possible forward realisations implicitly defined by the simulation model. In order to infer the subhalo abundance, previous analyses have relied on Approximate Bayesian Computation (ABC) together with domain-motivated but handcrafted summary statistics. Here, we introduce a likelihood-free Bayesian inference pipeline based on Amortised Approximate Likelihood Ratios (AALR), which automatically learns a mapping between the data and the simulator parameters and obviates the need to handcraft a possibly insufficient summary statistic. We apply the method to the simplified case where stellar streams are only perturbed by dark matter subhaloes, thus neglecting baryonic substructures, and describe several diagnostics that demonstrate the effectiveness of the new method and the statistical quality of the learned estimator.},
	number = {2},
	urldate = {2023-11-05},
	journal = {Monthly Notices of the Royal Astronomical Society},
	author = {Hermans, Joeri and Banik, Nilanjan and Weniger, Christoph and Bertone, Gianfranco and Louppe, Gilles},
	month = aug,
	year = {2021},
	note = {arXiv:2011.14923 [astro-ph, stat]},
	keywords = {Astrophysics - Astrophysics of Galaxies, Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics, Statistics - Machine Learning},
	pages = {1999--2011},
}

@misc{trippe_conditional_2018,
	title = {Conditional {Density} {Estimation} with {Bayesian} {Normalising} {Flows}},
	url = {http://arxiv.org/abs/1802.04908},
	abstract = {Modeling complex conditional distributions is critical in a variety of settings. Despite a long tradition of research into conditional density estimation, current methods employ either simple parametric forms or are difï¬cult to learn in practice. This paper employs normalising ï¬ows as a ï¬exible likelihood model and presents an efï¬cient method for ï¬tting them to complex densities. These estimators must trade-off between modeling distributional complexity, functional complexity and heteroscedasticity without overï¬tting. We recognize these trade-offs as modeling decisions and develop a Bayesian framework for placing priors over these conditional density estimators using variational Bayesian neural networks. We evaluate this method on several small benchmark regression datasets, on some of which it obtains state of the art performance. Finally, we apply the method to two spatial density modeling tasks with over 1 million datapoints using the New York City yellow taxi dataset and the Chicago crime dataset.},
	language = {en},
	urldate = {2023-11-05},
	publisher = {arXiv},
	author = {Trippe, Brian L. and Turner, Richard E.},
	month = feb,
	year = {2018},
	note = {arXiv:1802.04908 [stat]},
	keywords = {Statistics - Machine Learning},
}

@misc{rothfuss_conditional_2019,
	title = {Conditional {Density} {Estimation} with {Neural} {Networks}: {Best} {Practices} and {Benchmarks}},
	shorttitle = {Conditional {Density} {Estimation} with {Neural} {Networks}},
	url = {http://arxiv.org/abs/1903.00954},
	abstract = {Given a set of empirical observations, conditional density estimation aims to capture the statistical relationship between a conditional variable x and a dependent variable y by modeling their conditional probability p(y{\textbar}x). The paper develops best practices for conditional density estimation for ï¬nance applications with neural networks, grounded on mathematical insights and empirical evaluations. In particular, we introduce a noise regularization and data normalization scheme, alleviating problems with over-ï¬tting, initialization and hyper-parameter sensitivity of such estimators. We compare our proposed methodology with popular semi- and non-parametric density estimators, underpin its eï¬ectiveness in various benchmarks on simulated and Euro Stoxx 50 data and show its superior performance. Our methodology allows to obtain high-quality estimators for statistical expectations of higher moments, quantiles and non-linear return transformations, with very little assumptions about the return dynamic.},
	language = {en},
	urldate = {2023-11-05},
	publisher = {arXiv},
	author = {Rothfuss, Jonas and Ferreira, Fabio and Walther, Simon and Ulrich, Maxim},
	month = apr,
	year = {2019},
	note = {arXiv:1903.00954 [cs, q-fin, stat]},
	keywords = {Computer Science - Machine Learning, Quantitative Finance - Computational Finance, Quantitative Finance - Statistical Finance, Statistics - Machine Learning},
}

@misc{delaunoy_lightning-fast_2020,
	title = {Lightning-{Fast} {Gravitational} {Wave} {Parameter} {Inference} through {Neural} {Amortization}},
	url = {http://arxiv.org/abs/2010.12931},
	abstract = {Gravitational waves from compact binaries measured by the LIGO and Virgo detectors are routinely analyzed using Markov Chain Monte Carlo sampling algorithms. Because the evaluation of the likelihood function requires evaluating millions of waveform models that link between signal shapes and the source parameters, running Markov chains until convergence is typically expensive and requires days of computation. In this extended abstract, we provide a proof of concept that demonstrates how the latest advances in neural simulation-based inference can speed up the inference time by up to three orders of magnitude â from days to minutes â without impairing the performance. Our approach is based on a convolutional neural network modeling the likelihood-to-evidence ratio and entirely amortizes the computation of the posterior. We ï¬nd that our model correctly estimates credible intervals for the parameters of simulated gravitational waves.},
	language = {en},
	urldate = {2023-11-05},
	publisher = {arXiv},
	author = {Delaunoy, Arnaud and Wehenkel, Antoine and Hinderer, Tanja and Nissanke, Samaya and Weniger, Christoph and Williamson, Andrew R. and Louppe, Gilles},
	month = dec,
	year = {2020},
	note = {arXiv:2010.12931 [astro-ph, physics:gr-qc]},
	keywords = {Astrophysics - Instrumentation and Methods for Astrophysics, Computer Science - Machine Learning, General Relativity and Quantum Cosmology},
}

@misc{le_inference_2017,
	title = {Inference {Compilation} and {Universal} {Probabilistic} {Programming}},
	url = {http://arxiv.org/abs/1610.09900},
	abstract = {We introduce a method for using deep neural networks to amortize the cost of inference in models from the family induced by universal probabilistic programming languages, establishing a framework that combines the strengths of probabilistic programming and deep learning methods. We call what we do "compilation of inference" because our method transforms a denotational specification of an inference problem in the form of a probabilistic program written in a universal programming language into a trained neural network denoted in a neural network specification language. When at test time this neural network is fed observational data and executed, it performs approximate inference in the original model specified by the probabilistic program. Our training objective and learning procedure are designed to allow the trained neural network to be used as a proposal distribution in a sequential importance sampling inference engine. We illustrate our method on mixture models and Captcha solving and show significant speedups in the efficiency of inference.},
	language = {en},
	urldate = {2023-11-05},
	publisher = {arXiv},
	author = {Le, Tuan Anh and Baydin, Atilim Gunes and Wood, Frank},
	month = mar,
	year = {2017},
	note = {arXiv:1610.09900 [cs, stat]},
	keywords = {68T37, 68T05, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, G.3, I.2.6, Statistics - Machine Learning},
}

@misc{miller_contrastive_2023,
	title = {Contrastive {Neural} {Ratio} {Estimation}},
	url = {http://arxiv.org/abs/2210.06170},
	abstract = {Likelihood-to-evidence ratio estimation is usually cast as either a binary (NRE-A) or a multiclass (NRE-B) classiï¬cation task. In contrast to the binary classiï¬cation framework, the current formulation of the multiclass version has an intrinsic and unknown bias term, making otherwise informative diagnostics unreliable. We propose a multiclass framework free from the bias inherent to NRE-B at optimum, leaving us in the position to run diagnostics that practitioners depend on. It also recovers NRE-A in one corner case and NRE-B in the limiting case. For fair comparison, we benchmark the behavior of all algorithms in both familiar and novel training regimes: when jointly drawn data is unlimited, when data is ï¬xed but prior draws are unlimited, and in the commonplace ï¬xed data and parameters setting. Our investigations reveal that the highest performing models are distant from the competitors (NRE-A, NRE-B) in hyperparameter space. We make a recommendation for hyperparameters distinct from the previous models. We suggest a bound on the mutual information as a performance metric for simulation-based inference methods, without the need for posterior samples, and provide experimental results.},
	language = {en},
	urldate = {2023-11-05},
	publisher = {arXiv},
	author = {Miller, Benjamin Kurt and Weniger, Christoph and ForrÃ©, Patrick},
	month = jan,
	year = {2023},
	note = {arXiv:2210.06170 [astro-ph, physics:hep-ph, stat]},
	keywords = {Astrophysics - Instrumentation and Methods for Astrophysics, Computer Science - Machine Learning, High Energy Physics - Phenomenology, Statistics - Machine Learning},
}

@article{papamakarios_fast_nodate,
	title = {Fast -free {Inference} of {Simulation} {Models} with {Bayesian} {Conditional} {Density} {Estimation}},
	abstract = {Many statistical models can be simulated forwards but have intractable likelihoods. Approximate Bayesian Computation (ABC) methods are used to infer properties of these models from data. Traditionally these methods approximate the posterior over parameters by conditioning on data being inside an -ball around the observed data, which is only correct in the limit â 0. Monte Carlo methods can then draw samples from the approximate posterior to approximate predictions or error bars on parameters. These algorithms critically slow down as â 0, and in practice draw samples from a broader distribution than the posterior. We propose a new approach to likelihood-free inference based on Bayesian conditional density estimation. Preliminary inferences based on limited simulation data are used to guide later simulations. In some cases, learning an accurate parametric representation of the entire true posterior distribution requires fewer model simulations than Monte Carlo ABC methods need to produce a single sample from an approximate posterior.},
	language = {en},
	author = {Papamakarios, George and Murray, Iain},
}

@article{cranmer_frontier_2020,
	title = {The frontier of simulation-based inference},
	volume = {117},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.1912789117},
	doi = {10.1073/pnas.1912789117},
	abstract = {Many domains of science have developed complex simulations to describe phenomena of interest. While these simulations provide high-fidelity models, they are poorly suited for inference and lead to challenging inverse problems. We review the rapidly developing field of simulation-based inference and identify the forces giving additional momentum to the field. Finally, we describe how the frontier is expanding so that a broad audience can appreciate the profound influence these developments may have on science.},
	language = {en},
	number = {48},
	urldate = {2023-11-05},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Cranmer, Kyle and Brehmer, Johann and Louppe, Gilles},
	month = dec,
	year = {2020},
	pages = {30055--30062},
}

@misc{noauthor_simulation-based_nodate,
	title = {Simulation-based inference},
	url = {http://simulation-based-inference.org//},
	abstract = {Simulation-based Inference is the next evolution in statistics},
	language = {en},
	urldate = {2023-11-05},
	journal = {Simulation-based Inference},
}

@misc{sharrock_sequential_2022,
	title = {Sequential {Neural} {Score} {Estimation}: {Likelihood}-{Free} {Inference} with {Conditional} {Score} {Based} {Diffusion} {Models}},
	shorttitle = {Sequential {Neural} {Score} {Estimation}},
	url = {http://arxiv.org/abs/2210.04872},
	abstract = {We introduce Sequential Neural Posterior Score Estimation (SNPSE) and Sequential Neural Likelihood Score Estimation (SNLSE), two new score-based methods for Bayesian inference in simulator-based models. Our methods, inspired by the success of score-based methods in generative modelling, leverage conditional score-based diffusion models to generate samples from the posterior distribution of interest. These models can be trained using one of two possible objective functions, one of which approximates the score of the intractable likelihood, while the other directly estimates the score of the posterior. We embed these models into a sequential training procedure, which guides simulations using the current approximation of the posterior at the observation of interest, thereby reducing the simulation cost. We validate our methods, as well as their amortised, non-sequential variants, on several numerical examples, demonstrating comparable or superior performance to existing state-of-the-art methods such as Sequential Neural Posterior Estimation (SNPE) and Sequential Neural Likelihood Estimation (SNLE).},
	language = {en},
	urldate = {2023-11-05},
	publisher = {arXiv},
	author = {Sharrock, Louis and Simons, Jack and Liu, Song and Beaumont, Mark},
	month = nov,
	year = {2022},
	note = {arXiv:2210.04872 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{papamakarios_sequential_2019,
	title = {Sequential {Neural} {Likelihood}: {Fast} {Likelihood}-free {Inference} with {Autoregressive} {Flows}},
	shorttitle = {Sequential {Neural} {Likelihood}},
	url = {http://arxiv.org/abs/1805.07226},
	abstract = {We present Sequential Neural Likelihood (SNL), a new method for Bayesian inference in simulator models, where the likelihood is intractable but simulating data from the model is possible. SNL trains an autoregressive ï¬ow on simulated data in order to learn a model of the likelihood in the region of high posterior density. A sequential training procedure guides simulations and reduces simulation cost by orders of magnitude. We show that SNL is more robust, more accurate and requires less tuning than related neural-based methods, and we discuss diagnostics for assessing calibration, convergence and goodness-of-ï¬t.},
	language = {en},
	urldate = {2023-11-05},
	publisher = {arXiv},
	author = {Papamakarios, George and Sterratt, David C. and Murray, Iain},
	month = jan,
	year = {2019},
	note = {arXiv:1805.07226 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{elsemuller_sensitivity-aware_2023,
	title = {Sensitivity-{Aware} {Amortized} {Bayesian} {Inference}},
	url = {http://arxiv.org/abs/2310.11122},
	abstract = {Bayesian inference is a powerful framework for making probabilistic inferences and decisions under uncertainty. Fundamental choices in modern Bayesian workflows concern the specification of the likelihood function and prior distributions, the posterior approximator, and the data. Each choice can significantly influence model-based inference and subsequent decisions, thereby necessitating sensitivity analysis. In this work, we propose a multifaceted approach to integrate sensitivity analyses into amortized Bayesian inference (ABI, i.e., simulation-based inference with neural networks). First, we utilize weight sharing to encode the structural similarities between alternative likelihood and prior specifications in the training process with minimal computational overhead. Second, we leverage the rapid inference of neural networks to assess sensitivity to various data perturbations or pre-processing procedures. In contrast to most other Bayesian approaches, both steps circumvent the costly bottleneck of refitting the model(s) for each choice of likelihood, prior, or dataset. Finally, we propose to use neural network ensembles to evaluate variation in results induced by unreliable approximation on unseen data. We demonstrate the effectiveness of our method in applied modeling problems, ranging from the estimation of disease outbreak dynamics and global warming thresholds to the comparison of human decision-making models. Our experiments showcase how our approach enables practitioners to effectively unveil hidden relationships between modeling choices and inferential conclusions.},
	language = {en},
	urldate = {2023-11-03},
	publisher = {arXiv},
	author = {ElsemÃ¼ller, Lasse and OlischlÃ¤ger, Hans and Schmitt, Marvin and BÃ¼rkner, Paul-Christian and KÃ¶the, Ullrich and Radev, Stefan T.},
	month = oct,
	year = {2023},
	note = {arXiv:2310.11122 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
}

@misc{glockler_variational_2022,
	title = {Variational methods for simulation-based inference},
	url = {http://arxiv.org/abs/2203.04176},
	abstract = {We present Sequential Neural Variational Inference (SNVI), an approach to perform Bayesian inference in models with intractable likelihoods. SNVI combines likelihood-estimation (or likelihood-ratio-estimation) with variational inference to achieve a scalable simulation-based inference approach. SNVI maintains the ï¬exibility of likelihood(-ratio) estimation to allow arbitrary proposals for simulations, while simultaneously providing a functional estimate of the posterior distribution without requiring MCMC sampling. We present several variants of SNVI and demonstrate that they are substantially more computationally efï¬cient than previous algorithms, without loss of accuracy on benchmark tasks. We apply SNVI to a neuroscience model of the pyloric network in the crab and demonstrate that it can infer the posterior distribution with one order of magnitude fewer simulations than previously reported. SNVI vastly reduces the computational cost of simulationbased inference while maintaining accuracy and ï¬exibility, making it possible to tackle problems that were previously inaccessible.},
	language = {en},
	urldate = {2023-11-03},
	publisher = {arXiv},
	author = {GlÃ¶ckler, Manuel and Deistler, Michael and Macke, Jakob H.},
	month = oct,
	year = {2022},
	note = {arXiv:2203.04176 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{ramesh_gatsbi_2022,
	title = {{GATSBI}: {Generative} {Adversarial} {Training} for {Simulation}-{Based} {Inference}},
	shorttitle = {{GATSBI}},
	url = {http://arxiv.org/abs/2203.06481},
	abstract = {Simulation-based inference (SBI) refers to statistical inference on stochastic models for which we can generate samples, but not compute likelihoods. Like SBI algorithms, generative adversarial networks (GANs) do not require explicit likelihoods. We study the relationship between SBI and GANs, and introduce GATSBI, an adversarial approach to SBI. GATSBI reformulates the variational objective in an adversarial setting to learn implicit posterior distributions. Inference with GATSBI is amortised across observations, works in high-dimensional posterior spaces and supports implicit priors. We evaluate GATSBI on two SBI benchmark problems and on two high-dimensional simulators. On a model for wave propagation on the surface of a shallow water body, we show that GATSBI can return well-calibrated posterior estimates even in high dimensions. On a model of camera optics, it infers a high-dimensional posterior given an implicit prior, and performs better than a state-of-the-art SBI approach. We also show how GATSBI can be extended to perform sequential posterior estimation to focus on individual observations. Overall, GATSBI opens up opportunities for leveraging advances in GANs to perform Bayesian inference on high-dimensional simulation-based models.},
	language = {en},
	urldate = {2023-11-03},
	publisher = {arXiv},
	author = {Ramesh, Poornima and Lueckmann, Jan-Matthis and Boelts, Jan and Tejero-Cantero, Ãlvaro and Greenberg, David S. and GonÃ§alves, Pedro J. and Macke, Jakob H.},
	month = mar,
	year = {2022},
	note = {arXiv:2203.06481 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
}

@misc{hermans_trust_2022,
	title = {A {Trust} {Crisis} {In} {Simulation}-{Based} {Inference}? {Your} {Posterior} {Approximations} {Can} {Be} {Unfaithful}},
	shorttitle = {A {Trust} {Crisis} {In} {Simulation}-{Based} {Inference}?},
	url = {http://arxiv.org/abs/2110.06581},
	abstract = {We present extensive empirical evidence showing that current Bayesian simulation-based inference algorithms are inadequate for the falsiï¬cationist methodology of scientiï¬c inquiry. Our results collected through months of experimental computations show that all benchmarked algorithms â (s)npe, (s)nre, snl and variants of abc â may produce overconï¬dent posterior approximations, which makes them demonstrably unreliable and dangerous if oneâs scientiï¬c goal is to constrain parameters of interest. We believe that failing to address this issue will lead to a well-founded trust crisis in simulation-based inference. For this reason, we argue that research eï¬orts should now consider theoretical and methodological developments of conservative approximate inference algorithms and present research directions towards this objective. In this regard, we show empirical evidence that ensembles are consistently more reliable.},
	language = {en},
	urldate = {2023-11-03},
	publisher = {arXiv},
	author = {Hermans, Joeri and Delaunoy, Arnaud and Rozet, FranÃ§ois and Wehenkel, Antoine and Begy, Volodimir and Louppe, Gilles},
	month = dec,
	year = {2022},
	note = {arXiv:2110.06581 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{miller_simulation-based_2023,
	title = {Simulation-based {Inference} with the {Generalized} {Kullback}-{Leibler} {Divergence}},
	url = {http://arxiv.org/abs/2310.01808},
	abstract = {In Simulation-based Inference, the goal is to solve the inverse problem when the likelihood is only known implicitly. Neural Posterior Estimation commonly fits a normalized density estimator as a surrogate model for the posterior. This formulation cannot easily fit unnormalized surrogates because it optimizes the Kullback-Leibler divergence. We propose to optimize a generalized Kullback-Leibler divergence that accounts for the normalization constant in unnormalized distributions. The objective recovers Neural Posterior Estimation when the model class is normalized and unifies it with Neural Ratio Estimation, combining both into a single objective. We investigate a hybrid model that offers the best of both worlds by learning a normalized base distribution and a learned ratio. We also present benchmark results.},
	language = {en},
	urldate = {2023-11-03},
	publisher = {arXiv},
	author = {Miller, Benjamin Kurt and Federici, Marco and Weniger, Christoph and ForrÃ©, Patrick},
	month = oct,
	year = {2023},
	note = {arXiv:2310.01808 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Computation, Statistics - Machine Learning},
}

@article{cranmer_frontier_2020-1,
	title = {The frontier of simulation-based inference},
	volume = {117},
	issn = {0027-8424, 1091-6490},
	url = {http://arxiv.org/abs/1911.01429},
	doi = {10.1073/pnas.1912789117},
	abstract = {Many domains of science have developed complex simulations to describe phenomena of interest. While these simulations provide high-fidelity models, they are poorly suited for inference and lead to challenging inverse problems. We review the rapidly developing field of simulation-based inference and identify the forces giving new momentum to the field. Finally, we describe how the frontier is expanding so that a broad audience can appreciate the profound change these developments may have on science.},
	language = {en},
	number = {48},
	urldate = {2023-11-03},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Cranmer, Kyle and Brehmer, Johann and Louppe, Gilles},
	month = dec,
	year = {2020},
	note = {arXiv:1911.01429 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
	pages = {30055--30062},
}

@article{cranmer_frontier_2020-2,
	title = {The frontier of simulation-based inference},
	volume = {117},
	issn = {0027-8424, 1091-6490},
	url = {http://arxiv.org/abs/1911.01429},
	doi = {10.1073/pnas.1912789117},
	abstract = {Many domains of science have developed complex simulations to describe phenomena of interest. While these simulations provide high-fidelity models, they are poorly suited for inference and lead to challenging inverse problems. We review the rapidly developing field of simulation-based inference and identify the forces giving new momentum to the field. Finally, we describe how the frontier is expanding so that a broad audience can appreciate the profound change these developments may have on science.},
	language = {en},
	number = {48},
	urldate = {2023-11-03},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Cranmer, Kyle and Brehmer, Johann and Louppe, Gilles},
	month = dec,
	year = {2020},
	note = {arXiv:1911.01429 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
	pages = {30055--30062},
}

@article{del_moral_sequential_2006,
	title = {Sequential {Monte} {Carlo} {Samplers}},
	volume = {68},
	issn = {1369-7412, 1467-9868},
	url = {https://academic.oup.com/jrsssb/article/68/3/411/7110641},
	doi = {10.1111/j.1467-9868.2006.00553.x},
	abstract = {We propose a methodology to sample sequentially from a sequence of probability distributions that are deï¬ned on a common space, each distribution being known up to a normalizing constant. These probability distributions are approximated by a cloud of weighted random samples which are propagated over time by using sequential Monte Carlo methods. This methodology allows us to derive simple algorithms to make parallel Markov chain Monte Carlo algorithms interact to perform global optimization and sequential Bayesian estimation and to compute ratios of normalizing constants. We illustrate these algorithms for various integration tasks arising in the context of Bayesian inference.},
	language = {en},
	number = {3},
	urldate = {2023-10-26},
	journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
	author = {Del Moral, Pierre and Doucet, Arnaud and Jasra, Ajay},
	month = jun,
	year = {2006},
	pages = {411--436},
}

@misc{crucinio_properties_2023,
	title = {Properties of {Marginal} {Sequential} {Monte} {Carlo} {Methods}},
	url = {http://arxiv.org/abs/2303.03498},
	doi = {10.48550/arXiv.2303.03498},
	abstract = {We provide a framework which admits a number of ``marginal'' sequential Monte Carlo (SMC) algorithms as particular cases -- including the marginal particle filter [Klaas et al., 2005, in: Proceedings of Uncertainty in Artificial Intelligence, pp. 308--315], , the independent particle filter [Lin et al., 2005, Journal of the American Statistical Association 100, pp. 1412--1421] and linear-cost Approximate Bayesian Computation SMC [Sisson et al., 2007, Proceedings of the National Academy of Sciences (USA) 104, pp. 1760--1765.]. We provide conditions under which such algorithms obey laws of large numbers and central limit theorems and provide some further asymptotic characterizations. Finally, it is shown that the asymptotic variance of a class of estimators associated with certain marginal SMC algorithms is never greater than that of the estimators provided by a standard SMC algorithm using the same proposal distributions.},
	urldate = {2023-10-26},
	publisher = {arXiv},
	author = {Crucinio, Francesca R. and Johansen, Adam M.},
	month = mar,
	year = {2023},
	note = {arXiv:2303.03498 [math, stat]},
	keywords = {Mathematics - Probability, Mathematics - Statistics Theory, Statistics - Computation, Statistics - Methodology},
}

@misc{kuntz_divide-and-conquer_2023,
	title = {The divide-and-conquer sequential {Monte} {Carlo} algorithm: theoretical properties and limit theorems},
	shorttitle = {The divide-and-conquer sequential {Monte} {Carlo} algorithm},
	url = {http://arxiv.org/abs/2110.15782},
	abstract = {We provide a comprehensive characterisation of the theoretical properties of the divide-and-conquer sequential Monte Carlo (DaC-SMC) algorithm. We firmly establish it as a well-founded method by showing that it possesses the same basic properties as conventional sequential Monte Carlo (SMC) algorithms do. In particular, we derive pertinent laws of large numbers, \$L{\textasciicircum}p\$ inequalities, and central limit theorems; and we characterize the bias in the normalized estimates produced by the algorithm and argue the absence thereof in the unnormalized ones. We further consider its practical implementation and several interesting variants; obtain expressions for its globally and locally optimal intermediate targets, auxiliary measures, and proposal kernels; and show that, in comparable conditions, DaC-SMC proves more statistically efficient than its direct SMC analogue. We close the paper with a discussion of our results, open questions, and future research directions.},
	language = {en},
	urldate = {2023-10-26},
	publisher = {arXiv},
	author = {Kuntz, Juan and Crucinio, Francesca R. and Johansen, Adam M.},
	month = jun,
	year = {2023},
	note = {arXiv:2110.15782 [math, stat]},
	keywords = {65C05 (Primary) 60F05, 60F15, 62F15, 68W15 (Secondary), Mathematics - Probability, Mathematics - Statistics Theory, Statistics - Computation, Statistics - Methodology},
}

@book{liu_monte_2009,
	title = {Monte {Carlo} {Strategies} in {Scientic} {Computing}},
	isbn = {978-0-387-76369-9},
	author = {Liu, Jun},
	month = feb,
	year = {2009},
	doi = {10.1007/978-0-387-76371-2},
}

@article{van_osta_uncertainty_2021,
	title = {Uncertainty {Quantification} of {Regional} {Cardiac} {Tissue} {Properties} in {Arrhythmogenic} {Cardiomyopathy} {Using} {Adaptive} {Multiple} {Importance} {Sampling}},
	volume = {12},
	issn = {1664-042X},
	url = {https://www.frontiersin.org/articles/10.3389/fphys.2021.738926},
	abstract = {Introduction: Computational models of the cardiovascular system are widely used to simulate cardiac (dys)function. Personalization of such models for patient-specific simulation of cardiac function remains challenging. Measurement uncertainty affects accuracy of parameter estimations. In this study, we present a methodology for patient-specific estimation and uncertainty quantification of parameters in the closed-loop CircAdapt model of the human heart and circulation using echocardiographic deformation imaging. Based on patient-specific estimated parameters we aim to reveal the mechanical substrate underlying deformation abnormalities in patients with arrhythmogenic cardiomyopathy (AC).Methods: We used adaptive multiple importance sampling to estimate the posterior distribution of regional myocardial tissue properties. This methodology is implemented in the CircAdapt cardiovascular modeling platform and applied to estimate active and passive tissue properties underlying regional deformation patterns, left ventricular volumes, and right ventricular diameter. First, we tested the accuracy of this method and its inter- and intraobserver variability using nine datasets obtained in AC patients. Second, we tested the trueness of the estimation using nine in silico generated virtual patient datasets representative for various stages of AC. Finally, we applied this method to two longitudinal series of echocardiograms of two pathogenic mutation carriers without established myocardial disease at baseline.Results: Tissue characteristics of virtual patients were accurately estimated with a highest density interval containing the true parameter value of 9\% (95\% CI [0â79]). Variances of estimated posterior distributions in patient data and virtual data were comparable, supporting the reliability of the patient estimations. Estimations were highly reproducible with an overlap in posterior distributions of 89.9\% (95\% CI [60.1â95.9]). Clinically measured deformation, ejection fraction, and end-diastolic volume were accurately simulated. In presence of worsening of deformation over time, estimated tissue properties also revealed functional deterioration.Conclusion: This method facilitates patient-specific simulation-based estimation of regional ventricular tissue properties from non-invasive imaging data, taking into account both measurement and model uncertainties. Two proof-of-principle case studies suggested that this cardiac digital twin technology enables quantitative monitoring of AC disease progression in early stages of disease.},
	urldate = {2023-07-17},
	journal = {Frontiers in Physiology},
	author = {van Osta, Nick and Kirkels, Feddo P. and van Loon, Tim and Koopsen, Tijmen and Lyon, Aurore and Meiburg, Roel and Huberts, Wouter and Cramer, Maarten J. and Delhaas, Tammo and Haugaa, Kristina H. and Teske, Arco J. and Lumens, Joost},
	year = {2021},
}

@article{siren_reconstructing_2011,
	title = {Reconstructing {Population} {Histories} from {Single} {Nucleotide} {Polymorphism} {Data}},
	volume = {28},
	issn = {0737-4038},
	url = {https://doi.org/10.1093/molbev/msq236},
	doi = {10.1093/molbev/msq236},
	abstract = {Population genetics encompasses a strong theoretical and applied research tradition on the multiple demographic processes that shape genetic variation present within a species. When several distinct populations exist in the current generation, it is often natural to consider the pattern of their divergence from a single ancestral population in terms of a binary tree structure. Inference about such population histories based on molecular data has been an intensive research topic in the recent years. The most common approach uses coalescent theory to model genealogies of individuals sampled from the current populations. Such methods are able to compare several different evolutionary scenarios and to estimate demographic parameters. However, their major limitation is the enormous computational complexity associated with the indirect modeling of the demographies, which limits the application to small data sets. Here, we propose a novel Bayesian method for inferring population histories from unlinked single nucleotide polymorphisms, which is applicable also to data sets harboring large numbers of individuals from distinct populations. We use an approximation to the neutral WrightâFisher diffusion to model random fluctuations in allele frequencies. The population histories are modeled as binary rooted trees that represent the historical order of divergence of the different populations. A combination of analytical, numerical, and Monte Carlo integration techniques are utilized for the inferences. A particularly important feature of our approach is that it provides intuitive measures of statistical uncertainty related with the estimates computed, which may be entirely lacking for the alternative methods in this context. The potential of our approach is illustrated by analyses of both simulated and real data sets.},
	number = {1},
	urldate = {2023-07-17},
	journal = {Molecular Biology and Evolution},
	author = {SirÃ©n, Jukka and Marttinen, Pekka and Corander, Jukka},
	month = jan,
	year = {2011},
	pages = {673--683},
}

@article{xiong_adaptive_2017,
	title = {Adaptive multiple importance sampling for {Gaussian} processes},
	volume = {87},
	issn = {0094-9655},
	url = {https://doi.org/10.1080/00949655.2017.1280037},
	doi = {10.1080/00949655.2017.1280037},
	abstract = {In applications of Gaussian processes (GPs) where quantification of uncertainty is a strict requirement, it is necessary to accurately characterize the posterior distribution over Gaussian process covariance parameters. This is normally done by means of standard Markov chain Monte Carlo (MCMC) algorithms, which require repeated expensive calculations involving the marginal likelihood. Motivated by the desire to avoid the inefficiencies of MCMC algorithms rejecting a considerable amount of expensive proposals, this paper develops an alternative inference framework based on adaptive multiple importance sampling (AMIS). In particular, this paper studies the application of AMIS for GPs in the case of a Gaussian likelihood, and proposes a novel pseudo-marginal-based AMIS algorithm for non-Gaussian likelihoods, where the marginal likelihood is unbiasedly estimated. The results suggest that the proposed framework outperforms MCMC-based inference of covariance parameters in a wide range of scenarios.},
	number = {8},
	urldate = {2023-07-17},
	journal = {Journal of Statistical Computation and Simulation},
	author = {Xiong, Xiaoyu and Å mÃ­dl, VÃ¡clav and Filippone, Maurizio},
	month = may,
	year = {2017},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00949655.2017.1280037},
	keywords = {Bayesian inference, Gaussian processes, Markov chain Monte Carlo, importance sampling},
	pages = {1644--1665},
}

@article{elvira_generalized_2019,
	title = {Generalized {Multiple} {Importance} {Sampling}},
	volume = {34},
	issn = {0883-4237, 2168-8745},
	url = {https://projecteuclid.org/journals/statistical-science/volume-34/issue-1/Generalized-Multiple-Importance-Sampling/10.1214/18-STS668.full},
	doi = {10.1214/18-STS668},
	abstract = {Importance sampling (IS) methods are broadly used to approximate posterior distributions or their moments. In the standard IS approach, samples are drawn from a single proposal distribution and weighted adequately. However, since the performance in IS depends on the mismatch between the targeted and the proposal distributions, several proposal densities are often employed for the generation of samples. Under this multiple importance sampling (MIS) scenario, extensive literature has addressed the selection and adaptation of the proposal distributions, interpreting the sampling and weighting steps in different ways. In this paper, we establish a novel general framework with sampling and weighting procedures when more than one proposal is available. The new framework encompasses most relevant MIS schemes in the literature, and novel valid schemes appear naturally. All the MIS schemes are compared and ranked in terms of the variance of the associated estimators. Finally, we provide illustrative examples revealing that, even with a good choice of the proposal densities, a careful interpretation of the sampling and weighting procedures can make a significant difference in the performance of the method.},
	number = {1},
	urldate = {2023-07-17},
	journal = {Statistical Science},
	author = {Elvira, VÃ­ctor and Martino, Luca and Luengo, David and Bugallo, MÃ³nica F.},
	month = feb,
	year = {2019},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Bayesian inference, Monte Carlo methods, multiple importance sampling},
	pages = {129--155},
}

@inproceedings{el-laham_efficient_2019,
	title = {Efficient {Adaptive} {Multiple} {Importance} {Sampling}},
	doi = {10.23919/EUSIPCO.2019.8902642},
	abstract = {The adaptive multiple importance sampling (AMIS) algorithm is a powerful Monte Carlo tool for Bayesian estimation in intractable models. The uniqueness of this methodology from other adaptive importance sampling (AIS) schemes is in the weighting procedure, where at each iteration of the algorithm, all samples are re-weighted according to the temporal deterministic mixture approach. This re-weighting allows for substantial variance reduction of the AMIS estimator, at the expense of an increased computational cost that grows quadratically with the number of iterations. In this paper, we propose a novel AIS methodology which obtains most of the AMIS variance reduction while improving upon its computational complexity. The proposed method implements an approximate version of the temporal deterministic mixture approach and requires substantially less computation. Advantages are shown empirically through a numerical example, where the novel method is able to attain a desired mean-squared error with much less computation.},
	booktitle = {2019 27th {European} {Signal} {Processing} {Conference} ({EUSIPCO})},
	author = {El-Laham, Yousef and Martino, Luca and Elvira, VÃ­ctor and Bugallo, MÃ³nica F.},
	month = sep,
	year = {2019},
	note = {ISSN: 2076-1465},
	keywords = {Approximation algorithms, Artificial intelligence, Computational efficiency, Monte Carlo methods, Probability density function, Proposals, Standards},
	pages = {1--5},
}

@article{bugallo_adaptive_2015,
	title = {Adaptive importance sampling in signal processing},
	volume = {47},
	issn = {1051-2004},
	url = {https://www.sciencedirect.com/science/article/pii/S1051200415001864},
	doi = {https://doi.org/10.1016/j.dsp.2015.05.014},
	abstract = {In Bayesian signal processing, all the information about the unknowns of interest is contained in their posterior distributions. The unknowns can be parameters of a model, or a model and its parameters. In many important problems, these distributions are impossible to obtain in analytical form. An alternative is to generate their approximations by Monte Carlo-based methods like Markov chain Monte Carlo (MCMC) sampling, adaptive importance sampling (AIS) or particle filtering (PF). While MCMC sampling and PF have received considerable attention in the literature and are reasonably well understood, the AIS methodology remains relatively unexplored. This article reviews the basics of AIS as well as provides a comprehensive survey of the state-of-the-art of the topic. Some of its most relevant implementations are revisited and compared through computer simulation examples.},
	journal = {Digital Signal Processing},
	author = {Bugallo, MÃ³nica F. and Martino, Luca and Corander, Jukka},
	year = {2015},
	keywords = {Adaptive importance sampling, Markov chain Monte Carlo, Particle filtering, Population Monte Carlo},
	pages = {36--49},
}

@misc{marin_consistency_2014,
	title = {Consistency of the {Adaptive} {Multiple} {Importance} {Sampling}},
	url = {http://arxiv.org/abs/1211.2548},
	abstract = {Among Monte Carlo techniques, the importance sampling requires ï¬ne tuning of a proposal distribution, which is now ï¬uently resolved through iterative schemes. The Adaptive Multiple Importance Sampling (AMIS) of Cornuet et al. (2012) provides a signiï¬cant improvement in stability and eï¬ective sample size due to the introduction of a recycling procedure. However, the consistency of the AMIS estimator remains largely open. In this work we prove the convergence of the AMIS, at a cost of a slight modiï¬cation in the learning process. Contrary to Douc et al. (2007a), results are obtained here in the asymptotic regime where the number of iterations is going to inï¬nity while the number of drawings per iteration is a ï¬xed, but growing sequence of integers. Hence some of the results shed new light on adaptive population Monte Carlo algorithms in that last regime.},
	language = {en},
	urldate = {2023-07-17},
	publisher = {arXiv},
	author = {Marin, Jean-Michel and Pudlo, Pierre and Sedki, Mohammed},
	month = may,
	year = {2014},
	note = {arXiv:1211.2548 [math, stat]},
	keywords = {65C05 (Primary) 60F17 (Secondary), Mathematics - Statistics Theory, Statistics - Computation},
}

@article{noauthor_adaptive_2015,
	title = {Adaptive importance sampling in signal processing},
	volume = {47},
	issn = {1051-2004},
	url = {https://www.sciencedirect.com/science/article/pii/S1051200415001864},
	doi = {10.1016/j.dsp.2015.05.014},
	abstract = {In Bayesian signal processing, all the information about the unknowns of interest is contained in their posterior distributions. The unknowns can be pâ¦},
	language = {en},
	urldate = {2023-07-17},
	journal = {Digital Signal Processing},
	month = dec,
	year = {2015},
	note = {Publisher: Academic Press},
	pages = {36--49},
}

@article{owen_safe_2000,
	title = {Safe and {Effective} {Importance} {Sampling}},
	volume = {95},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/2669533},
	doi = {10.2307/2669533},
	abstract = {We present two improvements on the technique of importance sampling. First, we show that importance sampling from a mixture of densities, using those densities as control variates, results in a useful upper bound on the asymptotic variance. That bound is a small multiple of the asymptotic variance of importance sampling from the best single component density. This allows one to benefit from the great variance reductions obtainable by importance sampling, while protecting against the equally great variance increases that might take the practitioner by surprise. The second improvement is to show how importance sampling from two or more densities can be used to approach a zero sampling variance even for integrands that take both positive and negative values.},
	number = {449},
	urldate = {2023-07-17},
	journal = {Journal of the American Statistical Association},
	author = {Owen, Art and Zhou, Yi},
	year = {2000},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {135--143},
}

@misc{cornuet_adaptive_2011,
	title = {Adaptive {Multiple} {Importance} {Sampling}},
	url = {http://arxiv.org/abs/0907.1254},
	abstract = {The Adaptive Multiple Importance Sampling (AMIS) algorithm is aimed at an optimal recycling of past simulations in an iterated importance sampling scheme. The diï¬erence with earlier adaptive importance sampling implementations like Population Monte Carlo is that the importance weights of all simulated values, past as well as present, are recomputed at each iteration, following the technique of the deterministic multiple mixture estimator of Owen and Zhou (2000). Although the convergence properties of the algorithm cannot be investigated, we demonstrate through a challenging banana shape target distribution and a population genetics example that the improvement brought by this technique is substantial.},
	language = {en},
	urldate = {2023-07-17},
	publisher = {arXiv},
	author = {Cornuet, Jean-Marie and Marin, Jean-Michel and Mira, Antonietta and Robert, Christian P.},
	month = oct,
	year = {2011},
	note = {arXiv:0907.1254 [stat]},
	keywords = {Statistics - Applications, Statistics - Computation},
}

@article{delyon_accelerated_1993,
	title = {Accelerated {Stochastic} {Approximation}},
	volume = {3},
	issn = {1052-6234},
	url = {https://epubs.siam.org/doi/10.1137/0803045},
	doi = {10.1137/0803045},
	abstract = {A new recursive algorithm of stochastic approximation type with the averaging of trajectories is investigated. Convergence with probability one is proved for a variety of classical optimization and identification problems. It is also demonstrated for these problems that the proposed algorithm achieves the highest possible rate of convergence.},
	number = {4},
	urldate = {2023-07-04},
	journal = {SIAM Journal on Optimization},
	author = {Delyon, Bernard and Juditsky, Anatoli},
	month = nov,
	year = {1993},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {868--881},
}

@article{roberts_geometric_1996,
	title = {Geometric convergence and central limit theorems for multidimensional {Hastings} and {Metropolis} algorithms},
	volume = {83},
	issn = {0006-3444},
	url = {https://doi.org/10.1093/biomet/83.1.95},
	doi = {10.1093/biomet/83.1.95},
	abstract = {We develop results on geometric ergodicity of Markov chains and apply these and other recent results in Markov chain theory to multidimensional Hastings and Metropolis algorithms. For those based on random walk candidate distributions, we find sufficient conditions for moments and moment generating functions to converge at a geometric rate to a prescribed distribution Ï. By phrasing the conditions in terms of the curvature of the densities we show that the results apply to all distributions with positive densities in a large class which encompasses many commonly-used statistical forms. From these results we develop central limit theorems for the Metropolis algorithm. Converse results, showing non-geometric convergence rates for chains where the rejection rate is not bounded away from unity, are also given; these show that the negative-definiteness property is not redundant.},
	number = {1},
	urldate = {2023-07-03},
	journal = {Biometrika},
	author = {Roberts, G. O. and Tweedie, R. L.},
	month = mar,
	year = {1996},
	pages = {95--110},
}

@incollection{gelman_efficient_1996,
	title = {Efficient {Metropolis} jumping rules},
	booktitle = {Bayesian {Statistics}},
	publisher = {Oxford University Press, Oxford},
	author = {Gelman, A. and Roberts, G. O. and Gilks, W. R.},
	editor = {Bernardo, J. M. and Berger, J. O. and Dawid, A. P. and Smith, A. F. M.},
	year = {1996},
	pages = {599--608},
}

@article{noauthor_notitle_nodate-1,
}

@phdthesis{yang_ergodicity_2008,
	title = {Ergodicity of {Adaptive} {MCMC} and its {Applications}},
	url = {http://probability.ca/jeff/ftpdir/chaothesis.pdf},
	language = {en},
	school = {University of Toronto},
	author = {Yang, Chao},
	year = {2008},
}

@article{roberts_optimal_2001,
	title = {Optimal {Scaling} for {Various} {Metropolis}-{Hastings} {Algorithms}},
	volume = {16},
	issn = {0883-4237},
	url = {https://www.jstor.org/stable/3182776},
	abstract = {We review and extend results related to optimal scaling of Metropolis-Hastings algorithms. We present various theoretical results for the high-dimensional limit. We also present simulation studies which confirm the theoretical results in finite-dimensional contexts.},
	number = {4},
	urldate = {2023-07-03},
	journal = {Statistical Science},
	author = {Roberts, Gareth O. and Rosenthal, Jeffrey S.},
	year = {2001},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {351--367},
}

@misc{noauthor_max_nodate,
	title = {Max {T}. {Beans} (@mrmaxbeans) â¢ {Instagram} photos and videos},
	url = {https://www.instagram.com/mrmaxbeans/},
	abstract = {31 followers, 33 following, 131 posts â see Instagram photos and videos from Max T. Beans (@mrmaxbeans)},
	language = {en-gb},
	urldate = {2023-06-27},
}

@misc{noauthor_instagram_nodate,
	title = {Instagram {Graph} {API} - {Instagram} {Platform} - {Documentation}},
	url = {https://developers.facebook.com/docs/instagram-api/},
	abstract = {How to use the Instagram Graph API},
	language = {en},
	urldate = {2023-06-27},
	journal = {Meta for Developers},
}

@misc{noauthor_pricing_nodate,
	title = {Pricing},
	url = {https://openai.com/pricing},
	abstract = {Simple and flexible. Only pay for what you use.},
	language = {en-US},
	urldate = {2023-06-27},
}

@misc{noauthor_elo_2023,
	title = {Elo rating system},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Elo_rating_system&oldid=1161727100},
	abstract = {The Elo rating system is a method for calculating the relative skill levels of players in zero-sum games such as chess. It is named after its creator Arpad Elo, a Hungarian-American physics professor.
The Elo system was invented as an improved chess-rating system over the previously used Harkness system, but is also used as a rating system in association football, American football, baseball, basketball, pool, table tennis, various board games and esports, and most recently large language models.
The difference in the ratings between two players serves as a predictor of the outcome of a match. Two players with equal ratings who play against each other are expected to score an equal number of wins. A player whose rating is 100 points greater than their opponent's is expected to score 64\%; if the difference is 200 points, then the expected score for the stronger player is 76\%.
A player's Elo rating is represented by a number which may change depending on the outcome of rated games played. After every game, the winning player takes points from the losing one. The difference between the ratings of the winner and loser determines the total number of points gained or lost after a game. If the higher-rated player wins, then only a few rating points will be taken from the lower-rated player. However, if the lower-rated player scores an upset win, many rating points will be transferred. The lower-rated player will also gain a few points from the higher rated player in the event of a draw. This means that this rating system is self-correcting. Players whose ratings are too low or too high should, in the long run, do better or worse correspondingly than the rating system predicts and thus gain or lose rating points until the ratings reflect their true playing strength.
Elo ratings are comparative only, and are valid only within the rating pool in which they were calculated, rather than being an absolute measure of a player's strength.},
	language = {en},
	urldate = {2023-06-27},
	journal = {Wikipedia},
	month = jun,
	year = {2023},
	note = {Page Version ID: 1161727100},
}

@misc{dettmers_qlora_2023,
	title = {{QLoRA}: {Efficient} {Finetuning} of {Quantized} {LLMs}},
	shorttitle = {{QLoRA}},
	url = {http://arxiv.org/abs/2305.14314},
	abstract = {We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters{\textasciitilde}(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3\% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.},
	language = {en},
	urldate = {2023-06-27},
	publisher = {arXiv},
	author = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
	month = may,
	year = {2023},
	note = {arXiv:2305.14314 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{radford_learning_2021,
	title = {Learning {Transferable} {Visual} {Models} {From} {Natural} {Language} {Supervision}},
	url = {http://arxiv.org/abs/2103.00020},
	doi = {10.48550/arXiv.2103.00020},
	abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
	urldate = {2023-06-27},
	publisher = {arXiv},
	author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
	month = feb,
	year = {2021},
	note = {arXiv:2103.00020 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{zheng_judging_2023,
	title = {Judging {LLM}-as-a-judge with {MT}-{Bench} and {Chatbot} {Arena}},
	url = {http://arxiv.org/abs/2306.05685},
	abstract = {Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, such as position and verbosity biases and limited reasoning ability, and propose solutions to migrate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80\% agreement, the same level of agreement between humans. Hence, LLM-as-ajudge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA/Vicuna. We will publicly release 80 MT-bench questions, 3K expert votes, and 30K conversations with human preferences from Chatbot Arena 2.},
	language = {en},
	urldate = {2023-06-27},
	publisher = {arXiv},
	author = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric P. and Zhang, Hao and Gonzalez, Joseph E. and Stoica, Ion},
	month = jun,
	year = {2023},
	note = {arXiv:2306.05685 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{noauthor_vicuna_nodate,
	title = {Vicuna: {An} {Open}-{Source} {Chatbot} {Impressing} {GPT}-4 with 90\%* {ChatGPT} {Quality} {\textbar} {LMSYS} {Org}},
	shorttitle = {Vicuna},
	url = {https://lmsys.org/blog/2023-03-30-vicuna},
	abstract = {{\textless}p{\textgreater}We introduce Vicuna-13B, an open-source chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT. Preliminary evaluation ...},
	language = {en},
	urldate = {2023-06-27},
}

@article{atchade_limit_2010,
	title = {Limit theorems for some adaptive {MCMC} algorithms with subgeometric kernels},
	volume = {16},
	issn = {1350-7265},
	url = {https://www.jstor.org/stable/20680214},
	abstract = {This paper deals with the ergodicity (convergence of the marginals) and the law of large numbers for adaptive MCMC algorithms built from transition kernels that are not necessarily geometrically ergodic. We develop a number of results that significantly broaden the class of adaptive MCMC algorithms for which rigorous analysis is now possible. As an example, we give a detailed analysis of the adaptive Metropolis algorithm of Haario et al. [Bernoulli 7 (2001) 223-242] when the target distribution is subexponential in the tails.},
	number = {1},
	urldate = {2023-06-15},
	journal = {Bernoulli},
	author = {AtchadÃ©, Yves and Fort, Gersende},
	year = {2010},
	note = {Publisher: [Bernoulli Society for Mathematical Statistics and Probability, International Statistical Institute (ISI)]},
	pages = {116--154},
}

@article{roberts_coupling_2005,
	title = {Coupling and {Ergodicity} of {Adaptive} {MCMC}},
	abstract = {We consider basic ergodicity properties of adaptive MCMC algorithms under minimal assumptions, using coupling constructions. We prove convergence in distribution and a weak law of large numbers. We also give counter-examples to demonstrate that the assumptions we make are not redundant.},
	language = {en},
	author = {Roberts, Gareth O and Rosenthal, Jeï¬rey S},
	month = mar,
	year = {2005},
}

@article{andrieu_ergodicity_2006,
	title = {On the ergodicity properties of some adaptive {MCMC} algorithms},
	volume = {16},
	issn = {1050-5164, 2168-8737},
	url = {https://projecteuclid.org/journals/annals-of-applied-probability/volume-16/issue-3/On-the-ergodicity-properties-of-some-adaptive-MCMC-algorithms/10.1214/105051606000000286.full},
	doi = {10.1214/105051606000000286},
	abstract = {In this paper we study the ergodicity properties of some adaptive Markov chain Monte Carlo algorithms (MCMC) that have been recently proposed in the literature. We prove that under a set of verifiable conditions, ergodic averages calculated from the output of a so-called adaptive MCMC sampler converge to the required value and can even, under more stringent assumptions, satisfy a central limit theorem. We prove that the conditions required are satisfied for the independent MetropolisâHastings algorithm and the random walk Metropolis algorithm with symmetric increments. Finally, we propose an application of these results to the case where the proposal distribution of the MetropolisâHastings update is a mixture of distributions from a curved exponential family.},
	number = {3},
	urldate = {2023-06-15},
	journal = {The Annals of Applied Probability},
	author = {Andrieu, Christophe and Moulines, Ãric},
	month = aug,
	year = {2006},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {60J27, 60J35, 65C05, 65C40, 93E35, Adaptive Markov chain Monte Carlo, MetropolisâHastings algorithm, Poisson method, martingale, randomly varying truncation, self-tuning algorithm, state-dependent noise, stochastic approximation},
	pages = {1462--1505},
}

@article{gilks_adaptive_1998,
	title = {Adaptive {Markov} {Chain} {Monte} {Carlo} through {Regeneration}},
	volume = {93},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/2669848},
	doi = {10.2307/2669848},
	abstract = {Markov chain Monte Carlo (MCMC) is used for evaluating expectations of functions of interest under a target distribution Ï. This is done by calculating averages over the sample path of a Markov chain having Ï as its stationary distribution. For computational efficiency, the Markov chain should be rapidly mixing. This sometimes can be achieved only by careful design of the transition kernel of the chain, on the basis of a detailed preliminary exploratory analysis of Ï. An alternative approach might be to allow the transition kernel to adapt whenever new features of Ï are encountered during the MCMC run. However, if such adaptation occurs infinitely often, then the stationary distribution of the chain may be disturbed. We describe a framework, based on the concept of Markov chain regeneration, which allows adaptation to occur infinitely often but does not disturb the stationary distribution of the chain or the consistency of sample path averages.},
	number = {443},
	urldate = {2023-06-15},
	journal = {Journal of the American Statistical Association},
	author = {Gilks, Walter R. and Roberts, Gareth O. and Sahu, Sujit K.},
	year = {1998},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {1045--1054},
}

@article{rossky_brownian_2008,
	title = {Brownian dynamics as smart {Monte} {Carlo} simulation},
	volume = {69},
	issn = {0021-9606},
	url = {https://doi.org/10.1063/1.436415},
	doi = {10.1063/1.436415},
	abstract = {A new Monte Carlo simulation procedure is developed which is expected to produce more rapid convergence than the standard Metropolis method. The trial particle moves are chosen in accord with a Brownian dynamics algorithm rather than at random. For two model systems, a string of point masses joined by harmonic springs and a cluster of charged soft spheres, the new procedure is compared to the standard one and shown to manifest a more rapid convergence rate for some important energetic and structural properties.},
	number = {10},
	urldate = {2023-06-15},
	journal = {The Journal of Chemical Physics},
	author = {Rossky, P. J. and Doll, J. D. and Friedman, H. L.},
	month = aug,
	year = {2008},
	pages = {4628--4633},
}

@article{grenander_representations_1994,
	title = {Representations of {Knowledge} in {Complex} {Systems}},
	volume = {56},
	issn = {0035-9246},
	url = {https://www.jstor.org/stable/2346184},
	abstract = {Modern sensor technologies, especially in biomedicine, produce increasingly detailed and informative image ensembles, many extremely complex. It will be argued that pattern theory can supply mathematical representations of subject-matter knowledge that can be used as a basis for algorithmic `understanding' of such pictures. After a brief survey of the basic principles of pattern theory we shall illustrate them by an application to a concrete situation: high magnification (greater than 15 000 Ã) electron micrographs of cardiac muscle cells. The aim is to build algorithms for automatic hypothesis formation concerning the number, location, orientation and shape of mitochondria and membranes. For this we construct a pattern theoretic model in the form of a prior probability measure on the space of configurations describing these hypotheses. This measure is synthesized by solving sequentially a jump-diffusion equation of generalized Langevin form. The jumps occur for the creation-annihilation of hypotheses, corresponding to a jump from one continuum to another in configuration (hypothesis) space. These continua (subhypotheses) are expressed in terms of products of low dimensional Lie groups acting on the generators of a template. We use a modified Bayes approach to obtain the hypothesis formation, also organized by solving a generalized Langevin equation. To justify this it is shown that the resulting jump-diffusion process is ergodic so that the solution converges to the desired probability measure. To speed up the convergence we reduce the computation of the drift term in the stochastic differential equation analytically to a curvilinear integral, with the random term computed almost instantaneously. The algorithms thus obtained are implemented, both for mitochondria and membranes, on a 4000 processor parallel machine. Photographs of the graphics illustrate how automatic hypothesis formation is achieved. This approach is applied to deformable neuroanatomical atlases and tracking recognition from narrow band and high resolution sensor arrays.},
	number = {4},
	urldate = {2023-06-15},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Grenander, Ulf and Miller, Michael I.},
	year = {1994},
	note = {Publisher: [Royal Statistical Society, Wiley]},
	pages = {549--603},
}

@article{andrieu_tutorial_2008,
	title = {A tutorial on adaptive {MCMC}},
	volume = {18},
	issn = {0960-3174, 1573-1375},
	url = {http://link.springer.com/10.1007/s11222-008-9110-y},
	doi = {10.1007/s11222-008-9110-y},
	abstract = {We review adaptive Markov chain Monte Carlo algorithms (MCMC) as a mean to optimise their performance. Using simple toy examples we review their theoretical underpinnings, and in particular show why adaptive MCMC algorithms might fail when some fundamental properties are not satisï¬ed. This leads to guidelines concerning the design of correct algorithms. We then review criteria and the useful framework of stochastic approximation, which allows one to systematically optimise generally used criteria, but also analyse the properties of adaptive MCMC algorithms. We then propose a series of novel adaptive algorithms which prove to be robust and reliable in practice. These algorithms are applied to artiï¬cial and high dimensional scenarios, but also to the classic mine disaster dataset inference problem.},
	language = {en},
	number = {4},
	urldate = {2023-06-15},
	journal = {Statistics and Computing},
	author = {Andrieu, Christophe and Thoms, Johannes},
	month = dec,
	year = {2008},
	pages = {343--373},
}

@article{takaishi_bayesian_2010,
	title = {Bayesian inference with an adaptive proposal density for {GARCH} models},
	volume = {221},
	issn = {1742-6596},
	url = {https://iopscience.iop.org/article/10.1088/1742-6596/221/1/012011},
	doi = {10.1088/1742-6596/221/1/012011},
	abstract = {We perform the Bayesian inference of a GARCH model by the Metropolis-Hastings algorithm with an adaptive proposal density. The adaptive proposal density is assumed to be the Studentâs t-distribution and the distribution parameters are evaluated by using the data sampled during the simulation. We apply the method for the QGARCH model which is one of asymmetric GARCH models and make empirical studies for Nikkei 225, DAX and Hang indexes. We ï¬nd that autocorrelation times from our method are very small, thus the method is very eï¬cient for generating uncorrelated Monte Carlo data. The results from the QGARCH model show that all the three indexes show the leverage eï¬ect, i.e. the volatility is high after negative observations.},
	language = {en},
	urldate = {2023-06-15},
	journal = {Journal of Physics: Conference Series},
	author = {Takaishi, Tetsuya},
	month = apr,
	year = {2010},
	pages = {012011},
}

@article{atchade_wang-landau_nodate,
	title = {{THE} {WANG}-{LANDAU} {ALGORITHM} {IN} {GENERAL} {STATE} {SPACES}: {APPLICATIONS} {AND} {CONVERGENCE} {ANALYSIS}},
	abstract = {The Wang-Landau algorithm (Wang and Landau (2001)) is a recent Monte Carlo method that has generated much interest in the Physics literature due to some spectacular simulation performances. The objective of this paper is two-fold. First, we show that the algorithm can be naturally extended to more general state spaces and used to improve on Markov Chain Monte Carlo schemes of more interest in Statistics. In a second part, we study asymptotic behaviors of the algorithm. We show that with an appropriate choice of the step-size, the algorithm is consistent and a strong law of large numbers holds under some fairly mild conditions. We have also shown by simulations the potential advantage of the WL algorithm for problems in Bayesian inference.},
	language = {en},
	author = {Atchade, Yves F and Liu, Jun S},
}

@article{atchade_adaptive_2006,
	title = {An {Adaptive} {Version} for the {Metropolis} {Adjusted} {Langevin} {Algorithm} with a {Truncated} {Drift}},
	volume = {8},
	issn = {1387-5841, 1573-7713},
	url = {http://link.springer.com/10.1007/s11009-006-8550-0},
	doi = {10.1007/s11009-006-8550-0},
	abstract = {This paper proposes an adaptive version for the Metropolis adjusted Langevin algorithm with a truncated drift (T-MALA). The scale parameter and the covariance matrix of the proposal kernel of the algorithm are simultaneously and recursively updated in order to reach the optimal acceptance rate of 0.574 (see Roberts and Rosenthal (2001)) and to estimate and use the correlation structure of the target distribution. We develop some convergence results for the algorithm. A simulation example is presented.},
	language = {en},
	number = {2},
	urldate = {2023-06-15},
	journal = {Methodology and Computing in Applied Probability},
	author = {AtchadÃ©, Yves F.},
	month = jun,
	year = {2006},
	pages = {235--254},
}

@article{haario_adaptive_2001,
	title = {An {Adaptive} {Metropolis} {Algorithm}},
	volume = {7},
	issn = {1350-7265},
	url = {https://www.jstor.org/stable/3318737},
	doi = {10.2307/3318737},
	abstract = {A proper choice of a proposal distribution for Markov chain Monte Carlo methods, for example for the Metropolis-Hastings algorithm, is well known to be a crucial factor for the convergence of the algorithm. In this paper we introduce an adaptive Metropolis (AM) algorithm, where the Gaussian proposal distribution is updated along the process using the full information cumulated so far. Due to the adaptive nature of the process, the AM algorithm is non-Markovian, but we establish here that it has the correct ergodic properties. We also include the results of our numerical tests, which indicate that the AM algorithm competes well with traditional Metropolis-Hastings algorithms, and demonstrate that the AM algorithm is easy to use in practical computation.},
	number = {2},
	urldate = {2023-06-15},
	journal = {Bernoulli},
	author = {Haario, Heikki and Saksman, Eero and Tamminen, Johanna},
	year = {2001},
	note = {Publisher: International Statistical Institute (ISI) and Bernoulli Society for Mathematical Statistics and Probability},
	pages = {223--242},
}

@misc{roberts_introduction_2011,
	title = {An introduction to adaptive {MCMC}},
	url = {https://warwick.ac.uk/fac/sci/maths/research/miraw/days/montecarlo/abstracts/adaptivemiraw11_roberts.pdf},
	language = {en},
	author = {Roberts, Gareth},
	month = mar,
	year = {2011},
}

@article{doucett_rao-blackwellised_2000,
	title = {Rao-{Blackwellised} {Particle} {Filtering} for {Dynamic} {Bayesian} {Networks}},
	abstract = {Particle filters (PFs) are powerful samplingÂ­ based inference/learning algorithms for dynamic Bayesian networks (DBNs). They allow us to treat, in a principled way, any type of probabilÂ­ ity distribution, nonlinearity and non-stationarity. They have appeared in several fields under such names as "condensation", "sequential Monte Carlo" and "survival of the fittest". In this paÂ­ per, we show how we can exploit the structure of the DBN to increase the efficiency of partiÂ­ cle filtering, using a technique known as RaoÂ­ Blackwellisation. Essentially, this samples some of the variables, and marginalizes out the rest exactly, using the Kalman filter, HMM filter, junction tree algorithm, or any other finite diÂ­ mensional optimal filter. We show that RaoÂ­ Blackwellised particle filters (RBPFs) lead to more accurate estimates than standard PFs. We demonstrate RBPFs on two problems, namely non-stationary online regression with radial baÂ­ sis function networks and robot localization and map building. We also discuss other potential apÂ­ plication areas and provide references to some fiÂ­ nite dimensional optimal filters.},
	language = {en},
	author = {Doucett, Arnaud and de Freitast, Nando and Murphyt, Kevin and Russent, Stuart},
	year = {2000},
}

@article{athayde_forecasting_2022,
	title = {Forecasting {Covid}-19 in the {United} {Kingdom}: {A} dynamic {SIRD} model},
	volume = {17},
	issn = {1932-6203},
	shorttitle = {Forecasting {Covid}-19 in the {United} {Kingdom}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9365164/},
	doi = {10.1371/journal.pone.0271577},
	abstract = {Making use of a state space framework, we present a stochastic generalization of the SIRD model, where the mortality, infection, and underreporting rates change over time. A new format to the errors in the Susceptible-Infected-Recovered-Dead compartments is also presented, that permits reinfection. The estimated trajectories and (out-of-sample) forecasts of all these variables are presented with their confidence intervals. The model only uses as inputs the number of reported cases and deaths, and was applied for the UK from April, 2020 to Sep, 2021 (daily data). The estimated infection rate has shown a trajectory in waves very compatible with the emergence of new variants and adopted social measures. The estimated mortality rate has shown a significant descendant behaviour in 2021, which we attribute to the vaccination program, and the estimated underreporting rate has been considerably volatile, with a downward tendency, implying that, on average, more people are testing than in the beginning of the pandemic. The evolution of the proportions of the population divided into susceptible, infected, recovered and dead groups are also shown with their confidence intervals and forecast, along with an estimation of the amount of reinfection that, according to our model, has become quite significant in 2021. Finally, the estimated trajectory of the effective reproduction rate has proven to be very compatible with the real number of cases and deaths. Its forecasts with confident intervals are also presented.},
	number = {8},
	urldate = {2023-05-31},
	journal = {PLoS ONE},
	author = {Athayde, Gustavo M. and Alencar, Airlane P.},
	month = aug,
	year = {2022},
	pmid = {35947603},
	pmcid = {PMC9365164},
	pages = {e0271577},
}

@article{kitagawa_non-gaussian_1987,
	title = {Non-{Gaussian} {State}-{Space} {Modeling} of {Nonstationary} {Time} {Series}},
	volume = {82},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/2289375},
	doi = {10.2307/2289375},
	abstract = {A non-Gaussian state-space approach to the modeling of nonstationary time series is shown. The model is expressed in state-space form, where the system noise and the observational noise are not necessarily Gaussian. Recursive formulas of prediction, filtering, and smoothing for the state estimation and identification of the non-Gaussian state-space model are given. Also given is a numerical method based on piecewise linear approximation to the density functions for realizing these formulas. Significant merits of non-Gaussian modeling and the wide range of applicability of the method are illustrated by some numerical examples. A typical application of this non-Gaussian modeling is the smoothing of a time series that has mean value function with both abrupt and gradual changes. Simple Gaussian state-space modeling is not adequate for this situation. Here the model with small system noise variance cannot detect jump, whereas the one with large system noise variance yields unfavorable wiggle. To work out this problem within the ordinary linear Gaussian model framework, sophisticated treatment of outliers is required. But by the use of an appropriate non-Gaussian model for system noise, it is possible to reproduce both abrupt and gradual change of the mean without any special treatment. Nonstandard observations such as the ones distributed as non-Gaussian distribution can be easily treated by the direct modeling of an observational scheme. Smoothing of a transformed series such as a log periodogram can be treated by this method. Outliers in the observations can be treated as well by using heavy-tailed distribution for observational noise density. The algorithms herein can be easily extended to a wider class of models. As an example, the smoothing of nonhomogeneous binomial mean function is shown, where the observation is distributed according to a discrete random variable. Extension to a nonlinear system is also straightforward.},
	number = {400},
	urldate = {2023-05-31},
	journal = {Journal of the American Statistical Association},
	author = {Kitagawa, Genshiro},
	year = {1987},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {1032--1041},
}

@article{kalman_new_1960,
	title = {A {New} {Approach} to {Linear} {Filtering} and {Prediction} {Problems}},
	volume = {82},
	issn = {0021-9223},
	url = {https://doi.org/10.1115/1.3662552},
	doi = {10.1115/1.3662552},
	abstract = {The classical filtering and prediction problem is re-examined using the Bode-Shannon representation of random processes and the âstate-transitionâ method of analysis of dynamic systems. New results are: (1) The formulation and methods of solution of the problem apply without modification to stationary and nonstationary statistics and to growing-memory and infinite-memory filters. (2) A nonlinear difference (or differential) equation is derived for the covariance matrix of the optimal estimation error. From the solution of this equation the co-efficients of the difference (or differential) equation of the optimal linear filter are obtained without further calculations. (3) The filtering problem is shown to be the dual of the noise-free regulator problem. The new method developed here is applied to two well-known problems, confirming and extending earlier results. The discussion is largely self-contained and proceeds from first principles; basic concepts of the theory of random processes are reviewed in the Appendix.},
	number = {1},
	urldate = {2023-05-31},
	journal = {Journal of Basic Engineering},
	author = {Kalman, R. E.},
	month = mar,
	year = {1960},
	pages = {35--45},
}

@inproceedings{kurihara_bayesian_2001,
	title = {Bayesian on-line learning: a sequential {Monte} {Carlo} with importance resampling},
	shorttitle = {Bayesian on-line learning},
	doi = {10.1109/NNSP.2001.943121},
	abstract = {A Bayesian online learning scheme with sequential Monte Carlo incorporating importance resampling is proposed. The proposed scheme adjusts not only parameters for data fitting but also adjusts hyperparameters online so that the scheme attempts to avoid overfitting in an adaptive manner. One of the advantages of the scheme is the fact that it can adapt to environmental changes, i.e., it can perform learning, even when the underlying input-output relationship varies over time. The scheme is tested against simple examples and is shown to be functional.},
	booktitle = {Neural {Networks} for {Signal} {Processing} {XI}: {Proceedings} of the 2001 {IEEE} {Signal} {Processing} {Society} {Workshop} ({IEEE} {Cat}. {No}.{01TH8584})},
	author = {Kurihara, T. and Nakada, Y. and Yosui, K. and Matsumoto, T.},
	month = sep,
	year = {2001},
	note = {ISSN: 1089-3555},
	keywords = {Bayesian methods, Integral equations, Monte Carlo methods, Nonlinear equations, Sequential analysis, State estimation, Testing, Training data, Uncertainty},
	pages = {163--172},
}

@article{blei_variational_2017,
	title = {Variational {Inference}: {A} {Review} for {Statisticians}},
	volume = {112},
	issn = {0162-1459, 1537-274X},
	shorttitle = {Variational {Inference}},
	url = {http://arxiv.org/abs/1601.00670},
	doi = {10.1080/01621459.2017.1285773},
	abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.},
	number = {518},
	urldate = {2023-05-31},
	journal = {Journal of the American Statistical Association},
	author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
	month = apr,
	year = {2017},
	note = {arXiv:1601.00670 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Computation, Statistics - Machine Learning},
	pages = {859--877},
}

@misc{daviet_inference_2018,
	title = {Inference with {Hamiltonian} {Sequential} {Monte} {Carlo} {Simulators}},
	url = {http://arxiv.org/abs/1812.07978},
	abstract = {The paper proposes a new Monte-Carlo simulator combining the advantages of Sequential Monte Carlo simulators and Hamiltonian Monte Carlo simulators. The result is a method that is robust to multimodality and complex shapes to use for inference in presence of diï¬cult likelihoods or target functions. Several examples are provided.},
	language = {en},
	urldate = {2023-05-31},
	publisher = {arXiv},
	author = {Daviet, Remi},
	month = dec,
	year = {2018},
	note = {arXiv:1812.07978 [stat]},
	keywords = {Statistics - Computation},
}

@article{pitt_filtering_1997,
	title = {Filtering via {Simulation}: {Auxiliary} {Particle} {Filters}},
	volume = {94},
	shorttitle = {Filtering via {Simulation}},
	doi = {10.2307/2670179},
	abstract = {This paper analyses the recently suggested particle approach to filtering time series. We suggest that the algorithm is not robust to outliers for two reasons: the design of the simulators and the use of the discrete support to represent the sequentially updating prior distribution. Both problems are tackled in this paper. We believe we have largely solved the first problem and have reduced the order of magnitude of the second. In addition we introduce the idea of stratification into the particle filter which allows us to perform on-line Bayesian calculations about the parameters which index the models and maximum likelihood estimation. The new methods are illustrated by using a stochastic volatility model and a time series model of angles. Some key words: Filtering, Markov chain Monte Carlo, Particle filter, Simulation, SIR, State space. 1 1},
	journal = {Journal of the American Statistical Association},
	author = {Pitt, Michael and Shephard, Neil},
	month = nov,
	year = {1997},
}

@inproceedings{gordon_novel_1993,
	title = {Novel approach to nonlinear/non-{Gaussian} {Bayesian} state estimation},
	volume = {140},
	url = {https://digital-library.theiet.org/content/journals/10.1049/ip-f-2.1993.0015},
	doi = {10.1049/ip-f-2.1993.0015},
	abstract = {An algorithm, the bootstrap filter, is proposed for implementing recursive Bayesian filters. The required density of the state vector is represented as a set of random samples, which are updated and propagated by the algorithm. The method is not restricted by assumptions of linear- ity or Gaussian noise: it may be applied to any state transition or measurement model. A simula- tion example of the bearings only tracking problem is presented. This simulation includes schemes for improving the efficiency of the basic algorithm. For this example, the performance of the bootstrap filter is greatly superior to the standard extended Kalman filter.},
	language = {en},
	urldate = {2023-05-30},
	booktitle = {{IEE} {Proceedings} {F} {Radar} and {Signal} {Processing}},
	author = {Gordon, N.J. and Salmond, D.J. and Smith, A.F.M.},
	year = {1993},
	note = {ISSN: 0956375X
Issue: 2
Journal Abbreviation: IEE Proc. F Radar Signal Process. UK},
	pages = {107},
}

@book{halimeh_neural_2019,
	title = {Neural {Networks} {Sequential} {Training} {Using} {Variational} {Gaussian} {Particle} {Filter}},
	author = {Halimeh, Modar and Brendel, Andreas and Kellermann, Walter},
	month = may,
	year = {2019},
	doi = {10.1109/ICASSP.2019.8683886},
	note = {Pages: 3006},
}

@article{abbasi_robust_2020,
	title = {A {Robust} and {Accurate} {Particle} {Filter}-{Based} {Pupil} {Detection} {Method} for {Big} {Datasets} of {Eye} {Video}},
	volume = {18},
	issn = {1572-9184},
	url = {https://doi.org/10.1007/s10723-019-09502-1},
	doi = {10.1007/s10723-019-09502-1},
	abstract = {Accurate detection of pupil position in successive frames of eye videos is finding applications in many areas including assistive systems and E-learning. Processing the big datasets of eye videos in such systems requires robust and fast eye-tracking algorithms that can predict the position of eye pupil in consecutive video frames. As a major technique, particle filters provide adequate speed but have a low detection rate. To solve this problem, the present paper suggests the use of genetic algorithms in the sampling step of the particle filter technique. As a result, in each frame, the variety of particles required for predicting the pupil position in the next video frame is maintained and their uniformity is reduced. Finally, the speed and detection rate of the proposed method, as well as the basic particle filter method in predicting the pupil position in video frames are calculated and compared for various populations. The experimental results indicate that, in comparison with the basic particle filter algorithm, the proposed algorithm detects the pupil more accurately and in a shorter time. Also, by achieving an average detection rate of 79.89\% in estimation of the pupil center with an error of five pixels on a variety of eye videos with different situations of occlusion and illumination, not only the robustness of the proposed method is assessed but also its superiority to the state-of-the-art methods is evinced.},
	language = {en},
	number = {2},
	urldate = {2023-05-30},
	journal = {Journal of Grid Computing},
	author = {Abbasi, Mahdi and Khosravi, Mohammad R.},
	month = jun,
	year = {2020},
	keywords = {Big dataset, Detection rate, Eye tracking, Genetic algorithm, Particle filter, Pupil},
	pages = {305--325},
}

@article{andrieu_particle_2010,
	title = {Particle {Markov} {Chain} {Monte} {Carlo} {Methods}},
	volume = {72},
	issn = {1369-7412, 1467-9868},
	url = {https://academic.oup.com/jrsssb/article/72/3/269/7076437},
	doi = {10.1111/j.1467-9868.2009.00736.x},
	abstract = {Markov chain Monte Carlo and sequential Monte Carlo methods have emerged as the two main tools to sample from high dimensional probability distributions. Although asymptotic convergence of Markov chain Monte Carlo algorithms is ensured under weak assumptions, the performance of these algorithms is unreliable when the proposal distributions that are used to explore the space are poorly chosen and/or if highly correlated variables are updated independently. We show here how it is possible to build efï¬cient high dimensional proposal distributions by using sequential Monte Carlo methods. This allows us not only to improve over standard Markov chain Monte Carlo schemes but also to make Bayesian inference feasible for a large class of statistical models where this was not previously so. We demonstrate these algorithms on a non-linear state space model and a LÃ©vy-driven stochastic volatility model.},
	language = {en},
	number = {3},
	urldate = {2023-05-30},
	journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
	author = {Andrieu, Christophe and Doucet, Arnaud and Holenstein, Roman},
	month = jun,
	year = {2010},
	pages = {269--342},
}

@article{dobigeon_nonlinear_2014,
	title = {Nonlinear unmixing of hyperspectral images: models and algorithms},
	volume = {31},
	issn = {1053-5888},
	shorttitle = {Nonlinear unmixing of hyperspectral images},
	url = {http://arxiv.org/abs/1304.1875},
	doi = {10.1109/MSP.2013.2279274},
	abstract = {When considering the problem of unmixing hyperspectral images, most of the literature in the geoscience and image processing areas relies on the widely used linear mixing model (LMM). However, the LMM may be not valid and other nonlinear models need to be considered, for instance, when there are multi-scattering effects or intimate interactions. Consequently, over the last few years, several signiï¬cant contributions have been proposed to overcome the limitations inherent in the LMM. In this paper, we present an overview of recent advances in nonlinear unmixing modeling.},
	language = {en},
	number = {1},
	urldate = {2023-05-30},
	journal = {IEEE Signal Processing Magazine},
	author = {Dobigeon, Nicolas and Tourneret, Jean-Yves and Richard, CÃ©dric and Bermudez, JosÃ© C. M. and McLaughlin, Stephen and Hero, Alfred O.},
	month = jan,
	year = {2014},
	note = {arXiv:1304.1875 [physics, stat]},
	keywords = {Physics - Data Analysis, Statistics and Probability, Statistics - Applications, Statistics - Machine Learning, Statistics - Methodology},
	pages = {82--94},
}

@article{cappe_overview_2007,
	title = {An {Overview} of {Existing} {Methods} and {Recent} {Advances} in {Sequential} {Monte} {Carlo}},
	volume = {95},
	doi = {10.1109/JPROC.2007.893250},
	abstract = {It is now over a decade since the pioneering contribution of Gordon (1993), which is commonly regarded as the first instance of modern sequential Monte Carlo (SMC) approaches. Initially focussed on applications to tracking and vision, these techniques are now very widespread and have had a significant impact in virtually all areas of signal and image processing concerned with Bayesian dynamical models. This paper is intended to serve both as an introduction to SMC algorithms for nonspecialists and as a reference to recent contributions in domains where the techniques are still under significant development, including smoothing, estimation of fixed parameters and use of SMC methods beyond the standard filtering contexts.},
	journal = {Proceedings of the IEEE},
	author = {Cappe, Olivier and Godsill, S.J. and Moulines, Eric},
	month = jun,
	year = {2007},
	pages = {899--924},
}

@article{nguyen_efficient_2016,
	title = {Efficient {Sequential} {Monte}-{Carlo} {Samplers} for {Bayesian} {Inference}},
	volume = {64},
	issn = {1941-0476},
	doi = {10.1109/TSP.2015.2504342},
	abstract = {In many problems, complex non-Gaussian and/or nonlinear models are required to accurately describe a physical system of interest. In such cases, Monte-Carlo algorithms are remarkably flexible and extremely powerful approaches to solve such inference problems. However, in the presence of a high-dimensional and/or multimodal posterior distribution, it is widely documented that standard Monte-Carlo techniques could lead to poor performance. In this paper, the study is focused on a Sequential Monte-Carlo (SMC) sampler framework, a more robust and efficient Monte-Carlo algorithm. Although this approach presents many advantages over traditional Monte-Carlo methods, the potential of this emergent technique is, however, largely underexploited in signal processing. In this paper, we aim at proposing some novel strategies to improve the efficiency and facilitate practical implementation of the SMC sampler. First, we propose an automatic and adaptive strategy that selects the sequence of distributions within the SMC sampler that minimizes the asymptotic variance of the estimator of the posterior normalization constant. The second original contribution we present improves the global efficiency of the SMC sampler by introducing a novel correction mechanism that allows the use of the particles generated through all of the iterations of the algorithm (instead of only particles from the last iteration). This is a significant contribution as it removes the need to discard a large portion of the samples obtained, as is standard in standard SMC methods. This will improve estimation performance in practical settings where the computational budget is important to consider.},
	number = {5},
	journal = {IEEE Transactions on Signal Processing},
	author = {Nguyen, Thi Le Thu and Septier, FranÃ§ois and Peters, Gareth W. and Delignon, Yves},
	month = mar,
	year = {2016},
	note = {Conference Name: IEEE Transactions on Signal Processing},
	keywords = {Bayes methods, Bayesian inference, Kernel, Markov processes, Monte Carlo methods, Signal processing, Signal processing algorithms, Standards, complex models, sequential Monte Carlo sampler},
	pages = {1305--1319},
}

@article{buchholz_adaptive_2021,
	title = {Adaptive {Tuning} of {Hamiltonian} {Monte} {Carlo} {Within} {Sequential} {Monte} {Carlo}},
	volume = {16},
	issn = {1936-0975, 1931-6690},
	url = {https://projecteuclid.org/journals/bayesian-analysis/volume-16/issue-3/Adaptive-Tuning-of-Hamiltonian-Monte-Carlo-Within-Sequential-Monte-Carlo/10.1214/20-BA1222.full},
	doi = {10.1214/20-BA1222},
	abstract = {Sequential Monte Carlo (SMC) samplers are an alternative to MCMC for Bayesian computation. However, their performance depends strongly on the Markov kernels used to rejuvenate particles. We discuss how to calibrate automatically (using the current particles) Hamiltonian Monte Carlo kernels within SMC. To do so, we build upon the adaptive SMC approach of Fearnhead and Taylor (2013), and we also suggest alternative methods. We illustrate the advantages of using HMC kernels within an SMC sampler via an extensive numerical study.},
	number = {3},
	urldate = {2023-05-30},
	journal = {Bayesian Analysis},
	author = {Buchholz, Alexander and Chopin, Nicolas and Jacob, Pierre E.},
	month = sep,
	year = {2021},
	note = {Publisher: International Society for Bayesian Analysis},
	keywords = {62F15, 65C05, Hamiltonian Monte Carlo, sequential Monte Carlo},
	pages = {745--771},
}

@book{chopin_introduction_2020,
	address = {Cham},
	series = {Springer {Series} in {Statistics}},
	title = {An {Introduction} to {Sequential} {Monte} {Carlo}},
	isbn = {978-3-030-47844-5 978-3-030-47845-2},
	url = {https://link.springer.com/10.1007/978-3-030-47845-2},
	language = {en},
	urldate = {2023-05-30},
	publisher = {Springer International Publishing},
	author = {Chopin, Nicolas and Papaspiliopoulos, Omiros},
	year = {2020},
	doi = {10.1007/978-3-030-47845-2},
	keywords = {Bayesian inference, Feynman-Kac models, Hidden Markov models, Markov chain Monte Carlo, Particle filter, Sequential Monte Carlo, Sequential learning, State-space models, data-driven science, modeling and theory building},
}

@misc{noauthor_ieee_nodate,
	title = {{IEEE} {Xplore} {Full}-{Text} {PDF}:},
	url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9615956},
	urldate = {2023-05-30},
}

@article{abdessalem_automatic_2017,
	title = {Automatic {Kernel} {Selection} for {Gaussian} {Processes} {Regression} with {Approximate} {Bayesian} {Computation} and {Sequential} {Monte} {Carlo}},
	volume = {3},
	issn = {2297-3362},
	url = {https://www.frontiersin.org/articles/10.3389/fbuil.2017.00052},
	abstract = {The current work introduces a novel combination of two Bayesian tools, Gaussian Processes (GPs), and the use of the Approximate Bayesian Computation (ABC) algorithm for kernel selection and parameter estimation for machine learning applications. The combined methodology that this research article proposes and investigates offers the possibility to use different metrics and summary statistics of the kernels used for Bayesian regression. The presented work moves a step toward online, robust, consistent, and automated mechanism to formulate optimal kernels (or even mean functions) and their hyperparameters simultaneously offering confidence evaluation when these tools are used for mathematical or engineering problems such as structural health monitoring (SHM) and system identification (SI).},
	urldate = {2023-05-30},
	journal = {Frontiers in Built Environment},
	author = {Abdessalem, Anis Ben and Dervilis, Nikolaos and Wagg, David J. and Worden, Keith},
	year = {2017},
}

@article{zhang_sequential_2023,
	title = {Sequential {Gaussian} {Processes} for {Online} {Learning} of {Nonstationary} {Functions}},
	volume = {71},
	issn = {1053-587X, 1941-0476},
	url = {https://ieeexplore.ieee.org/document/10103648/},
	doi = {10.1109/TSP.2023.3267992},
	abstract = {We propose a sequential Monte Carlo algorithm to ï¬t inï¬nite mixtures of GPs that capture non-stationary behavior while allowing for online, distributed inference. Our approach empirically improves performance over state-of-the-art methods for online GP estimation in the presence of non-stationarity in time-series data. To demonstrate the utility of our proposed online Gaussian process mixture-of-experts approach in applied settings, we show that we can successfully implement an optimization algorithm using online Gaussian process bandits.},
	language = {en},
	urldate = {2023-05-30},
	journal = {IEEE Transactions on Signal Processing},
	author = {Zhang, Michael Minyi and Dumitrascu, Bianca and Williamson, Sinead A. and Engelhardt, Barbara E.},
	year = {2023},
	pages = {1539--1550},
}

@book{doucet_sequential_2001,
	address = {New York, NY},
	title = {Sequential {Monte} {Carlo} {Methods} in {Practice}},
	isbn = {978-1-4419-2887-0 978-1-4757-3437-9},
	url = {http://link.springer.com/10.1007/978-1-4757-3437-9},
	urldate = {2023-05-30},
	publisher = {Springer},
	editor = {Doucet, Arnaud and Freitas, Nando and Gordon, Neil},
	year = {2001},
	doi = {10.1007/978-1-4757-3437-9},
	keywords = {Likelihood, Monte Carlo Methods, Resampling, Statistical Models, artificial intelligence, bayesian statistics, calculus, data analysis, dynamic models, econometrics, machine learning, modeling, neural networks, statistics, time series analysis},
}

@misc{jj_allaire_parallel_2014,
	title = {Parallel {Distance} {Matrix} {Calculation} with {RcppParallel}},
	url = {https://gallery.rcpp.org/articles/parallel-distance-matrix/},
	urldate = {2023-05-29},
	author = {{JJ Allaire} and {Jim Bullard}},
	month = jul,
	year = {2014},
}

@misc{jj_allaire_computing_2014,
	title = {Computing an {Inner} {Product} with {RcppParallel}},
	url = {https://gallery.rcpp.org/articles/parallel-inner-product/},
	urldate = {2023-05-29},
	author = {{JJ Allaire}},
	month = jul,
	year = {2014},
}

@article{homan_no-u-turn_2014,
	title = {The {No}-{U}-{Turn} {Sampler}: {Adaptively} {Setting} {Path} {Lengths} in {Hamiltonian} {Monte} {Carlo}},
	abstract = {Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) algorithm that avoids the random walk behavior and sensitivity to correlated parameters that plague many MCMC methods by taking a series of steps informed by ï¬rst-order gradient information. These features allow it to converge to high-dimensional target distributions much more quickly than simpler methods such as random walk Metropolis or Gibbs sampling. However, HMCâs performance is highly sensitive to two user-speciï¬ed parameters: a step size and a desired number of steps L. In particular, if L is too small then the algorithm exhibits undesirable random walk behavior, while if L is too large the algorithm wastes computation. We introduce the No-U-Turn Sampler (NUTS), an extension to HMC that eliminates the need to set a number of steps L. NUTS uses a recursive algorithm to build a set of likely candidate points that spans a wide swath of the target distribution, stopping automatically when it starts to double back and retrace its steps. Empirically, NUTS performs at least as eï¬ciently as (and sometimes more eï¬ciently than) a well tuned standard HMC method, without requiring user intervention or costly tuning runs. We also derive a method for adapting the step size parameter on the ï¬y based on primal-dual averaging. NUTS can thus be used with no hand-tuning at all, making it suitable for applications such as BUGS-style automatic inference engines that require eï¬cient âturnkeyâ samplers.},
	language = {en},
	author = {Hoï¬man, Matthew D and Gelman, Andrew},
	month = apr,
	year = {2014},
}

@article{salvatier_probabilistic_2016,
	title = {Probabilistic programming in {Python} using {PyMC3}},
	volume = {2},
	issn = {2376-5992},
	url = {https://peerj.com/articles/cs-55},
	doi = {10.7717/peerj-cs.55},
	abstract = {Probabilistic programming allows for automatic Bayesian inference on user-defined probabilistic models. Recent advances in Markov chain Monte Carlo (MCMC) sampling allow inference on increasingly complex models. This class of MCMC, known as Hamiltonian Monte Carlo, requires gradient information which is often not readily available. PyMC3 is a new open source probabilistic programming framework written in Python that uses Theano to compute gradients via automatic differentiation as well as compile probabilistic programs on-the-fly to C for increased speed. Contrary to other probabilistic programming languages, PyMC3 allows model specification directly in Python code. The lack of a domain specific language allows for great flexibility and direct interaction with the model. This paper is a tutorial-style introduction to this software package.},
	language = {en},
	urldate = {2023-05-13},
	journal = {PeerJ Computer Science},
	author = {Salvatier, John and Wiecki, Thomas V. and Fonnesbeck, Christopher},
	month = apr,
	year = {2016},
	note = {Publisher: PeerJ Inc.},
	pages = {e55},
}

@article{haario_adaptive_2001,
	title = {An {Adaptive} {Metropolis} {Algorithm}},
	volume = {7},
	issn = {1350-7265},
	url = {https://www.jstor.org/stable/3318737},
	doi = {10.2307/3318737},
	abstract = {A proper choice of a proposal distribution for Markov chain Monte Carlo methods, for example for the Metropolis-Hastings algorithm, is well known to be a crucial factor for the convergence of the algorithm. In this paper we introduce an adaptive Metropolis (AM) algorithm, where the Gaussian proposal distribution is updated along the process using the full information cumulated so far. Due to the adaptive nature of the process, the AM algorithm is non-Markovian, but we establish here that it has the correct ergodic properties. We also include the results of our numerical tests, which indicate that the AM algorithm competes well with traditional Metropolis-Hastings algorithms, and demonstrate that the AM algorithm is easy to use in practical computation.},
	number = {2},
	urldate = {2023-05-05},
	journal = {Bernoulli},
	author = {Haario, Heikki and Saksman, Eero and Tamminen, Johanna},
	year = {2001},
	note = {Publisher: International Statistical Institute (ISI) and Bernoulli Society for Mathematical Statistics and Probability},
	pages = {223--242},
}

@misc{neal_mcmc_2011,
	title = {{MCMC} {Using} {Ensembles} of {States} for {Problems} with {Fast} and {Slow} {Variables} such as {Gaussian} {Process} {Regression}},
	url = {http://arxiv.org/abs/1101.0387},
	doi = {10.48550/arXiv.1101.0387},
	abstract = {I introduce a Markov chain Monte Carlo (MCMC) scheme in which sampling from a distribution with density pi(x) is done using updates operating on an "ensemble" of states. The current state x is first stochastically mapped to an ensemble, x{\textasciicircum}\{(1)\},...,x{\textasciicircum}\{(K)\}. This ensemble is then updated using MCMC updates that leave invariant a suitable ensemble density, rho(x{\textasciicircum}\{(1)\},...,x{\textasciicircum}\{(K)\}), defined in terms of pi(x{\textasciicircum}\{(i)\}) for i=1,...,K. Finally a single state is stochastically selected from the ensemble after these updates. Such ensemble MCMC updates can be useful when characteristics of pi and the ensemble permit pi(x{\textasciicircum}\{(i)\}) for all i in \{1,...,K\}, to be computed in less than K times the amount of computation time needed to compute pi(x) for a single x. One common situation of this type is when changes to some "fast" variables allow for quick re-computation of the density, whereas changes to other "slow" variables do not. Gaussian process regression models are an example of this sort of problem, with an overall scaling factor for covariances and the noise variance being fast variables. I show that ensemble MCMC for Gaussian process regression models can indeed substantially improve sampling performance. Finally, I discuss other possible applications of ensemble MCMC, and its relationship to the "multiple-try Metropolis" method of Liu, Liang, and Wong and the "multiset sampler" of Leman, Chen, and Lavine.},
	urldate = {2023-05-05},
	publisher = {arXiv},
	author = {Neal, Radford M.},
	month = jan,
	year = {2011},
	note = {arXiv:1101.0387 [stat]},
	keywords = {Statistics - Computation},
}

@article{liu_multiple-try_2000,
	title = {The {Multiple}-{Try} {Method} and {Local} {Optimization} in {Metropolis} {Sampling}},
	volume = {95},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/2669532},
	doi = {10.2307/2669532},
	abstract = {This article describes a new Metropolis-like transition rule, the multiple-try Metropolis, for Markov chain Monte Carlo (MCMC) simulations. By using this transition rule together with adaptive direction sampling, we propose a novel method for incorporating local optimization steps into a MCMC sampler in continuous state-space. Numerical studies show that the new method performs significantly better than the traditional Metropolis-Hastings (M-H) sampler. With minor tailoring in using the rule, the multiple-try method can also be exploited to achieve the effect of a griddy Gibbs sampler without having to bear with griddy approximations, and the effect of a hit-and-run algorithm without having to figure out the required conditional distribution in a random direction.},
	number = {449},
	urldate = {2023-05-05},
	journal = {Journal of the American Statistical Association},
	author = {Liu, Jun S. and Liang, Faming and Wong, Wing Hung},
	year = {2000},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {121--134},
}

@article{craiu_learn_2009,
	title = {Learn {From} {Thy} {Neighbor}: {Parallel}-{Chain} and {Regional} {Adaptive} {MCMC}},
	volume = {104},
	issn = {0162-1459},
	shorttitle = {Learn {From} {Thy} {Neighbor}},
	url = {https://doi.org/10.1198/jasa.2009.tm08393},
	doi = {10.1198/jasa.2009.tm08393},
	abstract = {Starting with the seminal paper of Haario, Saksman, and Tamminen (Haario, Saksman, and Tamminen 2001), a substantial amount of work has been done to validate adaptive Markov chain Monte Carlo algorithms. In this paper we focus on two practical aspects of adaptive Metropolis samplers. First, we draw attention to the deficient performance of standard adaptation when the target distribution is multimodal. We propose a parallel chain adaptation strategy that incorporates multiple Markov chains which are run in parallel. Second, we note that the current adaptive MCMC paradigm implicitly assumes that the adaptation is uniformly efficient on all regions of the state space. However, in many practical instances, different âoptimalâ kernels are needed in different regions of the state space. We propose here a regional adaptation algorithm in which we account for possible errors made in defining the adaptation regions. This corresponds to the more realistic case in which one does not know exactly the optimal regions for adaptation. The methods focus on the random walk Metropolis sampling algorithm but their scope is much wider. We provide theoretical justification for the two adaptive approaches using the existent theory build for adaptive Markov chain Monte Carlo. We illustrate the performance of the methods using simulations and analyze a mixture model for real data using an algorithm that combines the two approaches.},
	number = {488},
	urldate = {2023-05-04},
	journal = {Journal of the American Statistical Association},
	author = {Craiu, Radu V. and Rosenthal, Jeffrey and Yang, Chao},
	month = dec,
	year = {2009},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1198/jasa.2009.tm08393},
	keywords = {Adaptive Markov chain Monte Carlo, Metropolis sampling, Parallel chains, Random walk Metropolis sampling, Regional adaptation},
	pages = {1454--1466},
}

@article{solonen_efficient_2012,
	title = {Efficient {MCMC} for {Climate} {Model} {Parameter} {Estimation}: {Parallel} {Adaptive} {Chains} and {Early} {Rejection}},
	volume = {7},
	issn = {1936-0975, 1931-6690},
	shorttitle = {Efficient {MCMC} for {Climate} {Model} {Parameter} {Estimation}},
	url = {https://projecteuclid.org/journals/bayesian-analysis/volume-7/issue-3/Efficient-MCMC-for-Climate-Model-Parameter-Estimation--Parallel-Adaptive/10.1214/12-BA724.full},
	doi = {10.1214/12-BA724},
	abstract = {The emergence of Markov chain Monte Carlo (MCMC) methods has opened a way for Bayesian analysis of complex models. Running MCMC samplers typically requires thousands of model evaluations, which can exceed available computer resources when this evaluation is computationally intensive. We will discuss two generally applicable techniques to improve the efficiency of MCMC. First, we consider a parallel version of the adaptive MCMC algorithm of Haario et al. (2001), implementing the idea of inter-chain adaptation introduced by Craiu et al. (2009). Second, we present an early rejection (ER) approach, where model simulation is stopped as soon as one can conclude that the proposed parameter value will be rejected by the MCMC algorithm. This work is motivated by practical needs in estimating parameters of climate and Earth system models. These computationally intensive models involve non-linear expressions of the geophysical and biogeochemical processes of the Earth system. Modeling of these processes, especially those operating in scales smaller than the model grid, involves a number of specified parameters, or âtunablesâ. MCMC methods are applicable for estimation of these parameters, but they are computationally very demanding. Efficient MCMC variants are thus needed to obtain reliable results in reasonable time. Here we evaluate the computational gains attainable through parallel adaptive MCMC and Early Rejection using both simple examples and a realistic climate model.},
	number = {3},
	urldate = {2023-05-04},
	journal = {Bayesian Analysis},
	author = {Solonen, Antti and Ollinaho, Pirkka and Laine, Marko and Haario, Heikki and Tamminen, Johanna and JÃ¤rvinen, Heikki},
	month = sep,
	year = {2012},
	note = {Publisher: International Society for Bayesian Analysis},
	keywords = {Climate Models, Early Rejection, Parallel MCMC, adaptive MCMC},
	pages = {715--736},
}

@article{rosenthal_parallel_nodate,
	title = {Parallel computing and {Monte} {Carlo} algorithms},
	abstract = {We argue that Monte Carlo algorithms are ideally suited to parallel computing, and that âparallel Monte Carloâ should be more widely used. We consider a number of issues that arise, including dealing with slow or unreliable computers. We also discuss the possibilities of parallel Markov chain Monte Carlo. We illustrate our results with actual computer experiments.},
	language = {en},
	author = {Rosenthal, Jeï¬rey S},
}

@article{diaconis_markov_2008,
	title = {The {Markov} chain {Monte} {Carlo} revolution},
	volume = {46},
	issn = {0273-0979},
	url = {http://www.ams.org/journal-getitem?pii=S0273-0979-08-01238-X},
	doi = {10.1090/S0273-0979-08-01238-X},
	abstract = {The use of simulation for high dimensional intractable computations has revolutionized applied mathematics. Designing, improving and understanding the new tools leads to (and leans on) fascinating mathematics, from representation theory through micro-local analysis.},
	language = {en},
	number = {2},
	urldate = {2023-05-04},
	journal = {Bulletin of the American Mathematical Society},
	author = {Diaconis, Persi},
	month = nov,
	year = {2008},
	pages = {179--205},
}

@misc{geffner_variational_2022,
	title = {Variational {Inference} with {Locally} {Enhanced} {Bounds} for {Hierarchical} {Models}},
	url = {http://arxiv.org/abs/2203.04432},
	abstract = {Hierarchical models represent a challenging setting for inference algorithms. MCMC methods struggle to scale to large models with many local variables and observations, and variational inference (VI) may fail to provide accurate approximations due to the use of simple variational families. Some variational methods (e.g. importance weighted VI) integrate Monte Carlo methods to give better accuracy, but these tend to be unsuitable for hierarchical models, as they do not allow for subsampling and their performance tends to degrade for high dimensional models. We propose a new family of variational bounds for hierarchical models, based on the application of tightening methods (e.g. importance weighting) separately for each group of local random variables. We show that our approach naturally allows the use of subsampling to get unbiased gradients, and that it fully leverages the power of methods that build tighter lower bounds by applying them independently in lower dimensional spaces, leading to better results and more accurate posterior approximations than relevant baselines.},
	language = {en},
	urldate = {2023-05-04},
	publisher = {arXiv},
	author = {Geffner, Tomas and Domke, Justin},
	month = jul,
	year = {2022},
	note = {arXiv:2203.04432 [cs, stat]},
	keywords = {68T99, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{warden_speech_2018,
	title = {Speech {Commands}: {A} {Dataset} for {Limited}-{Vocabulary} {Speech} {Recognition}},
	shorttitle = {Speech {Commands}},
	url = {http://arxiv.org/abs/1804.03209},
	doi = {10.48550/arXiv.1804.03209},
	abstract = {Describes an audio dataset of spoken words designed to help train and evaluate keyword spotting systems. Discusses why this task is an interesting challenge, and why it requires a specialized dataset that is different from conventional datasets used for automatic speech recognition of full sentences. Suggests a methodology for reproducible and comparable accuracy metrics for this task. Describes how the data was collected and verified, what it contains, previous versions and properties. Concludes by reporting baseline results of models trained on this dataset.},
	urldate = {2023-05-02},
	publisher = {arXiv},
	author = {Warden, Pete},
	month = apr,
	year = {2018},
	note = {arXiv:1804.03209 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Human-Computer Interaction},
}

@article{jia_sequence_2021,
	title = {Sequence to point learning based on bidirectional dilated residual network for non-intrusive load monitoring},
	volume = {129},
	issn = {0142-0615},
	url = {https://www.sciencedirect.com/science/article/pii/S0142061521000776},
	doi = {10.1016/j.ijepes.2021.106837},
	abstract = {Non-Intrusive Load Monitoring (NILM) or Energy Disaggregation, seeks to save energy by decomposing corresponding appliances power reading from an aggregate power reading of the whole house. It is regarded as a single channel blind source separation problem to extract sources from a mixed signal. Recent studies have shown that deep learning is widely applied to NILM problem. Theoretically,the ability of any neural network to extract load features is closely related to its depth. However, a deep neural network is difficult to train because of exploding gradient, vanishing gradient, and network degradation. Therefore, Bi-TCN residual block, inspired by a temporal convolution network (TCN), is applied to solve these problems. Causal dilated convolution is replaced by bidirectional(non-causal) dilated convolution to enlarge the receptive field of network and improve the performance of model. Two forms of residual connections are introduced to deep models. One is designed to facilitate training deep models, and the other is pursuing performance-boosting by combining load features extracted of different hierarchical levels to final prediction. We propose a sequence to point learning based on bidirectional dilated convolution for NILM on low-frequency data, called BitcnNILM. We compare our method with existing algorithms on low-frequency data via REDD and UK-DALE datasets. Experiments show that the superiority of our BitcnNILM in both load disaggregation and load on/off identification.},
	language = {en},
	urldate = {2023-05-01},
	journal = {International Journal of Electrical Power \& Energy Systems},
	author = {Jia, Ziyue and Yang, Linfeng and Zhang, Zhenrong and Liu, Hui and Kong, Fannie},
	month = jul,
	year = {2021},
	keywords = {Convolution network, Energy disaggregation, Non-causal dilated convolution, Non-intrusive load monitoring, Residual network},
	pages = {106837},
}

@misc{ho_denoising_2020,
	title = {Denoising {Diffusion} {Probabilistic} {Models}},
	url = {http://arxiv.org/abs/2006.11239},
	abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion.},
	language = {en},
	urldate = {2023-05-01},
	publisher = {arXiv},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	month = dec,
	year = {2020},
	note = {arXiv:2006.11239 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{ho_denoising_2020-1,
	title = {Denoising {Diffusion} {Probabilistic} {Models}},
	url = {http://arxiv.org/abs/2006.11239},
	abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion.},
	language = {en},
	urldate = {2023-05-01},
	publisher = {arXiv},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	month = dec,
	year = {2020},
	note = {arXiv:2006.11239 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{zen_fast_2016,
	title = {Fast, {Compact}, and {High} {Quality} {LSTM}-{RNN} {Based} {Statistical} {Parametric} {Speech} {Synthesizers} for {Mobile} {Devices}},
	url = {http://arxiv.org/abs/1606.06061},
	abstract = {Acoustic models based on long short-term memory recurrent neural networks (LSTM-RNNs) were applied to statistical parametric speech synthesis (SPSS) and showed signiï¬cant improvements in naturalness and latency over those based on hidden Markov models (HMMs). This paper describes further optimizations of LSTM-RNN-based SPSS for deployment on mobile devices; weight quantization, multi-frame inference, and robust inference using an -contaminated Gaussian loss function. Experimental results in subjective listening tests show that these optimizations can make LSTM-RNN-based SPSS comparable to HMM-based SPSS in runtime speed while maintaining naturalness. Evaluations between LSTM-RNNbased SPSS and HMM-driven unit selection speech synthesis are also presented.},
	language = {en},
	urldate = {2023-05-01},
	publisher = {arXiv},
	author = {Zen, Heiga and Agiomyrgiannakis, Yannis and Egberts, Niels and Henderson, Fergus and Szczepaniak, PrzemysÅaw},
	month = jun,
	year = {2016},
	note = {arXiv:1606.06061 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound},
}

@article{fischman_phase_1997,
	title = {The phase vocoder: theory and practice},
	volume = {2},
	shorttitle = {The phase vocoder},
	doi = {10.1017/S1355771897009060},
	abstract = {This article discusses the theoretical principles governing the short time Fourier transform (STFT) and its musical applications, using the CARL phase vocoder and the Composers' Desktop Project (CDP) spectral transformations. The former was originally developed by Dolson (1986, 1991) and ported to personal computer systems by Atkins, Bentley, Henderson, Orton and Wishart (Atkins, Bentley, Endrich, Fischman, Malham, Orton and Wishart 1987, Wishart 1994a). The latter was developed by Wishart (1994b). When possible, theoretical matters are presented in a qualitative way, keeping the number and complexity of mathematical expressions within the essential requirements for those interested in musical applications. The latter are accompanied by sound examples, including excerpts from pieces by the author.},
	journal = {Organised Sound},
	author = {Fischman, Rajmil},
	month = aug,
	year = {1997},
	pages = {127--145},
}

@misc{jeong_diff-tts_2021,
	title = {Diff-{TTS}: {A} {Denoising} {Diffusion} {Model} for {Text}-to-{Speech}},
	shorttitle = {Diff-{TTS}},
	url = {http://arxiv.org/abs/2104.01409},
	abstract = {Although neural text-to-speech (TTS) models have attracted a lot of attention and succeeded in generating human-like speech, there is still room for improvements to its naturalness and architectural efï¬ciency. In this work, we propose a novel nonautoregressive TTS model, namely Diff-TTS, which achieves highly natural and efï¬cient speech synthesis. Given the text, Diff-TTS exploits a denoising diffusion framework to transform the noise signal into a mel-spectrogram via diffusion time steps. In order to learn the mel-spectrogram distribution conditioned on the text, we present a likelihood-based optimization method for TTS. Furthermore, to boost up the inference speed, we leverage the accelerated sampling method that allows Diff-TTS to generate raw waveforms much faster without signiï¬cantly degrading perceptual quality. Through experiments, we veriï¬ed that Diff-TTS generates 28 times faster than the real-time with a single NVIDIA 2080Ti GPU.},
	language = {en},
	urldate = {2023-05-01},
	publisher = {arXiv},
	author = {Jeong, Myeonghun and Kim, Hyeongju and Cheon, Sung Jun and Choi, Byoung Jin and Kim, Nam Soo},
	month = apr,
	year = {2021},
	note = {arXiv:2104.01409 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{tan_survey_2021,
	title = {A {Survey} on {Neural} {Speech} {Synthesis}},
	url = {http://arxiv.org/abs/2106.15561},
	abstract = {Text to speech (TTS), or speech synthesis, which aims to synthesize intelligible and natural speech given text, is a hot research topic in speech, language, and machine learning communities and has broad applications in the industry. As the development of deep learning and artiï¬cial intelligence, neural network-based TTS has signiï¬cantly improved the quality of synthesized speech in recent years. In this paper, we conduct a comprehensive survey on neural TTS, aiming to provide a good understanding of current research and future trends. We focus on the key components in neural TTS, including text analysis, acoustic models, and vocoders, and several advanced topics, including fast TTS, low-resource TTS, robust TTS, expressive TTS, and adaptive TTS, etc. We further summarize resources related to TTS (e.g., datasets, opensource implementations) and discuss future research directions. This survey can serve both academic researchers and industry practitioners working on TTS.},
	language = {en},
	urldate = {2023-04-28},
	publisher = {arXiv},
	author = {Tan, Xu and Qin, Tao and Soong, Frank and Liu, Tie-Yan},
	month = jul,
	year = {2021},
	note = {arXiv:2106.15561 [cs, eess]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Multimedia, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{ping_deep_2018,
	title = {Deep {Voice} 3: {Scaling} {Text}-to-{Speech} with {Convolutional} {Sequence} {Learning}},
	shorttitle = {Deep {Voice} 3},
	url = {http://arxiv.org/abs/1710.07654},
	abstract = {We present Deep Voice 3, a fully-convolutional attention-based neural textto-speech (TTS) system. Deep Voice 3 matches state-of-the-art neural speech synthesis systems in naturalness while training an order of magnitude faster. We scale Deep Voice 3 to dataset sizes unprecedented for TTS, training on more than eight hundred hours of audio from over two thousand speakers. In addition, we identify common error modes of attention-based speech synthesis networks, demonstrate how to mitigate them, and compare several different waveform synthesis methods. We also describe how to scale inference to ten million queries per day on a single GPU server.},
	language = {en},
	urldate = {2023-04-28},
	publisher = {arXiv},
	author = {Ping, Wei and Peng, Kainan and Gibiansky, Andrew and Arik, Sercan O. and Kannan, Ajay and Narang, Sharan and Raiman, Jonathan and Miller, John},
	month = feb,
	year = {2018},
	note = {arXiv:1710.07654 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{noauthor_wavenet_nodate,
	title = {{WaveNet}: {A} generative model for raw audio},
	shorttitle = {{WaveNet}},
	url = {https://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-audio},
	abstract = {This post presents WaveNet, a deep generative model of raw audio waveforms. We show that WaveNets are able to generate speech which mimics any human voice and which sounds more natural than the best existing Text-to-Speech systems, reducing the gap with human performance by over 50\%.},
	language = {en},
	urldate = {2023-04-28},
}

@misc{oord_wavenet_2016,
	title = {{WaveNet}: {A} {Generative} {Model} for {Raw} {Audio}},
	shorttitle = {{WaveNet}},
	url = {http://arxiv.org/abs/1609.03499},
	abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efï¬ciently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-ofthe-art performance, with human listeners rating it as signiï¬cantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal ï¬delity, and can switch between them by conditioning on the speaker identity. When trained to model music, we ï¬nd that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
	language = {en},
	urldate = {2023-04-28},
	publisher = {arXiv},
	author = {Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
	month = sep,
	year = {2016},
	note = {arXiv:1609.03499 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound},
}

@misc{donahue_adversarial_2019,
	title = {Adversarial {Audio} {Synthesis}},
	url = {http://arxiv.org/abs/1802.04208},
	abstract = {Audio signals are sampled at high temporal resolutions, and learning to synthesize audio requires capturing structure across a range of timescales. Generative adversarial networks (GANs) have seen wide success at generating images that are both locally and globally coherent, but they have seen little application to audio generation. In this paper we introduce WaveGAN, a ï¬rst attempt at applying GANs to unsupervised synthesis of raw-waveform audio. WaveGAN is capable of synthesizing one second slices of audio waveforms with global coherence, suitable for sound effect generation. Our experiments demonstrate thatâwithout labelsâWaveGAN learns to produce intelligible words when trained on a smallvocabulary speech dataset, and can also synthesize audio from other domains such as drums, bird vocalizations, and piano. We compare WaveGAN to a method which applies GANs designed for image generation on image-like audio feature representations, ï¬nding both approaches to be promising.},
	language = {en},
	urldate = {2023-04-28},
	publisher = {arXiv},
	author = {Donahue, Chris and McAuley, Julian and Puckette, Miller},
	month = feb,
	year = {2019},
	note = {arXiv:1802.04208 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound},
}

@misc{kong_diffwave_2021,
	title = {{DiffWave}: {A} {Versatile} {Diffusion} {Model} for {Audio} {Synthesis}},
	shorttitle = {{DiffWave}},
	url = {http://arxiv.org/abs/2009.09761},
	abstract = {In this work, we propose DiffWave, a versatile diffusion probabilistic model for conditional and unconditional waveform generation. The model is non-autoregressive, and converts the white noise signal into structured waveform through a Markov chain with a constant number of steps at synthesis. It is efficiently trained by optimizing a variant of variational bound on the data likelihood. DiffWave produces high-fidelity audios in different waveform generation tasks, including neural vocoding conditioned on mel spectrogram, class-conditional generation, and unconditional generation. We demonstrate that DiffWave matches a strong WaveNet vocoder in terms of speech quality (MOS: 4.44 versus 4.43), while synthesizing orders of magnitude faster. In particular, it significantly outperforms autoregressive and GAN-based waveform models in the challenging unconditional generation task in terms of audio quality and sample diversity from various automatic and human evaluations.},
	language = {en},
	urldate = {2023-04-28},
	publisher = {arXiv},
	author = {Kong, Zhifeng and Ping, Wei and Huang, Jiaji and Zhao, Kexin and Catanzaro, Bryan},
	month = mar,
	year = {2021},
	note = {arXiv:2009.09761 [cs, eess, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
}

@misc{chen_wavegrad_2020,
	title = {{WaveGrad}: {Estimating} {Gradients} for {Waveform} {Generation}},
	shorttitle = {{WaveGrad}},
	url = {http://arxiv.org/abs/2009.00713},
	abstract = {This paper introduces WaveGrad, a conditional model for waveform generation which estimates gradients of the data density. The model is built on prior work on score matching and diffusion probabilistic models. It starts from a Gaussian white noise signal and iteratively reï¬nes the signal via a gradient-based sampler conditioned on the mel-spectrogram. WaveGrad offers a natural way to trade inference speed for sample quality by adjusting the number of reï¬nement steps, and bridges the gap between non-autoregressive and autoregressive models in terms of audio quality. We ï¬nd that it can generate high ï¬delity audio samples using as few as six iterations. Experiments reveal WaveGrad to generate high ï¬delity audio, outperforming adversarial non-autoregressive baselines and matching a strong likelihood-based autoregressive baseline using fewer sequential operations. Audio samples are available at https://wavegrad.github.io/.},
	language = {en},
	urldate = {2023-04-28},
	publisher = {arXiv},
	author = {Chen, Nanxin and Zhang, Yu and Zen, Heiga and Weiss, Ron J. and Norouzi, Mohammad and Chan, William},
	month = oct,
	year = {2020},
	note = {arXiv:2009.00713 [cs, eess, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
}

@misc{hawthorne_multi-instrument_2022,
	title = {Multi-instrument {Music} {Synthesis} with {Spectrogram} {Diffusion}},
	url = {http://arxiv.org/abs/2206.05408},
	abstract = {An ideal music synthesizer should be both interactive and expressive, generating high-ï¬delity audio in realtime for arbitrary combinations of instruments and notes. Recent neural synthesizers have exhibited a tradeoff between domain-speciï¬c models that offer detailed control of only speciï¬c instruments, or raw waveform models that can train on any music but with minimal control and slow generation. In this work, we focus on a middle ground of neural synthesizers that can generate audio from MIDI sequences with arbitrary combinations of instruments in realtime. This enables training on a wide range of transcription datasets with a single model, which in turn offers notelevel control of composition and instrumentation across a wide range of instruments. We use a simple two-stage process: MIDI to spectrograms with an encoder-decoder Transformer, then spectrograms to audio with a generative adversarial network (GAN) spectrogram inverter. We compare training the decoder as an autoregressive model and as a Denoising Diffusion Probabilistic Model (DDPM) and ï¬nd that the DDPM approach is superior both qualitatively and as measured by audio reconstruction and FrÃ©chet distance metrics. Given the interactivity and generality of this approach, we ï¬nd this to be a promising ï¬rst step towards interactive and expressive neural synthesis for arbitrary combinations of instruments and notes.},
	language = {en},
	urldate = {2023-04-28},
	publisher = {arXiv},
	author = {Hawthorne, Curtis and Simon, Ian and Roberts, Adam and Zeghidour, Neil and Gardner, Josh and Manilow, Ethan and Engel, Jesse},
	month = dec,
	year = {2022},
	note = {arXiv:2206.05408 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@misc{heejkoo_resources_2023,
	title = {Resources},
	copyright = {MIT},
	url = {https://github.com/heejkoo/Awesome-Diffusion-Models},
	abstract = {A collection of resources and papers on Diffusion Models},
	urldate = {2023-04-28},
	author = {heejkoo},
	month = apr,
	year = {2023},
	note = {original-date: 2021-09-18T01:24:24Z},
	keywords = {artificial-intelligence, diffusion-models, generative-model, machine-learning, score-based, score-matching},
}

@misc{noauthor_papers_2023,
	title = {Papers},
	copyright = {MPL-2.0},
	url = {https://github.com/coqui-ai/TTS-papers},
	abstract = {ð¸ collection of TTS papers},
	urldate = {2023-04-28},
	publisher = {coqui},
	month = apr,
	year = {2023},
	note = {original-date: 2020-05-15T12:03:35Z},
	keywords = {coqui-ai, deep-learning, papers, research-paper, speech, tts},
}

@article{ng_spectral_2001,
	title = {On {Spectral} {Clustering}: {Analysis} and an algorithm},
	abstract = {Despite many empirical successes of spectral clustering methodsalgorithms that cluster points using eigenvectors of matrices derived from the data- there are several unresolved issues. First, there are a wide variety of algorithms that use the eigenvectors in slightly different ways. Second, many of these algorithms have no proof that they will actually compute a reasonable clustering. In this paper, we present a simple spectral clustering algorithm that can be implemented using a few lines of Matlab. Using tools from matrix perturbation theory, we analyze the algorithm, and give conditions under which it can be expected to do well. We also show surprisingly good experimental results on a number of challenging clustering problems.},
	language = {en},
	author = {Ng, Andrew Y and Jordan, Michael I and Weiss, Yair},
	year = {2001},
}

@article{zelnik-manor_self-tuning_2004,
	title = {Self-{Tuning} {Spectral} {Clustering}},
	abstract = {We study a number of open issues in spectral clustering: (i) Selecting the appropriate scale of analysis, (ii) Handling multi-scale data, (iii) Clustering with irregular background clutter, and, (iv) Finding automatically the number of groups. We ï¬rst propose that a âlocalâ scale should be used to compute the afï¬nity between each pair of points. This local scaling leads to better clustering especially when the data includes multiple scales and when the clusters are placed within a cluttered background. We further suggest exploiting the structure of the eigenvectors to infer automatically the number of groups. This leads to a new algorithm in which the ï¬nal randomly initialized k-means stage is eliminated.},
	language = {en},
	author = {Zelnik-manor, Lihi and Perona, Pietro},
	year = {2004},
}

@misc{noauthor_icamusical_nodate,
	title = {icamusical},
	url = {https://www.kaggle.com/datasets/chittalpatel/icamusical},
	abstract = {Kaggle is the worldâs largest data science community with powerful tools and resources to help you achieve your data science goals.},
	language = {en},
	urldate = {2023-02-03},
}

@article{mousavi_hamiltonian_2021,
	title = {Hamiltonian {Adaptive} {Importance} {Sampling}},
	volume = {28},
	issn = {1070-9908, 1558-2361},
	url = {http://arxiv.org/abs/2209.13716},
	doi = {10.1109/LSP.2021.3068616},
	abstract = {Importance sampling (IS) is a powerful Monte Carlo (MC) methodology for approximating integrals, for instance in the context of Bayesian inference. In IS, the samples are simulated from the so-called proposal distribution, and the choice of this proposal is key for achieving a high performance. In adaptive IS (AIS) methods, a set of proposals is iteratively improved. AIS is a relevant and timely methodology although many limitations remain yet to be overcome, e.g., the curse of dimensionality in high-dimensional and multi-modal problems. Moreover, the Hamiltonian Monte Carlo (HMC) algorithm has become increasingly popular in machine learning and statistics. HMC has several appealing features such as its exploratory behavior, especially in high-dimensional targets, when other methods suffer. In this paper, we introduce the novel Hamiltonian adaptive importance sampling (HAIS) method. HAIS implements a two-step adaptive process with parallel HMC chains that cooperate at each iteration. The proposed HAIS efï¬ciently adapts a population of proposals, extracting the advantages of HMC. HAIS can be understood as a particular instance of the generic layered AIS family with an additional resampling step. HAIS achieves a signiï¬cant performance improvement in high-dimensional problems w.r.t. state-of-the-art algorithms. We discuss the statistical properties of HAIS and show its high performance in two challenging examples.},
	language = {en},
	urldate = {2023-02-03},
	journal = {IEEE Signal Processing Letters},
	author = {Mousavi, Ali and Monsefi, Reza and Elvira, VÃ­ctor},
	year = {2021},
	note = {arXiv:2209.13716 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {713--717},
}

@misc{kingma_auto-encoding_2014,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1312.6114},
	doi = {10.48550/arXiv.1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	urldate = {2023-02-03},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Welling, Max},
	year = {2014},
	note = {arXiv:1312.6114 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{hinton_wake-sleep_1995,
	title = {The "{Wake}-{Sleep}" {Algorithm} for {Unsupervised} {Neural} {Networks}},
	volume = {268},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.7761831},
	doi = {10.1126/science.7761831},
	abstract = {An unsupervised learning algorithm for a multilayer network of stochastic neurons is described. Bottom-up "recognition" connections convert the input into representations in successive hidden layers, and top-down "generative" connections reconstruct the representation in one layer from the representation in the layer above. In the "wake" phase, neurons are driven by recognition connections, and generative connections are adapted to increase the probability that they would reconstruct the correct activity vector in the layer below. In the "sleep" phase, neurons are driven by generative connections, and recognition connections are adapted to increase the probability that they would produce the correct activity vector in the layer above.},
	language = {en},
	number = {5214},
	urldate = {2023-02-03},
	journal = {Science},
	author = {Hinton, Geoffrey E. and Dayan, Peter and Frey, Brendan J. and Neal, Radford M.},
	month = may,
	year = {1995},
	pages = {1158--1161},
}

@article{hastings_monte_1970,
	title = {Monte {Carlo} {Sampling} {Methods} {Using} {Markov} {Chains} and {Their} {Applications}},
	abstract = {A generalization of the sampling method introduced by Metropolis et al. (1953) is presented along with an exposition of the relevant theory, techniques of application and methods and difficulties of assessing the error in Monte Carlo estimates. Examples of the methods, including the generation of random orthogonal matrices and potential applications of the methods to numerical problems arising in statistics, are discussed.},
	language = {en},
	author = {Hastings, W K},
	year = {1970},
}

@incollection{jordan_introduction_1998,
	address = {Dordrecht},
	title = {An {Introduction} to {Variational} {Methods} for {Graphical} {Models}},
	isbn = {978-94-010-6104-9 978-94-011-5014-9},
	url = {http://link.springer.com/10.1007/978-94-011-5014-9_5},
	abstract = {This paper presents a tutorial introduction to the use of variational methods for inference and learning in graphical models (Bayesian networks and Markov random ï¬elds). We present a number of examples of graphical models, including the QMR-DT database, the sigmoid belief network, the Boltzmann machine, and several variants of hidden Markov models, in which it is infeasible to run exact inference algorithms. We then introduce variational methods, which exploit laws of large numbers to transform the original graphical model into a simpliï¬ed graphical model in which inference is efï¬cient. Inference in the simpiï¬ed model provides bounds on probabilities of interest in the original model. We describe a general framework for generating variational transformations based on convex duality. Finally we return to the examples and demonstrate how variational algorithms can be formulated in each case.},
	language = {en},
	urldate = {2023-02-03},
	booktitle = {Learning in {Graphical} {Models}},
	publisher = {Springer Netherlands},
	author = {Jordan, Michael I. and Ghahramani, Zoubin and Jaakkola, Tommi S. and Saul, Lawrence K.},
	editor = {Jordan, Michael I.},
	year = {1998},
	doi = {10.1007/978-94-011-5014-9_5},
	pages = {105--161},
}

@article{carpenter_stan_2017,
	title = {Stan: {A} probabilistic programming language},
	volume = {76},
	number = {1},
	journal = {Journal of statistical software},
	author = {Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew D and Lee, Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker, Marcus and Guo, Jiqiang and Li, Peter and Riddell, Allen},
	year = {2017},
	note = {Publisher: Columbia Univ., New York, NY (United States); Harvard Univ., Cambridge, MA (United States)},
}

@inproceedings{ge_turing_2018,
	title = {Turing: {A} {Language} for {Flexible} {Probabilistic} {Inference}},
	shorttitle = {Turing},
	url = {https://proceedings.mlr.press/v84/ge18b.html},
	abstract = {Probabilistic programming promises to simplify and democratize probabilistic machine learning, but successful probabilistic programming systems require flexible, generic and efficient inference engines. In this work, we present a system called Turing for building MCMC algorithms for probabilistic programming inference. Turing has a very simple syntax and makes full use of the numerical capabilities in the Julia programming language, including all implemented probability distributions, and automatic differentiation. Turing supports a wide range of popular Monte Carlo algorithms, including Hamiltonian Monte Carlo (HMC), HMC with No-U-Turns (NUTS), Gibbs sampling, sequential Monte Carlo (SMC), and several particle MCMC (PMCMC) samplers. Most importantly, Turing inference is composable: it combines MCMC operations on subsets of variables, for example using a combination of an HMC engine and a particle Gibbs (PG) engine.  We explore several combinations of inference methods with the aim of finding approaches that are both efficient and universal, i.e. applicable to arbitrary probabilistic models. NUTSâa popular variant of HMC that adapts Hamiltonian simulation path length automatically, although quite powerful for exploring differentiable target distributions, is however not universal. We identify some failure modes for the NUTS engine, and demonstrate that composition of PG (for discrete variables) and NUTS (for continuous variables) can be useful when the NUTS engine is either not applicable, or simply does not work well. Our aim is to present Turing and its composable inference engines to the world and encourage other researchers to build on this system to help advance the field of probabilistic machine learning.},
	language = {en},
	urldate = {2023-02-03},
	booktitle = {Proceedings of the {Twenty}-{First} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Ge, Hong and Xu, Kai and Ghahramani, Zoubin},
	month = mar,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {1682--1690},
}

@misc{bornschein_reweighted_2015,
	title = {Reweighted {Wake}-{Sleep}},
	url = {http://arxiv.org/abs/1406.2751},
	abstract = {Training deep directed graphical models with many hidden variables and performing inference remains a major challenge. Helmholtz machines and deep belief networks are such models, and the wake-sleep algorithm has been proposed to train them. The wake-sleep algorithm relies on training not just the directed generative model but also a conditional generative model (the inference network) that runs backward from visible to latent, estimating the posterior distribution of latent given visible. We propose a novel interpretation of the wake-sleep algorithm which suggests that better estimators of the gradient can be obtained by sampling latent variables multiple times from the inference network. This view is based on importance sampling as an estimator of the likelihood, with the approximate inference network as a proposal distribution. This interpretation is conï¬rmed experimentally, showing that better likelihood can be achieved with this reweighted wake-sleep procedure. Based on this interpretation, we propose that a sigmoidal belief network is not sufï¬ciently powerful for the layers of the inference network in order to recover a good estimator of the posterior distribution of latent variables. Our experiments show that using a more powerful layer model, such as NADE, yields substantially better generative models.},
	language = {en},
	urldate = {2023-02-03},
	publisher = {arXiv},
	author = {Bornschein, JÃ¶rg and Bengio, Yoshua},
	month = apr,
	year = {2015},
	note = {arXiv:1406.2751 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{burda_importance_2016,
	title = {Importance {Weighted} {Autoencoders}},
	url = {http://arxiv.org/abs/1509.00519},
	abstract = {The variational autoencoder (VAE; Kingma \& Welling (2014)) is a recently proposed generative model pairing a top-down generative network with a bottom-up recognition network which approximates posterior inference. It typically makes strong assumptions about posterior inference, for instance that the posterior distribution is approximately factorial, and that its parameters can be approximated with nonlinear regression from the observations. As we show empirically, the VAE objective can lead to overly simpliï¬ed representations which fail to use the networkâs entire modeling capacity. We present the importance weighted autoencoder (IWAE), a generative model with the same architecture as the VAE, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting. In the IWAE, the recognition network uses multiple samples to approximate the posterior, giving it increased ï¬exibility to model complex posteriors which do not ï¬t the VAE modeling assumptions. We show empirically that IWAEs learn richer latent space representations than VAEs, leading to improved test log-likelihood on density estimation benchmarks.},
	language = {en},
	urldate = {2023-02-03},
	publisher = {arXiv},
	author = {Burda, Yuri and Grosse, Roger and Salakhutdinov, Ruslan},
	month = nov,
	year = {2016},
	note = {arXiv:1509.00519 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{metropolis_equation_1953,
	title = {Equation of {State} {Calculations} by {Fast} {Computing} {Machines}},
	volume = {21},
	issn = {0021-9606, 1089-7690},
	url = {http://aip.scitation.org/doi/10.1063/1.1699114},
	doi = {10.1063/1.1699114},
	language = {en},
	number = {6},
	urldate = {2023-02-03},
	journal = {The Journal of Chemical Physics},
	author = {Metropolis, Nicholas and Rosenbluth, Arianna W. and Rosenbluth, Marshall N. and Teller, Augusta H. and Teller, Edward},
	month = jun,
	year = {1953},
	pages = {1087--1092},
}

@article{calderhead_general_2014,
	title = {A general construction for parallelizing {Metropolis}â{Hastings} algorithms},
	volume = {111},
	url = {https://www.pnas.org/doi/10.1073/pnas.1408184111},
	doi = {10.1073/pnas.1408184111},
	abstract = {Markov chain Monte Carlo methods (MCMC) are essential tools for solving many modern-day statistical and computational problems; however, a major limitation is the inherently sequential nature of these algorithms. In this paper, we propose a natural generalization of the MetropolisâHastings algorithm that allows for parallelizing a single chain using existing MCMC methods. We do so by proposing multiple points in parallel, then constructing and sampling from a finite-state Markov chain on the proposed points such that the overall procedure has the correct target density as its stationary distribution. Our approach is generally applicable and straightforward to implement. We demonstrate how this construction may be used to greatly increase the computational speed and statistical efficiency of a variety of existing MCMC methods, including Metropolis-Adjusted Langevin Algorithms and Adaptive MCMC. Furthermore, we show how it allows for a principled way of using every integration step within Hamiltonian Monte Carlo methods; our approach increases robustness to the choice of algorithmic parameters and results in increased accuracy of Monte Carlo estimates with little extra computational cost.},
	number = {49},
	urldate = {2023-02-03},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Calderhead, Ben},
	month = dec,
	year = {2014},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {17408--17413},
}

@article{chatterjee_sample_2018,
	title = {The sample size required in importance sampling},
	volume = {28},
	issn = {1050-5164},
	url = {https://projecteuclid.org/journals/annals-of-applied-probability/volume-28/issue-2/The-sample-size-required-in-importance-sampling/10.1214/17-AAP1326.full},
	doi = {10.1214/17-AAP1326},
	language = {en},
	number = {2},
	urldate = {2023-02-03},
	journal = {The Annals of Applied Probability},
	author = {Chatterjee, Sourav and Diaconis, Persi},
	month = apr,
	year = {2018},
}

@misc{aitchison_tensor_2019,
	title = {Tensor {Monte} {Carlo}: particle methods for the {GPU} era},
	shorttitle = {Tensor {Monte} {Carlo}},
	url = {http://arxiv.org/abs/1806.08593},
	abstract = {Multi-sample, importance-weighted variational autoencoders (IWAE) give tighter bounds and more accurate uncertainty estimates than variational autoencoders (VAE) trained with a standard single-sample objective. However, IWAEs scale poorly: as the latent dimensionality grows, they require exponentially many samples to retain the beneï¬ts of importance weighting. While sequential Monte-Carlo (SMC) can address this problem, it is prohibitively slow because the resampling step imposes sequential structure which cannot be parallelised, and moreover, resampling is nondifferentiable which is problematic when learning approximate posteriors. To address these issues, we developed tensor Monte-Carlo (TMC) which gives exponentially many importance samples by separately drawing K samples for each of the n latent variables, then averaging over all Kn possible combinations. While the sum over exponentially many terms might seem to be intractable, in many cases it can be computed efï¬ciently as a series of tensor inner-products. We show that TMC is superior to IWAE on a generative model with multiple stochastic layers trained on the MNIST handwritten digit database, and we show that TMC can be combined with standard variance reduction techniques.},
	language = {en},
	urldate = {2023-02-03},
	publisher = {arXiv},
	author = {Aitchison, Laurence},
	month = jan,
	year = {2019},
	note = {arXiv:1806.08593 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{giordano_casey_r_nodate,
	title = {R: {Iterated} {Principal} {Axis} {Factor} {Analysis} (fapa)},
	url = {https://search.r-project.org/CRAN/refmans/fungible/html/fapa.html},
	urldate = {2023-02-02},
	author = {{Giordano, Casey} and {Waller, Niels}},
}

@article{noauthor_notitle_nodate,
}

@misc{wilkinson_simple_nodate,
	title = {A simple {Gibbs} sampler},
	url = {https://www.mas.ncl.ac.uk/~ndjw1/teaching/sim/gibbs/gibbs.html},
	urldate = {2023-01-09},
	author = {Wilkinson, Darren},
}

@misc{larangeira_three_nodate,
	title = {Three {Simple} {Applications} of {Markov} {Chains} and {Gibbs} {Sampling}},
	url = {http://rstudio-pubs-static.s3.amazonaws.com/279858_010f9da7c8d744988019397e3fe51cb2.html},
	urldate = {2023-01-09},
	author = {Larangeira, Gui},
}

@misc{chan_metropolis_2016,
	title = {Metropolis and {Gibbs} {Sampling} â {Computational} {Statistics} in {Python}},
	url = {https://people.duke.edu/~ccc14/sta-663-2016/16A_MCMC.html#The-Gibbs-sampler},
	urldate = {2023-01-09},
	journal = {Computational Statistics In Python},
	author = {Chan, Cliburn and McCarthy, Janice},
	year = {2016},
}

@misc{noauthor_monte_nodate,
	title = {Monte {Carlo} integration - {SC1}},
	url = {https://awllee.github.io/sc1/integration/monte-carlo/#markov-chain-monte-carlo},
	abstract = {Quadrature rules give excellent rates of convergence, in terms of computational cost, for one-dimensional integrals of sufficiently smooth functions. However, they quickly become prohibitively \&hellip;},
	language = {en-us},
	urldate = {2023-01-09},
}

@article{yoon_hidden_2009,
	title = {Hidden {Markov} {Models} and their {Applications} in {Biological} {Sequence} {Analysis}},
	volume = {10},
	issn = {1389-2029},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2766791/},
	doi = {10.2174/138920209789177575},
	abstract = {Hidden Markov models (HMMs) have been extensively used in biological sequence analysis. In this paper, we give a tutorial review of HMMs and their applications in a variety of problems in molecular biology. We especially focus on three types of HMMs: the profile-HMMs, pair-HMMs, and context-sensitive HMMs. We show how these HMMs can be used to solve various sequence analysis problems, such as pairwise and multiple sequence alignments, gene annotation, classification, similarity search, and many others.},
	number = {6},
	urldate = {2022-11-27},
	journal = {Current Genomics},
	author = {Yoon, Byung-Jun},
	month = sep,
	year = {2009},
	pmid = {20190955},
	pmcid = {PMC2766791},
	pages = {402--415},
}

@article{rabiner_tutorial_1989,
	title = {A tutorial on hidden {Markov} models and selected applications in speech recognition},
	volume = {77},
	issn = {1558-2256},
	doi = {10.1109/5.18626},
	abstract = {This tutorial provides an overview of the basic theory of hidden Markov models (HMMs) as originated by L.E. Baum and T. Petrie (1966) and gives practical details on methods of implementation of the theory along with a description of selected applications of the theory to distinct problems in speech recognition. Results from a number of original sources are combined to provide a single source of acquiring the background required to pursue further this area of research. The author first reviews the theory of discrete Markov chains and shows how the concept of hidden states, where the observation is a probabilistic function of the state, can be used effectively. The theory is illustrated with two simple examples, namely coin-tossing, and the classic balls-in-urns system. Three fundamental problems of HMMs are noted and several practical techniques for solving these problems are given. The various types of HMMs that have been studied, including ergodic as well as left-right models, are described.{\textless}{\textgreater}},
	number = {2},
	journal = {Proceedings of the IEEE},
	author = {Rabiner, L.R.},
	month = feb,
	year = {1989},
	keywords = {Hidden Markov models, Speech recognition, Tutorial},
	pages = {257--286},
}

@article{wong_dna_2013,
	title = {{DNA} motif elucidation using belief propagation},
	volume = {41},
	issn = {0305-1048},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3763557/},
	doi = {10.1093/nar/gkt574},
	abstract = {Protein-binding microarray (PBM) is a high-throughout platform that can measure the DNA-binding preference of a protein in a comprehensive and unbiased manner. A typical PBM experiment can measure binding signal intensities of a protein to all the possible DNA k-mers (k = 8 â¼10); such comprehensive binding affinity data usually need to be reduced and represented as motif models before they can be further analyzed and applied. Since proteins can often bind to DNA in multiple modes, one of the major challenges is to decompose the comprehensive affinity data into multimodal motif representations. Here, we describe a new algorithm that uses Hidden Markov Models (HMMs) and can derive precise and multimodal motifs using belief propagations. We describe an HMM-based approach using belief propagations (kmerHMM), which accepts and preprocesses PBM probe raw data into median-binding intensities of individual k-mers. The k-mers are ranked and aligned for training an HMM as the underlying motif representation. Multiple motifs are then extracted from the HMM using belief propagations. Comparisons of kmerHMM with other leading methods on several data sets demonstrated its effectiveness and uniqueness. Especially, it achieved the best performance on more than half of the data sets. In addition, the multiple binding modes derived by kmerHMM are biologically meaningful and will be useful in interpreting other genome-wide data such as those generated from ChIP-seq. The executables and source codes are available at the authorsâ websites: e.g. http://www.cs.toronto.edu/â¼wkc/kmerHMM.},
	number = {16},
	urldate = {2022-11-27},
	journal = {Nucleic Acids Research},
	author = {Wong, Ka-Chun and Chan, Tak-Ming and Peng, Chengbin and Li, Yue and Zhang, Zhaolei},
	month = sep,
	year = {2013},
	pmid = {23814189},
	pmcid = {PMC3763557},
	pages = {e153},
}

@article{frazzoli_16410_nodate,
	title = {16.410 {Lecture} 21: {Intro} to {Hidden} {Markov} {Models} the {Baum}-{Welch} algorithm},
	language = {en},
	author = {Frazzoli, Emilio},
	pages = {24},
}

@article{slingsby_recursive_1992,
	title = {Recursive {Parameter} {Estimation} for {Arbitrary} {Hidden} {Markov} {Models}},
	volume = {25},
	issn = {14746670},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S147466701750703X},
	doi = {10.1016/S1474-6670(17)50703-X},
	abstract = {This paper describes on-line re-estimation formulae for arbitrary hidden Markov models, extending previous work that proposed a recursive formula for re-estimation of a diagonally dominant state transition probability matrix. Simulation experiments show that for some applications the formula described here should be used instead in order to ensure algorithm convergence. Also proposed is a re-estimation formula for online update of non-parametric observation probabilities. Simulation experiments show that these estimates are in general asymptotically biased.},
	language = {en},
	number = {14},
	urldate = {2022-11-24},
	journal = {IFAC Proceedings Volumes},
	author = {Slingsby, P.L.},
	month = jul,
	year = {1992},
	pages = {1--4},
}

@article{rydkn_recursive_nodate,
	title = {On recursive estimation for hidden {Markov} models},
	abstract = {Hidden Markov models (HMMs) have during the last decade become a widespread tool for modelling sequences of dependent random variables. In this paper we consider a recursive estimator for HMMs based on the m-dimensional distribution of the process and show that this estimator converges to the set of stationary points of the corresponding Kullback-Leibler information. We also investigate averaging in this recursive scheme and show that conditional on convergence to the true parameter, and provided m is chosen large enough, the averaged estimator is close to optimal.},
	language = {en},
	author = {Rydkn, Tobias},
	pages = {18},
}

@inproceedings{legland_recursive_1997,
	address = {San Diego, CA, USA},
	title = {Recursive estimation in hidden {Markov} models},
	volume = {4},
	isbn = {978-0-7803-4187-6},
	url = {http://ieeexplore.ieee.org/document/652384/},
	doi = {10.1109/CDC.1997.652384},
	abstract = {We consider a hidden Markov model (HMM) with multidimensional observations, and where the coefficients (transition probability matrix, and observation conditional densities) depend on some unknown parameter. We study the asymptotic behaviour of two recursive estimators, the recursive maximum likelihood estimator (RMLE), and the recursive conditional least squares estimator (RCLSE), as the number of observations increases to infinity. Firstly, we exhibit the contrast functions associated with the two non-recursive estimators, and we prove that the recursive estimators converge a.s. to the set of stationary points of the corresponding contrast function. Secondly, we prove that the two recursive estimators are asymptotically normal.},
	language = {en},
	urldate = {2022-11-24},
	booktitle = {Proceedings of the 36th {IEEE} {Conference} on {Decision} and {Control}},
	publisher = {IEEE},
	author = {LeGland, F. and Mevel, L.},
	year = {1997},
	pages = {3468--3473},
}

@inproceedings{plessis_analysis_2014,
	address = {Cambridge, MA, USA},
	series = {{NIPS}'14},
	title = {Analysis of learning from positive and unlabeled data},
	abstract = {Learning a classifier from positive and unlabeled data is an important class of classification problems that are conceivable in many practical applications. In this paper, we first show that this problem can be solved by cost-sensitive learning between positive and unlabeled data. We then show that convex surrogate loss functions such as the hinge loss may lead to a wrong classification boundary due to an intrinsic bias, but the problem can be avoided by using non-convex loss functions such as the ramp loss. We next analyze the excess risk when the class prior is estimated from data, and show that the classification accuracy is not sensitive to class prior estimation if the unlabeled data is dominated by the positive data (this is naturally satisfied in inlier-based outlier detection because inliers are dominant in the unlabeled dataset). Finally, we provide generalization error bounds and show that, for an equal number of labeled and unlabeled samples, the generalization error of learning only from positive and unlabeled samples is no worse than 2â2 times the fully supervised case. These theoretical findings are also validated through experiments.},
	urldate = {2022-11-24},
	booktitle = {Proceedings of the 27th {International} {Conference} on {Neural} {Information} {Processing} {Systems} - {Volume} 1},
	publisher = {MIT Press},
	author = {Plessis, Marthinus C. du and Niu, Gang and Sugiyama, Masashi},
	month = dec,
	year = {2014},
	pages = {703--711},
}

@misc{liu_stein_2019,
	title = {Stein {Variational} {Gradient} {Descent}: {A} {General} {Purpose} {Bayesian} {Inference} {Algorithm}},
	shorttitle = {Stein {Variational} {Gradient} {Descent}},
	url = {http://arxiv.org/abs/1608.04471},
	abstract = {We propose a general purpose variational inference algorithm that forms a natural counterpart of gradient descent for optimization. Our method iteratively transports a set of particles to match the target distribution, by applying a form of functional gradient descent that minimizes the KL divergence. Empirical studies are performed on various real world models and datasets, on which our method is competitive with existing state-of-the-art methods. The derivation of our method is based on a new theoretical result that connects the derivative of KL divergence under smooth transforms with Stein's identity and a recently proposed kernelized Stein discrepancy, which is of independent interest.},
	urldate = {2022-11-24},
	publisher = {arXiv},
	author = {Liu, Qiang and Wang, Dilin},
	month = sep,
	year = {2019},
	note = {arXiv:1608.04471 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@book{bishop_pattern_2006,
	address = {New York},
	series = {Information science and statistics},
	title = {Pattern recognition and machine learning},
	isbn = {9780387310732},
	publisher = {Springer},
	author = {Bishop, Christopher M.},
	year = {2006},
	keywords = {Machine learning, Pattern perception},
}

@misc{uefacom_uefa_2022,
	title = {{UEFA} approves final format and access list for its club competitions as of the 2024/25 season {\textbar} {Inside} {UEFA}},
	url = {https://www.uefa.com/insideuefa/mediaservices/mediareleases/news/0275-151c779310c3-b92bbf0d24f9-1000--uefa-approves-final-format-and-access-list-for-its-club-competi/},
	abstract = {No more access granted based on club coefficients. Eight matches instead of ten in the new league phase.},
	language = {en},
	urldate = {2022-05-16},
	journal = {UEFA.com},
	author = {UEFA.com},
	month = may,
	year = {2022},
	note = {Section: Football},
}

@misc{gavenciak_sorting_2018,
	title = {Sorting by {Swaps} with {Noisy} {Comparisons}},
	url = {http://arxiv.org/abs/1803.04509},
	abstract = {We study sorting of permutations by random swaps if each comparison gives the wrong result with some ï¬xed probability p {\textless} 1/2. We use this process as prototype for the behaviour of randomized, comparisonbased optimization heuristics in the presence of noisy comparisons. As quality measure, we compute the expected ï¬tness of the stationary distribution. To measure the runtime, we compute the minimal number of steps after which the average ï¬tness approximates the expected ï¬tness of the stationary distribution.},
	language = {en},
	urldate = {2022-05-16},
	publisher = {arXiv},
	author = {GavenÄiak, TomÃ¡Å¡ and Geissmann, Barbara and Lengler, Johannes},
	month = mar,
	year = {2018},
	note = {Number: arXiv:1803.04509
arXiv:1803.04509 [cs, math]},
	keywords = {Computer Science - Neural and Evolutionary Computing, Mathematics - Probability},
}

@misc{a_j_ganesh_multi-armed_2019,
	address = {University of Bristol},
	type = {Lecture {Notes}},
	title = {Multi-armed {Bandits}},
	url = {https://people.maths.bris.ac.uk/~maajg/teaching/stochopt/ucb.pdf},
	author = {{A. J. Ganesh}},
	month = oct,
	year = {2019},
}

@inproceedings{li_contextual-bandit_2010,
	title = {A {Contextual}-{Bandit} {Approach} to {Personalized} {News} {Article} {Recommendation}},
	url = {http://arxiv.org/abs/1003.0146},
	doi = {10.1145/1772690.1772758},
	abstract = {Personalized web services strive to adapt their services (advertisements, news articles, etc) to individual users by making use of both content and user information. Despite a few recent advances, this problem remains challenging for at least two reasons. First, web service is featured with dynamically changing pools of content, rendering traditional collaborative filtering methods inapplicable. Second, the scale of most web services of practical interest calls for solutions that are both fast in learning and computation. In this work, we model personalized recommendation of news articles as a contextual bandit problem, a principled approach in which a learning algorithm sequentially selects articles to serve users based on contextual information about the users and articles, while simultaneously adapting its article-selection strategy based on user-click feedback to maximize total user clicks. The contributions of this work are three-fold. First, we propose a new, general contextual bandit algorithm that is computationally efficient and well motivated from learning theory. Second, we argue that any bandit algorithm can be reliably evaluated offline using previously recorded random traffic. Finally, using this offline evaluation method, we successfully applied our new algorithm to a Yahoo! Front Page Today Module dataset containing over 33 million events. Results showed a 12.5\% click lift compared to a standard context-free bandit algorithm, and the advantage becomes even greater when data gets more scarce.},
	urldate = {2022-05-14},
	booktitle = {Proceedings of the 19th international conference on {World} wide web - {WWW} '10},
	author = {Li, Lihong and Chu, Wei and Langford, John and Schapire, Robert E.},
	year = {2010},
	note = {arXiv:1003.0146 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Information Retrieval, Computer Science - Machine Learning, H.3.5, I.2.6},
	pages = {661},
}

@techreport{valko_finite-time_2013,
	title = {Finite-{Time} {Analysis} of {Kernelised} {Contextual} {Bandits}},
	url = {http://arxiv.org/abs/1309.6869},
	abstract = {We tackle the problem of online reward maximisation over a large finite set of actions described by their contexts. We focus on the case when the number of actions is too big to sample all of them even once. However we assume that we have access to the similarities between actions' contexts and that the expected reward is an arbitrary linear function of the contexts' images in the related reproducing kernel Hilbert space (RKHS). We propose KernelUCB, a kernelised UCB algorithm, and give a cumulative regret bound through a frequentist analysis. For contextual bandits, the related algorithm GP-UCB turns out to be a special case of our algorithm, and our finite-time analysis improves the regret bound of GP-UCB for the agnostic case, both in the terms of the kernel-dependent quantity and the RKHS norm of the reward function. Moreover, for the linear kernel, our regret bound matches the lower bound for contextual linear bandits.},
	number = {arXiv:1309.6869},
	urldate = {2022-05-14},
	institution = {arXiv},
	author = {Valko, Michal and Korda, Nathaniel and Munos, Remi and Flaounas, Ilias and Cristianini, Nelo},
	month = sep,
	year = {2013},
	doi = {10.48550/arXiv.1309.6869},
	note = {arXiv:1309.6869 [cs, stat]
type: article},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{scott_modern_2010,
	title = {A modern {Bayesian} look at the multi-armed bandit},
	volume = {26},
	issn = {1526-4025},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/asmb.874},
	doi = {10.1002/asmb.874},
	abstract = {A multi-armed bandit is an experiment with the goal of accumulating rewards from a payoff distribution with unknown parameters that are to be learned sequentially. This article describes a heuristic for managing multi-armed bandits called randomized probability matching, which randomly allocates observations to arms according the Bayesian posterior probability that each arm is optimal. Advances in Bayesian computation have made randomized probability matching easy to apply to virtually any payoff distribution. This flexibility frees the experimenter to work with payoff distributions that correspond to certain classical experimental designs that have the potential to outperform methods that are âoptimalâ in simpler contexts. I summarize the relationships between randomized probability matching and several related heuristics that have been used in the reinforcement learning literature. Copyright Â© 2010 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {6},
	urldate = {2022-05-14},
	journal = {Applied Stochastic Models in Business and Industry},
	author = {Scott, Steven L.},
	year = {2010},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/asmb.874},
	keywords = {Bayesian adaptive design, exploration vs exploitation, probability matching, sequential design},
	pages = {639--658},
}

@article{scott_modern_2010-1,
	title = {A modern {Bayesian} look at the multi-armed bandit},
	volume = {26},
	issn = {1526-4025},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/asmb.874},
	doi = {10.1002/asmb.874},
	abstract = {A multi-armed bandit is an experiment with the goal of accumulating rewards from a payoff distribution with unknown parameters that are to be learned sequentially. This article describes a heuristic for managing multi-armed bandits called randomized probability matching, which randomly allocates observations to arms according the Bayesian posterior probability that each arm is optimal. Advances in Bayesian computation have made randomized probability matching easy to apply to virtually any payoff distribution. This flexibility frees the experimenter to work with payoff distributions that correspond to certain classical experimental designs that have the potential to outperform methods that are âoptimalâ in simpler contexts. I summarize the relationships between randomized probability matching and several related heuristics that have been used in the reinforcement learning literature. Copyright Â© 2010 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {6},
	urldate = {2022-05-14},
	journal = {Applied Stochastic Models in Business and Industry},
	author = {Scott, Steven L.},
	year = {2010},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/asmb.874},
	keywords = {Bayesian adaptive design, exploration vs exploitation, probability matching, sequential design},
	pages = {639--658},
}

@article{auer_finite-time_2002,
	title = {Finite-time {Analysis} of the {Multiarmed} {Bandit} {Problem}},
	volume = {47},
	doi = {10.1023/A:1013689704352},
	abstract = {Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to find profitable actions while taking the empirically best action as often as possible. A popular measure of a policy's success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the first ones to show that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we show that the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies, and for all reward distributions with bounded support.},
	journal = {Machine Learning},
	author = {Auer, Peter and Cesa-Bianchi, NicolÃ² and Fischer, Paul},
	month = may,
	year = {2002},
	pages = {235--256},
}

@inproceedings{auer_using_2000,
	title = {Using upper confidence bounds for online learning},
	doi = {10.1109/SFCS.2000.892116},
	abstract = {We show how a standard tool from statistics, namely confidence bounds, can be used to elegantly deal with situations which exhibit an exploitation/exploration trade-off. Our technique for designing and analyzing algorithms for such situations is very general and can be applied when an algorithm has to make exploitation-versus-exploration decisions based on uncertain information provided by a random process. We consider two models with such an exploitation/exploration trade-off. For the adversarial bandit problem our new algorithm suffers only O/spl tilde/(T/sup 1/2/) regret over T trials which improves significantly over the previously best O/spl tilde/(T/sup 2/3/) regret. We also extend our results for the adversarial bandit problem to shifting bandits. The second model we consider is associative reinforcement learning with linear value functions. For this model our technique improves the regret from O/spl tilde/(T/sup 3/4/) to O/spl tilde/(T/sup 1/2/).},
	booktitle = {Proceedings 41st {Annual} {Symposium} on {Foundations} of {Computer} {Science}},
	author = {Auer, P.},
	month = nov,
	year = {2000},
	note = {ISSN: 0272-5428},
	keywords = {Algorithm design and analysis, Computer science, Information analysis, Learning, Random processes, Random variables, Statistical analysis, Uncertainty},
	pages = {270--279},
}

@article{chapelle_empirical_2011,
	title = {An {Empirical} {Evaluation} of {Thompson} {Sampling}},
	abstract = {Thompson sampling is one of oldest heuristic to address the exploration / exploitation trade-off, but it is surprisingly unpopular in the literature. We present here some empirical results using Thompson sampling on simulated and real data, and show that it is highly competitive. And since this heuristic is very easy to implement, we argue that it should be part of the standard baselines to compare against.},
	language = {en},
	author = {Chapelle, Olivier and Li, Lihong},
	year = {2011},
	pages = {9},
}

@article{robbins_aspects_1952,
	title = {Some aspects of the sequential design of experiments},
	volume = {58},
	issn = {0002-9904, 1936-881X},
	url = {https://www.ams.org/bull/1952-58-05/S0002-9904-1952-09620-8/},
	doi = {10.1090/S0002-9904-1952-09620-8},
	abstract = {Advancing research. Creating connections.},
	language = {en},
	number = {5},
	urldate = {2022-05-12},
	journal = {Bulletin of the American Mathematical Society},
	author = {Robbins, Herbert},
	year = {1952},
	pages = {527--535},
}

@article{berry_bandit_1997,
	title = {Bandit {Problems} {With} {Infinitely} {Many} {Arms}},
	volume = {25},
	doi = {10.1214/aos/1069362389},
	abstract = {We consider a bandit problem consisting of a sequence of n choices from an infinite number of Bernoulli arms, with \$n {\textbackslash}to {\textbackslash}infty\$. The objective is to minimize the long-run failure rate. The Bernoulli parameters are independent observations from a distribution F. We first assume F to be the uniform distribution on (0, 1) and consider various extensions. In the uniform case we show that the best lower bound for the expected failure proportion is between \${\textbackslash}sqrt\{2\}/{\textbackslash}sqrt\{n\}\$ and \$2/{\textbackslash}sqrt\{n\}\$ and we exhibit classes of strategies that achieve the latter.},
	journal = {The Annals of Statistics},
	author = {Berry, Donald and Chen, Robert and Zame, Alan and Heath, David and Shepp, Larry},
	month = oct,
	year = {1997},
}

@article{mandelbaum_continuous_1987,
	title = {Continuous {Multi}-{Armed} {Bandits} and {Multiparameter} {Processes}},
	volume = {15},
	issn = {0091-1798, 2168-894X},
	url = {https://projecteuclid.org/journals/annals-of-probability/volume-15/issue-4/Continuous-Multi-Armed-Bandits-and-Multiparameter-Processes/10.1214/aop/1176991992.full},
	doi = {10.1214/aop/1176991992},
	abstract = {A general framework is proposed for continuous time dynamic allocation models of a scarce resource among competing projects. The allocation model is formulated as a multi-armed bandit model and solved as a control problem of a multiparameter process. In contrast to discrete time bandits, where only one arm can be pulled at a time, the continuous time bandit must allow simultaneous pulls. The multiparameter approach allows a strong solution of diffusion-type bandits. Here the main problem is to define precisely how to switch among arms and the solution involves local times.},
	number = {4},
	urldate = {2022-05-12},
	journal = {The Annals of Probability},
	author = {Mandelbaum, Avi},
	month = oct,
	year = {1987},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {60G17, 60J55, 60J60, 60K10, 62L99, 93E20, Diffusions, Dynamic allocation, Gittins' index, Local time, Multi-armed bandits, Stochastic control, multiparameter processes, optional increasing path},
	pages = {1527--1556},
}

@article{colley_colleys_nodate,
	title = {Colleyâs {Bias} {Free} {College} {Football} {Ranking} {Method}:},
	url = {http://www.colleyrankings.com/matrate.pdf},
	abstract = {Colleyâs matrix method for ranking college football teams is explained in detail, with many examples and explicit derivations. The method is based on very simple statistical principles, and uses only Div. I-A wins and losses as input â margin of victory does not matter. The scheme adjusts effectively for strength of schedule, in a way that is free of bias toward conference, tradition, or region. Comparison of rankings produced by this method to those produced by the press polls shows that despite its simplicity, the scheme produces common sense results.},
	language = {en},
	author = {Colley, Wesley N},
	pages = {23},
}

@article{scarf_numerical_2009,
	title = {A {Numerical} {Study} of {Designs} for {Sporting} {Contests}},
	volume = {198},
	doi = {10.1016/j.ejor.2008.07.029},
	abstract = {Operational Research may be used to compare different designs for a sporting contest or tournament. This paper considers a methodology for this purpose. We propose a number of tournament metrics that can be used to measure the success of a sporting contest or tournament, and describe how these metrics may be evaluated for a particular tournament design. Knowledge of these measures can then be used to compare competing designs, such as round-robin, pure knockout and hybrids of these designs. We show, for example, how the design of the tournament influences the outcome uncertainty of the tournament and the number of unimportant matches within the tournament. In this way, where new designs are proposed, the implications of these designs may be explored within a modelling paradigm. In football (soccer), the UEFA Champions League has adopted a number of designs over its 50 year history; the design of the tournament has been modified principally in response to the changing demands of national league football and television â the paper uses this particular tournament to illustrate the methodology.},
	journal = {European Journal of Operational Research},
	author = {Scarf, Phil and Yusof, Muhammad and Bilbao, Mark},
	month = oct,
	year = {2009},
	pages = {190--198},
}

@article{scarf_numerical_2011,
	title = {A numerical study of tournament structure and seeding policy for the soccer {World} {Cup} {Finals}},
	volume = {65},
	doi = {10.1111/j.1467-9574.2010.00471.x},
	abstract = {Tournament outcome uncertainty depends on: the design of the tournament; and the relative strengths of the competitors â the competitive balance. A tournament design comprises the arrangement of the individual matches, which we call the tournament structure, the seeding policy and the progression rules. In this paper, we investigate the effect of seeding policy for various tournament structures, while taking account of competitive balance. Our methodology uses tournament outcome uncertainty to consider the effect of seeding policy and other design changes. The tournament outcome uncertainty is measured using the tournament outcome characteristic which is the probability Pq,R that a team in the top 100q pre-tournament rank percentile progresses forward from round R, for all q and R. We use Monte Carlo simulation to calculate the values of this metric. We find that, in general, seeding favours stronger competitors, but that the degree of favouritism varies with the type of seeding. Reseeding after each round favours the strong to the greatest extent. The ideas in the paper are illustrated using the soccer World Cup Finals tournament.},
	journal = {Statistica Neerlandica},
	author = {Scarf, Phil and Yusof, M.},
	month = feb,
	year = {2011},
	pages = {43--57},
}

@article{arrow_difficulty_1950,
	title = {A {Difficulty} in the {Concept} of {Social} {Welfare}},
	volume = {58},
	issn = {0022-3808},
	url = {https://www.journals.uchicago.edu/doi/10.1086/256963},
	doi = {10.1086/256963},
	number = {4},
	urldate = {2022-05-12},
	journal = {Journal of Political Economy},
	author = {Arrow, Kenneth J.},
	month = aug,
	year = {1950},
	note = {Publisher: The University of Chicago Press},
	pages = {328--346},
}

@article{james_p_keener_perron-frobenius_1993,
	title = {The {Perron}-{Frobenius} {Theorem} and the {Ranking} of {Football} {Teams}},
	volume = {35},
	abstract = {The author describes four different methods to rank teams in an uneven paired competition and shows how each of these methods depends in some fundamental way on the Perron-Frobenius theorem.},
	number = {1},
	journal = {SIAM Review},
	author = {{James P. Keener}},
	month = mar,
	year = {1993},
	pages = {80--93},
}

@misc{kenneth_massey_massey_nodate,
	title = {Massey {Ratings} - {Sports} {Computer} {Ratings}, {Scores}, and {Analysis}},
	url = {https://masseyratings.com/},
	urldate = {2022-05-12},
	author = {{Kenneth Massey}},
}

@article{glenn_comparison_1960,
	title = {A {Comparison} of the {Effectiveness} of {Tournaments}},
	volume = {47},
	issn = {0006-3444},
	url = {https://www.jstor.org/stable/2333297},
	doi = {10.2307/2333297},
	number = {3/4},
	urldate = {2022-05-12},
	journal = {Biometrika},
	author = {Glenn, W. A.},
	year = {1960},
	note = {Publisher: [Oxford University Press, Biometrika Trust]},
	pages = {253--262},
}

@book{csato_role_2021,
	title = {On the role of tournament design in sporting success: {A} study of the {North}, {Central} {American} and {Caribbean} qualification for the 2022 {FIFA} {World} {Cup}},
	shorttitle = {On the role of tournament design in sporting success},
	abstract = {Playing in the FIFA World Cup finals is an ambition shared by several nations. Since, besides luck and skill, the probability of qualification depends on the design of the qualifiers, the study of these competitions forms an integral part of sports analytics. The Confederation of North, Central America and Caribbean Association Football (CONCACAF) announced a novel qualifying format for the 2022 FIFA World Cup in July 2019. However, the COVID-19 pandemic forced the organisers to return to a more traditional structure. The present chapter analyses how this reform impacted the chances of the national teams to qualify. It is found that the probability of participating in the FIFA World Cup finals can change by more than 5 percentage points under the assumption of fixed strengths for the teams. The idea behind the original design, to divide the contestants into two distinct sets, is worth considering due to the increased competitiveness of the matches played by the strongest and the weakest teams. We recommend mitigating the sharp nonlinearity caused by the seeding policy via a probabilistic rule to the analogy of the NBA draft lottery system.},
	author = {Csato, Laszlo},
	month = oct,
	year = {2021},
}

@article{searls_probability_2012,
	title = {On the {Probability} of {Winning} with {Different} {Tournament} {Procedures}},
	copyright = {Copyright Taylor and Francis Group, LLC},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1963.10480688},
	abstract = {Paired comparisons among competing items are analogous to games between contestants and the process of determining the winning item among t contestants is analogous to a tournament. In this paper f...},
	language = {en},
	urldate = {2022-05-12},
	journal = {Journal of the American Statistical Association},
	author = {Searls, Donald T.},
	month = apr,
	year = {2012},
	note = {Publisher: Taylor \& Francis Group},
}

@book{sziklai_efficacy_2021,
	title = {The efficacy of tournament designs},
	abstract = {Tournaments are a widely used mechanism to rank alternatives in a noisy environment. We investigate a fundamental issue of economics in tournament design: what is the best usage of limited resources, that is, how should the alternatives be compared pairwise to best approximate their true but latent ranking. We consider various formats including knockout tournaments, multi-stage championships consisting of round-robin groups followed by single elimination, and the Swiss-system. They are evaluated via Monte-Carlo simulations under six different assumptions on winning probabilities. Comparing the same pairs of alternatives multiple times turns out to be an inefficacious policy. The Swiss-system is found to be the most accurate among all these designs, especially in its ability to rank all participants. A possible explanation is that it does not eliminate an alternative after a single loss, while it takes the history of the comparisons into account. Hence, this particular format may deserve more attention from the decision-makers such as the governing bodies of major sports.},
	author = {Sziklai, BalÃ¡zs and BirÃ³, PÃ©ter and Csato, Laszlo},
	month = mar,
	year = {2021},
}

@article{horen_comparing_1985,
	title = {Comparing {Draws} for {Single} {Elimination} {Tournaments}},
	volume = {33},
	issn = {0030-364X},
	url = {https://pubsonline.informs.org/doi/abs/10.1287/opre.33.2.249},
	doi = {10.1287/opre.33.2.249},
	abstract = {Tournaments are used to select a single winner from a group of participants in a sporting event or a paired-comparison experiment. This study compares different draws, or pairings, of teams in single-elimination tournaments under a set of relatively nonrestricting assumptions about the participating teams' pairwise probabilities of winning. We analyze and compare draws for four-team tournaments using various criteria, then attempt to generalize the results to eight-team tournaments. For example, only one-four team draw maximizes the probability that the best team wins for all pairwise probabilities, whereas eight draws are possibly optimal for eight-team tournaments.},
	number = {2},
	urldate = {2022-05-12},
	journal = {Operations Research},
	author = {Horen, Jeff and Riezman, Raymond},
	month = apr,
	year = {1985},
	note = {Publisher: INFORMS},
	keywords = {237 pairing teams in single-elimination tournaments, 570 pairing teams in single-elimination tournaments},
	pages = {249--262},
}

@book{david_method_1963,
	address = {New York},
	title = {The method of paired comparisons},
	language = {English},
	publisher = {Hafner Pub. Co.},
	author = {David, H. A},
	year = {1963},
	note = {OCLC: 2107241},
}

@article{chung_stronger_1978,
	title = {Do {Stronger} {Players} {Win} {More} {Knockout} {Tournaments}?},
	volume = {73},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/2286606},
	doi = {10.2307/2286606},
	abstract = {We study knockout tournaments using a set of players with which a (pairwise) preference scheme is associated. We show that if the preference scheme is of the Bradley-Terry type, then for any knockout tournament, if player i has a larger merit value than player j, he also has a greater probability of winning the tournament. When the preference scheme is not of the Bradley-Terry type, counter-examples are given for several of the main results.},
	number = {363},
	urldate = {2022-05-12},
	journal = {Journal of the American Statistical Association},
	author = {Chung, F. R. K. and Hwang, F. K.},
	year = {1978},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {593--596},
}

@article{david_stronger_1998,
	title = {Stronger {Opposition} {May} {Make} {It} {Easier} to {Win}},
	volume = {52},
	issn = {0003-1305},
	url = {https://www.jstor.org/stable/2685440},
	doi = {10.2307/2685440},
	abstract = {Suppose that in a three-player tournament the strongest player, A1, meets the winner of the match between A2 and the weakest player, A3. It is shown that, for A1 and A2 of fixed strength, A1's chances of winning may be increased if A3 is replaced by a stronger player. The paradox is studied with special emphasis on probability models important in the method of paired comparisons.},
	number = {4},
	urldate = {2022-05-12},
	journal = {The American Statistician},
	author = {David, H. A.},
	year = {1998},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {351--353},
}

@article{oliveira_new_nodate,
	title = {A {New} and {Flexible} {Approach} to the {Analysis} of {Paired} {Comparison} {Data}},
	abstract = {We consider the situation where I items are ranked by paired comparisons. It is usually assumed that the probability that item i is preferred over item j is pij = F (Âµi âÂµj) where F is a symmetric distribution function, which we refer to as the comparison function, and Âµi and Âµj are the merits or scores of the compared items. This modelling framework, which is ubiquitous in the paired comparison literature, strongly depends on the assumption that the comparison function F is known. In practice, however, this assumption is often unrealistic and may result in poor ï¬t and erroneous inferences. This limitation has motivated us to relax the assumption that F is fully known and simultaneously estimate the merits of the objects and the underlying comparison function. Our formulation yields a ï¬exible semi-deï¬nite programming problem that we use as a reï¬nement step for estimating the paired comparison probability matrix. We provide a detailed sensitivity analysis and, as a result, we establish the consistency of the resulting estimators and provide bounds on the estimation and approximation errors. Some statistical properties of the resulting estimators as well as model selection criteria are investigated. Finally, using a large data-set of computer chess matches, we estimate the comparison function and ï¬nd that the model used by the International Chess Federation does not seem to apply to computer chess.},
	language = {en},
	author = {Oliveira, Ivo F D and Ailon, Nir and Davidov, Ori},
	pages = {29},
}

@misc{page_pagerank_1999,
	type = {Techreport},
	title = {The {PageRank} {Citation} {Ranking}: {Bringing} {Order} to the {Web}.},
	shorttitle = {The {PageRank} {Citation} {Ranking}},
	url = {http://ilpubs.stanford.edu:8090/422/},
	abstract = {The importance of a Web page is an inherently subjective matter, which depends on the readers interests, knowledge and attitudes. But there is still much that can be said objectively about the relative importance of Web pages. This paper describes PageRank, a mathod for rating Web pages objectively and mechanically, effectively measuring the human interest and attention devoted to them. We compare PageRank to an idealized random Web surfer. We show how to efficiently compute PageRank for large numbers of pages. And, we show how to apply PageRank to search and to user navigation.},
	urldate = {2022-05-12},
	author = {Page, Lawrence and Brin, Sergey and Motwani, Rajeev and Winograd, Terry},
	month = nov,
	year = {1999},
	note = {Publisher: Stanford InfoLab},
}

@phdthesis{anjela_yuryevna_govan_ranking_2008,
	address = {North Carolina},
	title = {Ranking {Theory} with {Application} to {Popular} {Sports}},
	url = {https://repository.lib.ncsu.edu/handle/1840.16/3102},
	abstract = {The rank of an object is its relative importance to the other objects in a finite set of size n. Often a rank is an integer assigned from the set \{ 1,2,...,n\}. Ideally an assignment of available ranks (\{1,2,...,n\}) to n objects is one-to-one. However in certain circumstances it is possible that more than one object is assigned the same rank. A ranking model is a method of determining a way in which the ranks are assigned. Typically a ranking model uses information available to determine a rating for each object. The ratings carry more information than the ranks; they provide us with the degree of relative importance of each object. Once we have the ratings the assignment of ranks can be as simple as sorting the objects in descending order of the corresponding ratings. Ranking models can be used for a number of applications such as sports, web search, literature search, etc. The type of ranking investigated in this work has close ties with the Method of Paired Comparison. Oftentimes the information that is the easiest to obtain or naturally available is the relative preference of the objects taken two at a time. The information is then summarized in a weighted directed graph and hence as the corresponding matrix. A number of ranking models makes use of the matrix representation of paired comparisons to compute ratings of the individual objects. Two ranking models proposed and investigated in this work start with forming nonegative matrices that do represent certain pairwise type comparisons. The models have different approaches to computing the rating scores. The Offense-Defense Model makes use of the Sinkhorn-Knopp Theorem on equivalence matrix balancing, whereas the Generalized Markov model is based on Markov Chain theory. Both models are then used to compute the ratings of the National Football League teams, National Collegiate Athletic Association football and basketball teams. The ratings are used to perform game predictions. The proposed models are not specific to sports and can be applied to any situation consisting of a set of objects and a set of pairwise information. However, picking team sports as a ranking application allows for unrestricted access of free and abundant data. The game predictions experiments consisted of both foresight and hindsight predictions. All the experiments included the proposed models as well as several current sports ranking methods.},
	school = {North Carolina State University},
	author = {Anjela Yuryevna Govan},
	month = dec,
	year = {2008},
}

@article{bao_analysis_2016,
	title = {An {Analysis} of {Tournament} {Structure}},
	url = {http://arxiv.org/abs/1611.08499},
	abstract = {This paper explores a novel way for analyzing the tournament structures to find a best suitable one for the tournament under consideration. It concerns about three aspects such as tournament conducting cost, competitiveness development and ranking precision. It then proposes a new method using progress tree to detect potential throwaway matches. The analysis performed using the proposed method reveals the strengths and weaknesses of tournament structures. As a conclusion, single elimination is best if we want to qualify one winner only, all matches conducted are exciting in term of competitiveness. Double elimination with proper seeding system is a better choice if we want to qualify more winners. A reasonable number of extra matches need to be conducted in exchange of being able to qualify top four winners. Round-robin gives reliable ranking precision for all participants. However, its conduction cost is very high, and it fails to maintain competitiveness development.},
	urldate = {2022-05-11},
	journal = {arXiv:1611.08499 [cs]},
	author = {Bao, Nhien Pham Hoang and Iida, Hiroyuki},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.08499},
	keywords = {Computer Science - Artificial Intelligence},
}

@misc{noauthor_figure_nodate,
	title = {Figure 4: {Progress} tree of double elimination tournament for 8...},
	shorttitle = {Figure 4},
	url = {https://www.researchgate.net/figure/Progress-tree-of-double-elimination-tournament-for-8-participants_fig4_310953167},
	abstract = {Download scientific diagram {\textbar} Progress tree of double elimination tournament for 8 participants Â  from publication: An Analysis of Tournament Structure {\textbar} This paper explores a novel way for analyzing the tournament structures to find a best suitable one for the tournament under consideration. It concerns about three aspects such as tournament conducting cost, competitiveness development and ranking precision. It then proposes... {\textbar} Competitiveness and Strength {\textbar} ResearchGate, the professional network for scientists.},
	language = {en},
	urldate = {2022-05-11},
	journal = {ResearchGate},
}

@techreport{john_bullinaria_lecture_2019,
	title = {Lecture {Notes} for {Data} {Structures} and {Algorithms}},
	url = {https://www.cs.bham.ac.uk/~jxb/dsa.html},
	institution = {School of Computer Science, University of Birmingham},
	author = {{John Bullinaria}},
	month = mar,
	year = {2019},
}

@misc{sambowyer_tournaments_2022,
	title = {Tournaments},
	url = {https://github.com/sambowyer/tournaments},
	abstract = {Code for running and analysing various tournament systems for a fourth year MSci project in mathematics.},
	urldate = {2022-05-11},
	author = {sambowyer},
	month = mar,
	year = {2022},
	note = {original-date: 2022-03-25T15:40:48Z},
}

@article{esperet_acyclic_2013,
	title = {Acyclic edge-coloring using entropy compression},
	volume = {34},
	issn = {01956698},
	url = {http://arxiv.org/abs/1206.1535},
	doi = {10.1016/j.ejc.2013.02.007},
	abstract = {An edge-coloring of a graph G is acyclic if it is a proper edge-coloring of G and every cycle contains at least three colors. We prove that every graph with maximum degree â has an acyclic edge-coloring with at most 4â â 4 colors, improving the previous bound of â9.62 (â â 1)â. Our bound results from the analysis of a very simple randomised procedure using the so-called entropy compression method. We show that the expected running time of the procedure is O(mnâ2 log â), where n and m are the number of vertices and edges of G. Such a randomised procedure running in expected polynomial time was only known to exist in the case where at least 16â colors were available.},
	language = {en},
	number = {6},
	urldate = {2022-04-24},
	journal = {European Journal of Combinatorics},
	author = {Esperet, Louis and Parreau, Aline},
	month = aug,
	year = {2013},
	note = {arXiv: 1206.1535},
	keywords = {Mathematics - Combinatorics},
	pages = {1019--1027},
}

@article{kolmogorov_extensions_2019,
	title = {Extensions of the {Algorithmic} {Lovasz} {Local} {Lemma}},
	abstract = {These papers analyze a random walk algorithm for finding objects that avoid undesired "bad events" (or "flaws"), and prove that under certain conditions the algorithm is guaranteed to find a "flawless" object quickly. We consider recent formulations of the algorithmic Lovasz Local Lemma by Achlioptas-Iliopoulos-Kolmogorov [2] and by Achlioptas-Iliopoulos-Sinclair [3]. These papers analyze a random walk algorithm for finding objects that avoid undesired "bad events" (or "flaws"), and prove that under certain conditions the algorithm is guaranteed to find a "flawless" object quickly. We show that conditions proposed in these papers are incomparable, and introduce a new family of conditions that includes those in [2, 3] as special cases. 
Secondly, we extend our previous notion of "commutativity" in [15] to this more general setting, and prove that it allows to use an arbitrary strategy for selecting the next flaw to address. In the special case of primary flaws we prove a stronger property: the flaw selection strategy does not affect at all the expected number of steps until termination, and also does not affect the distribution induced by the algorithm upon termination. This applies, in particular, to the single-clause backtracking algorithm for constraint satisfaction problems considered in [3].},
	journal = {ArXiv},
	author = {Kolmogorov, V.},
	year = {2019},
}

@article{molloy_list_2018,
	title = {The list chromatic number of graphs with small clique number},
	url = {http://arxiv.org/abs/1701.09133},
	abstract = {We prove that every triangle-free graph with maximum degree \${\textbackslash}Delta\$ has list chromatic number at most \$(1+o(1)){\textbackslash}frac\{{\textbackslash}Delta\}\{{\textbackslash}ln {\textbackslash}Delta\}\$. This matches the best-known bound for graphs of girth at least 5. We also provide a new proof that for any \$r{\textbackslash}geq 4\$ every \$K\_r\$-free graph has list-chromatic number at most \$200r{\textbackslash}frac\{{\textbackslash}Delta{\textbackslash}ln{\textbackslash}ln{\textbackslash}Delta\}\{{\textbackslash}ln{\textbackslash}Delta\}\$.},
	language = {en},
	urldate = {2022-04-24},
	journal = {arXiv:1701.09133 [math]},
	author = {Molloy, Michael},
	month = jun,
	year = {2018},
	note = {arXiv: 1701.09133},
	keywords = {Mathematics - Combinatorics},
}

@article{spencer_asymptotic_1977,
	title = {Asymptotic lower bounds for {Ramsey} functions},
	volume = {20},
	issn = {0012-365X},
	url = {https://www.sciencedirect.com/science/article/pii/0012365X77900449},
	doi = {10.1016/0012-365X(77)90044-9},
	abstract = {A probability theorem, due to Lovasz, is used to derive lower bounds for various Ramsey functions. A short proof of the known result R(3, t) â©¾ ct2(ln t)2 is given.},
	language = {en},
	urldate = {2022-04-22},
	journal = {Discrete Mathematics},
	author = {Spencer, Joel},
	month = jan,
	year = {1977},
	pages = {69--76},
}

@misc{institute_for_advanced_study_algorithmic_2015,
	title = {Algorithmic proof of the {Lovasz} {Local} {Lemma} via resampling oracles -{Vondrak}},
	url = {https://www.youtube.com/watch?v=dsEzZSvEKoA},
	abstract = {https://www.math.ias.edu/seminars/abs...},
	urldate = {2022-04-22},
	author = {{Institute for Advanced Study}},
	month = oct,
	year = {2015},
}

@article{chung_diameter_2001,
	title = {The {Diameter} of {Sparse} {Random} {Graphs}},
	volume = {26},
	issn = {0196-8858},
	url = {https://www.sciencedirect.com/science/article/pii/S0196885801907201},
	doi = {10.1006/aama.2001.0720},
	abstract = {We consider the diameter of a random graph G(n,p) for various ranges of p close to the phase transition point for connectivity. For a disconnected graph G, we use the convention that the diameter of G is the maximum diameter of its connected components. We show that almost surely the diameter of random graph G(n,p) is close to lognlog(np) if npââ. Moreover if nplogn=c{\textgreater}8, then the diameter of G(n,p) is concentrated on two values. In general, if nplogn=c{\textgreater}c0, the diameter is concentrated on at most 2â1/c0â+4 values. We also proved that the diameter of G(n,p) is almost surely equal to the diameter of its giant component if np{\textgreater}3.6.},
	language = {en},
	number = {4},
	urldate = {2022-04-22},
	journal = {Advances in Applied Mathematics},
	author = {Chung, Fan and Lu, Linyuan},
	month = may,
	year = {2001},
	pages = {257--279},
}

@article{klee_diameters_1981,
	title = {Diameters of {Random} {Graphs}},
	volume = {33},
	issn = {0008-414X, 1496-4279},
	url = {https://www.cambridge.org/core/journals/canadian-journal-of-mathematics/article/diameters-of-random-graphs/5814B8500B407B6ACE756C51F1DEA50C},
	doi = {10.4153/CJM-1981-050-1},
	abstract = {For two nodes x and y of a graph G, the distance Î´G(x,y) is the smallest integer k such that k edges form a path from x to y; Î´G(x, x) = 0, and Î´G(x,y) = â when x â  y and there is no path from x to y. The diameter Î´G
                is the maximum of Î´G
               (x, y) as x and y range over the nodes of G. When G is connected, Î´(G) is the smallest integer k such that any two nodes of G can be joined by a path formed from at most k edges. When G is not connected, Î´(G) = â and there is interest in Î´c(G), the maximum of Î´(G) over the components C of G.
            For 2 â§ n {\textless} â and 0 â§ E â§ n(n â l)/2, let  denote the set of all loopless undirected graphs with the node-set \{1, â¦, n\} and exactly E edges.},
	language = {en},
	number = {3},
	urldate = {2022-04-22},
	journal = {Canadian Journal of Mathematics},
	author = {Klee, Victor and Larman, David},
	month = jun,
	year = {1981},
	note = {Publisher: Cambridge University Press},
	pages = {618--640},
}

@article{p_erdos_random_1959,
	title = {On {Random} {Graphs} {I}},
	volume = {6},
	journal = {Publicationes Mathematicae Debrecen},
	author = {P. ErdÃ¶s, A. RÃ©yni},
	year = {1959},
	pages = {290},
}

@article{albert_statistical_2002,
	title = {Statistical mechanics of complex networks},
	volume = {74},
	url = {https://link.aps.org/doi/10.1103/RevModPhys.74.47},
	doi = {10.1103/RevModPhys.74.47},
	abstract = {Complex networks describe a wide range of systems in nature and society. Frequently cited examples include the cell, a network of chemicals linked by chemical reactions, and the Internet, a network of routers and computers connected by physical links. While traditionally these systems have been modeled as random graphs, it is increasingly recognized that the topology and evolution of real networks are governed by robust organizing principles. This article reviews the recent advances in the field of complex networks, focusing on the statistical mechanics of network topology and dynamics. After reviewing the empirical data that motivated the recent interest in networks, the authors discuss the main models and analytical tools, covering random graphs, small-world and scale-free networks, the emerging theory of evolving networks, and the interplay between topology and the networkâs robustness against failures and attacks.},
	number = {1},
	urldate = {2022-04-21},
	journal = {Reviews of Modern Physics},
	author = {Albert, RÃ©ka and BarabÃ¡si, Albert-LÃ¡szlÃ³},
	month = jan,
	year = {2002},
	note = {Publisher: American Physical Society},
	pages = {47--97},
}

@article{dorogovtsev_evolution_2002,
	title = {Evolution of networks},
	volume = {51},
	issn = {0001-8732},
	url = {https://doi.org/10.1080/00018730110112519},
	doi = {10.1080/00018730110112519},
	abstract = {We review the recent rapid progress in the statistical physics of evolving networks. Interest has focused mainly on the structural properties of complex networks in communications, biology, social sciences and economics. A number of giant artificial networks of this kind have recently been created, which opens a wide field for the study of their topology, evolution, and the complex processes which occur in them. Such networks possess a rich set of scaling properties. A number of them are scale-free and show striking resilience against random breakdowns. In spite of the large sizes of these networks, the distances between most of their vertices are short - a feature known as the 'small-world' effect. We discuss how growing networks self-organize into scale-free structures, and investigate the role of the mechanism of preferential linking. We consider the topological and structural properties of evolving networks, and percolation and disease spread on these networks. We present a number of models demonstrating the main features of evolving networks and discuss current approaches for their simulation and analytical study. Applications of the general results to particular networks in nature are discussed. We demonstrate the generic connections of the network growth processes with the general problems of non-equilibrium physics, econophysics, evolutionary biology, and so on.},
	number = {4},
	urldate = {2022-04-21},
	journal = {Advances in Physics},
	author = {Dorogovtsev, S. N. and Mendes, J. F. F.},
	month = jun,
	year = {2002},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00018730110112519},
	pages = {1079--1187},
}

@article{yang_application_2020,
	title = {Application of {Modified} {NSGA}-{II} to the {Transit} {Network} {Design} {Problem}},
	volume = {2020},
	issn = {0197-6729, 2042-3195},
	url = {https://www.hindawi.com/journals/jat/2020/3753601/},
	doi = {10.1155/2020/3753601},
	abstract = {The transit network design problem involves determining a certain number of routes to operate in an urban area to balance the costs of the passengers and the operator. In this paper, we simultaneously determine the route structure of each route and the number of routes in the final solution. A novel initial route set generation algorithm and a route set size alternating heuristic are embedded into a nondominated sorting genetic algorithm-II- (NSGA-II-) based solution framework to produce the approximate Pareto front. The initial route set generation algorithm aims to generate high-quality initial solutions for succeeding optimization procedures. To explore the solution space and to have solutions with a different number of routes, a route set size alternating heuristic is developed to change the number of routes in a solution by adding or deleting one route. Experiments were performed on Mandlâs network and four larger Mumfordâs networks. Compared with a fixed route set size approach, the proposed NSGA-II-based solution method can produce an approximate Pareto front with much higher solution quality as well as improved computation efficiency.},
	language = {en},
	urldate = {2022-04-21},
	journal = {Journal of Advanced Transportation},
	author = {Yang, Jie and Jiang, Yangsheng},
	month = aug,
	year = {2020},
	pages = {1--24},
}

@article{zakirov_nsga-ii_2017,
	title = {{NSGA}-{II} for biological graph compression},
	volume = {9},
	issn = {13147668},
	url = {http://www.m-hikari.com/asb/asb2017/asb1-4-2017/61143.html},
	doi = {10.12988/asb.2017.61143},
	abstract = {Examinations of a common biological reference organism, (E. coli), demonstrate that NSGA-II is able to provide a series of compressions at various ratios, allows a biologist to examine the organismâs connective networks with a measure of certainty of connectiveness. This is due to a novel method of scoring the similarity of the compressed network to the origional during the graphâs creation based on the number of false links added to the graph during the compression method.},
	language = {en},
	urldate = {2022-04-21},
	journal = {Advanced Studies in Biology},
	author = {Zakirov, A. N. and Brown, J. A.},
	year = {2017},
	pages = {1--7},
}

@article{floyd_algorithm_1962,
	title = {Algorithm 97: {Shortest} path},
	volume = {5},
	issn = {0001-0782},
	shorttitle = {Algorithm 97},
	url = {https://doi.org/10.1145/367766.368168},
	doi = {10.1145/367766.368168},
	number = {6},
	urldate = {2022-04-21},
	journal = {Communications of the ACM},
	author = {Floyd, Robert W.},
	month = jun,
	year = {1962},
	pages = {345},
}

@inproceedings{kolipaka_moser_2011,
	address = {San Jose, California, USA},
	title = {Moser and tardos meet {LovÃ¡sz}},
	isbn = {978-1-4503-0691-1},
	url = {http://portal.acm.org/citation.cfm?doid=1993636.1993669},
	doi = {10.1145/1993636.1993669},
	language = {en},
	urldate = {2022-04-20},
	booktitle = {Proceedings of the 43rd annual {ACM} symposium on {Theory} of computing - {STOC} '11},
	publisher = {ACM Press},
	author = {Kolipaka, Kashyap Babu Rao and Szegedy, Mario},
	year = {2011},
	pages = {235},
}

@article{achlioptas_random_2014,
	title = {Random {Walks} {That} {Find} {Perfect} {Objects} and the {Lovasz} {Local} {Lemma}},
	doi = {10.1145/2818352},
	abstract = {The key point is that algorithmic progress is measured in terms of entropy rather than energy so that termination can be established even under the proliferation of states in which every step of the algorithm (random walk) increases the total number of violated constraints. We give an algorithmic local lemma by establishing a sufficient condition for the uniform random walk on a directed graph to reach a sink quickly. Our work is inspired by Moser's entropic method proof of the Lovasz Local Lemma (LLL) for satisfiability and completely bypasses the Probabilistic Method formulation of the LLL. In particular, our method works when the underlying state space is entirely unstructured. Similarly to Moser's argument, the key point is that algorithmic progress is measured in terms of entropy rather than energy (number of violated constraints) so that termination can be established even under the proliferation of states in which every step of the algorithm (random walk) increases the total number of violated constraints.},
	journal = {2014 IEEE 55th Annual Symposium on Foundations of Computer Science},
	author = {Achlioptas, D. and Iliopoulos, F.},
	year = {2014},
}

@inproceedings{harris_constructive_2014,
	title = {A constructive algorithm for the {LovÃ¡sz} {Local} {Lemma} on permutations},
	isbn = {978-1-61197-338-9 978-1-61197-340-2},
	url = {https://epubs.siam.org/doi/10.1137/1.9781611973402.68},
	doi = {10.1137/1.9781611973402.68},
	abstract = {While there has been signiï¬cant progress on algorithmic aspects of the LovÂ´asz Local Lemma (LLL) in recent years, a noteworthy exception is when the LLL is used in the context of random permutations. The breakthrough algorithm of Moser \& Tardos only works in the setting of independent variables, and does not apply in this context. We resolve this by developing a randomized polynomial-time algorithm for such applications. A noteworthy application is for Latin transversals: the best-known general result here (Bissacot et al., improving on ErdoËs and Spencer), states that any n Ã n matrix in which each entry appears at most (27/256)n times, has a Latin transversal. We present the ï¬rst polynomial-time algorithm to construct such a transversal. We also develop RNC algorithms for Latin transversals, rainbow Hamiltonian cycles, strong chromatic number, and hypergraph packing.},
	language = {en},
	urldate = {2022-04-20},
	booktitle = {Proceedings of the {Twenty}-{Fifth} {Annual} {ACM}-{SIAM} {Symposium} on {Discrete} {Algorithms}},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Harris, David G. and Srinivasan, Aravind},
	month = jan,
	year = {2014},
	pages = {907--925},
}

@article{newman_random_2001,
	title = {Random graphs with arbitrary degree distributions and their applications},
	volume = {64},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.64.026118},
	doi = {10.1103/PhysRevE.64.026118},
	abstract = {Recent work on the structure of social networks and the internet has focused attention on graphs with distributions of vertex degree that are significantly different from the Poisson degree distributions that have been widely studied in the past. In this paper we develop in detail the theory of random graphs with arbitrary degree distributions. In addition to simple undirected, unipartite graphs, we examine the properties of directed and bipartite graphs. Among other results, we derive exact expressions for the position of the phase transition at which a giant component first forms, the mean component size, the size of the giant component if there is one, the mean number of vertices a certain distance away from a randomly chosen vertex, and the average vertex-vertex distance within a graph. We apply our theory to some real-world graphs, including the world-wide web and collaboration graphs of scientists and Fortune 1000 company directors. We demonstrate that in some cases random graphs with appropriate distributions of vertex degree predict with surprising accuracy the behavior of the real world, while in others there is a measurable discrepancy between theory and reality, perhaps indicating the presence of additional social structure in the network that is not captured by the random graph., This article appears in the following collections:},
	number = {2},
	urldate = {2022-04-17},
	journal = {Physical Review E},
	author = {Newman, M. E. J. and Strogatz, S. H. and Watts, D. J.},
	month = jul,
	year = {2001},
	note = {Publisher: American Physical Society},
	pages = {026118},
}

@article{gilbert_random_1959,
	title = {Random {Graphs}},
	volume = {30},
	issn = {0003-4851, 2168-8990},
	url = {https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-30/issue-4/Random-Graphs/10.1214/aoms/1177706098.full},
	doi = {10.1214/aoms/1177706098},
	abstract = {The Annals of Mathematical Statistics},
	number = {4},
	urldate = {2022-04-17},
	journal = {The Annals of Mathematical Statistics},
	author = {Gilbert, E. N.},
	month = dec,
	year = {1959},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {1141--1144},
}

@inproceedings{kang_proof_2022,
	title = {A proof of the {ErdÃ¶s}-{Faber}-{LovÃ¡sz} conjecture: {Algorithmic} aspects},
	shorttitle = {A proof of the {ErdÃ¶s}-{Faber}-{LovÃ¡sz} conjecture},
	doi = {10.1109/FOCS52979.2021.00107},
	abstract = {The Erdos-Faber-LovÃ¡sz conjecture (posed in 1972) states that the chromatic index of any linear hypergraph on \$n\$ vertices is at most n. ErdÃ¶s considered this to be one of his three most favorite combinatorial problems and offered a \$500 reward for a proof of this conjecture. We prove this conjecture for every large n. Here, we also provide a randomised algorithm to find such a colouring in polynomial time with high probability.},
	booktitle = {2021 {IEEE} 62nd {Annual} {Symposium} on {Foundations} of {Computer} {Science} ({FOCS})},
	author = {Kang, Dong Yeap and Kelly, Tom and KÃ¼hn, Daniela and Methuku, Abhishek and Osthus, Deryk},
	month = feb,
	year = {2022},
	note = {ISSN: 2575-8454},
	keywords = {Computer science, ErdÃ¶s-Faber-LovÃ¡sz, Hypergraph colouring, RÃ¶dl nibble},
	pages = {1080--1089},
}

@article{chandrasekaran_deterministic_2013,
	title = {Deterministic {Algorithms} for the {LovÃ¡sz} {Local} {Lemma}},
	copyright = {Article is made available in accordance with the publisher's policy and may be subject to US copyright law. Please refer to the publisher's site for terms of use.},
	issn = {0097-5397},
	url = {https://dspace.mit.edu/handle/1721.1/85941},
	abstract = {The LovÃ¡sz local lemma (LLL) [P. ErdÃ¶s and L. LovÃ¡sz, Problems and results on 3-chromatic hypergraphs and some related questions, in Infinite and Finite Sets, Vol. II, A. Hajnal, R. Rado, and V. T. SÃ³s, eds., North--Holland, Amsterdam, 1975, pp. 609--627] is a powerful result in probability theory that informally states the following: the probability that none of a set of bad events happens is positive if the probability of each event is small compared to the number of events that depend on it. The LLL is often used for nonconstructive existence proofs of combinatorial structures. A prominent application is to \$k\$-CNF formulas, where the LLL implies that if every clause in a formula shares variables with at most \$d {\textbackslash}leq 2{\textasciicircum}k/e-1\$ other clauses, then such a formula has a satisfying assignment. Recently, a randomized algorithm to efficiently construct a satisfying assignment in this setting was given by Moser [A constructive proof of the LovÃ¡sz local lemma, in STOC '09: Proceedings of the 41st Annual ACM Symposium on Theory of Computing, ACM, New York, 2009, pp. 343--350]. Subsequently Moser and Tardos [J. ACM, 57 (2010), pp. 11:1--11:15] gave a general algorithmic framework for the LLL and a randomized algorithm within this framework to construct the structures guaranteed by the LLL. The main problem left open by Moser and Tardos was to design an efficient deterministic algorithm for constructing structures guaranteed by the LLL. In this paper we provide such an algorithm. Our algorithm works in the general framework of Moser and Tardos with a minimal loss in parameters. For the special case of constructing satisfying assignments for \$k\$-CNF formulas with \$m\$ clauses, where each clause shares variables with at most \$d {\textbackslash}leq 2{\textasciicircum}\{k/(1+{\textbackslash}epsilon)\}/e - 1\$ other clauses, for any \${\textbackslash}epsilon{\textbackslash}in (0,1)\$, we give a deterministic algorithm that finds a satisfying assignment in time \${\textbackslash}tilde\{O\}(m{\textasciicircum}\{2(1+1/{\textbackslash}epsilon)\})\$. This improves upon the deterministic algorithms of Moser and of Moser and Tardos with running times \$m{\textasciicircum}\{{\textbackslash}Omega(k{\textasciicircum}2)\}\$ and \$m{\textasciicircum}\{{\textbackslash}Omega(d {\textbackslash}log d)\}\$, respectively, which are superpolynomial for \$k={\textbackslash}omega(1)\$ and \$d={\textbackslash}omega(1)\$, and upon the previous best deterministic algorithm of Beck, which runs in polynomial time only for \$d{\textbackslash}leq 2{\textasciicircum}\{k/16\}/4\$. Our algorithm is the first deterministic algorithm that works in the general framework of Moser and Tardos. We also give a parallel NC algorithm for the same setting, improving upon an algorithm of Alon [Random Structures Algorithms, 2 (1991), pp. 367--378].},
	language = {en\_US},
	urldate = {2022-04-17},
	journal = {Society for Industrial and Applied Mathematics},
	author = {Chandrasekaran, Karthekeyan and Goyal, Navin and Haeupler, Bernhard},
	month = nov,
	year = {2013},
	note = {Accepted: 2014-03-28T13:39:52Z
Publisher: Society for Industrial and Applied Mathematics},
}

@inproceedings{giotis_algorithmic_2015,
	title = {On the {Algorithmic} {LovÃ¡sz} {Local} {Lemma} and {Acyclic} {Edge} {Coloring}},
	isbn = {978-1-61197-376-1},
	url = {https://epubs.siam.org/doi/10.1137/1.9781611973761.2},
	doi = {10.1137/1.9781611973761.2},
	abstract = {The algorithm for LovÂ´asz Local Lemma by Moser and Tardos gives a constructive way to prove the existence of combinatorial objects that satisfy a system of constraints. We present an alternative probabilistic analysis of the algorithm that does not involve reconstructing the history of the algorithm from the witness tree. We apply our technique to improve the best known upper bound to acyclic chromatic index. Speciï¬cally we show that a graph with maximum degree â has an acyclic proper edge coloring with at most â3.74(â â 1)â + 1 colors, whereas the previously known best bound was 4(â â 1). The same technique is also applied to improve corresponding bounds for graphs with bounded girth. An interesting aspect of this application is that the probability of the âundesirableâ events do not have a uniform upper bound, i.e. it constitutes a case of the asymmetric LovÂ´asz Local Lemma.},
	language = {en},
	urldate = {2022-04-17},
	booktitle = {2015 {Proceedings} of the {Twelfth} {Workshop} on {Analytic} {Algorithmics} and {Combinatorics} ({ANALCO})},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Giotis, Ioannis and Kirousis, Lefteris and Psaromiligkos, Kostas I. and Thilikos, Dimitrios M.},
	month = jan,
	year = {2015},
	pages = {16--25},
}

@article{beck_algorithmic_1991,
	title = {An algorithmic approach to the {LovÃ¡sz} local lemma. {I}},
	volume = {2},
	issn = {1098-2418},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/rsa.3240020402},
	doi = {10.1002/rsa.3240020402},
	abstract = {The LovÃ¡sz Local Lemma is a remarkable sieve method to prove the existence of certain structures without supplying any efficient way of finding these structures. In this article we convert some of the applications of the Local Lemma into polynomial time sequential algorithms (at the cost of a weaker constant factor in the âexponentâ). Our main example is the following: assume that in an n-uniform hypergraph every hyperedge intersects at most 2n/48 other hyperedges, then there is a polynomial time algorithm that finds a two-coloring of the points such that no hyperedge is monochromatic.},
	language = {en},
	number = {4},
	urldate = {2022-04-17},
	journal = {Random Structures \& Algorithms},
	author = {Beck, JÃ³zsef},
	year = {1991},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/rsa.3240020402},
	pages = {343--365},
}

@article{alon_parallel_1991,
	title = {A parallel algorithmic version of the local lemma},
	volume = {2},
	issn = {1098-2418},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/rsa.3240020403},
	doi = {10.1002/rsa.3240020403},
	abstract = {The Lovasz Local Lemma is a tool that enables one to show that certain events hold with positive, though very small probability. It often yields existence proofs of results without supplying any efficient way of solving the corresponding algorithmic problems. J. Beck recently has found a method for converting some of these existence proofs into efficient algorithmic procedures, at the cost of losing a little in the estimates. His method does not seem to be parallelizable. Here we modify his technique and achieve an algorithmic version that can be parallelized, thus obtaining deterministic NCl algorithms for several interesting algorithmic problems.},
	language = {en},
	number = {4},
	urldate = {2022-04-17},
	journal = {Random Structures \& Algorithms},
	author = {Alon, Noga},
	year = {1991},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/rsa.3240020403},
	pages = {367--378},
}

@article{moser_derandomizing_2008,
	title = {Derandomizing the {Lovasz} {Local} {Lemma} more effectively},
	url = {http://arxiv.org/abs/0807.2120},
	abstract = {The famous Lovasz Local Lemma [EL75] is a powerful tool to non-constructively prove the existence of combinatorial objects meeting a prescribed collection of criteria. Kratochvil et al. applied this technique to prove that a k-CNF in which each variable appears at most 2{\textasciicircum}k/(ek) times is always satisfiable [KST93]. In a breakthrough paper, Beck found that if we lower the occurrences to O(2{\textasciicircum}(k/48)/k), then a deterministic polynomial-time algorithm can find a satisfying assignment to such an instance [Bec91]. Alon randomized the algorithm and required O(2{\textasciicircum}(k/8)/k) occurrences [Alo91]. In [Mos06], we exhibited a refinement of his method which copes with O(2{\textasciicircum}(k/6)/k) of them. The hitherto best known randomized algorithm is due to Srinivasan and is capable of solving O(2{\textasciicircum}(k/4)/k) occurrence instances [Sri08]. Answering two questions asked by Srinivasan, we shall now present an approach that tolerates O(2{\textasciicircum}(k/2)/k) occurrences per variable and which can most easily be derandomized. The new algorithm bases on an alternative type of witness tree structure and drops a number of limiting aspects common to all previous methods.},
	urldate = {2022-04-17},
	journal = {arXiv:0807.2120 [cs]},
	author = {Moser, Robin A.},
	month = sep,
	year = {2008},
	note = {arXiv: 0807.2120},
	keywords = {Computer Science - Computational Complexity, Computer Science - Data Structures and Algorithms, F.2, G.2},
}

@inproceedings{guo_uniform_2017,
	address = {Montreal Canada},
	title = {Uniform sampling through the {Lovasz} local lemma},
	isbn = {978-1-4503-4528-6},
	url = {https://dl.acm.org/doi/10.1145/3055399.3055410},
	doi = {10.1145/3055399.3055410},
	abstract = {We propose a new algorithmic framework, called âpartial rejection samplingâ, to draw samples exactly from a product distribution, conditioned on none of a number of bad events occurring. Our framework builds (perhaps surprising) new connections between the variable framework of the LovÃ¡sz Local Lemma and some classical sampling algorithms such as the âcycle-poppingâ algorithm for rooted spanning trees by Wilson. Among other applications, we discover new algorithms to sample satisfying assignments of k-CNF formulas with bounded variable occurrences.},
	language = {en},
	urldate = {2022-04-17},
	booktitle = {Proceedings of the 49th {Annual} {ACM} {SIGACT} {Symposium} on {Theory} of {Computing}},
	publisher = {ACM},
	author = {Guo, Heng and Jerrum, Mark and Liu, Jingcheng},
	month = jun,
	year = {2017},
	pages = {342--355},
}

@article{fronczak_average_2004,
	title = {Average path length in uncorrelated random networks with hidden variables},
	volume = {70},
	issn = {1539-3755, 1550-2376},
	url = {http://arxiv.org/abs/cond-mat/0407098},
	doi = {10.1103/PhysRevE.70.056110},
	abstract = {Analytic solution for the average path length in a large class of uncorrelated random networks with hidden variables is found. We apply the approach to classical random graphs of Erdos and Renyi (ER), evolving networks introduced by Barabasi and Albert (BA) as well as random networks with asymptotic scale-free connectivity distributions characterized by an arbitrary scaling exponent \${\textbackslash}alpha{\textgreater}2\$. Our result for \$2{\textless}{\textbackslash}alpha{\textless}3\$ shows that structural properties of asymptotic scale-free networks including numerous examples of real-world systems are even more intriguing then ultra-small world behavior noticed in pure scale-free structures and for large system sizes \$N{\textbackslash}to{\textbackslash}infty\$ there is a saturation effect for the average path length.},
	number = {5},
	urldate = {2022-04-17},
	journal = {Physical Review E},
	author = {Fronczak, Agata and Fronczak, Piotr and Holyst, Janusz A.},
	month = nov,
	year = {2004},
	note = {arXiv: cond-mat/0407098},
	keywords = {Condensed Matter - Disordered Systems and Neural Networks, Condensed Matter - Statistical Mechanics},
	pages = {056110},
}

@article{bollobas_diameter_1981,
	title = {The {Diameter} of {Random} {Graphs}},
	volume = {267},
	issn = {0002-9947},
	url = {https://www.jstor.org/stable/1998567},
	doi = {10.2307/1998567},
	abstract = {Extending some recent theorems of Klee and Larman, we prove rather sharp results about the diameter of a random graph. Among others we show that if \$d = d(n) {\textbackslash}geqslant 3\$ and \$m = m(n)\$ satisfy \$({\textbackslash}log n)/d - 3 {\textbackslash}log {\textbackslash}log n {\textbackslash}rightarrow {\textbackslash}infty, 2{\textasciicircum}\{d-1\}m{\textasciicircum}d/n{\textasciicircum}\{d+1\} - {\textbackslash}log n {\textbackslash}rightarrow {\textbackslash}infty\$ and \$d{\textasciicircum}\{d-2\}m{\textasciicircum}\{d-1\}/n{\textasciicircum}d - {\textbackslash}log n {\textbackslash}rightarrow - {\textbackslash}infty\$ then almost every graph with \$n\$ labelled vertices and \$m\$ edges has diameter \$d\$.},
	number = {1},
	urldate = {2022-04-17},
	journal = {Transactions of the American Mathematical Society},
	author = {BollobÃ¡s, BÃ©la},
	year = {1981},
	note = {Publisher: American Mathematical Society},
	pages = {41--52},
}

@article{bissacot_improvement_2011,
	title = {An {Improvement} of the {LovÃ¡sz} {Local} {Lemma} via {Cluster} {Expansion}},
	doi = {10.1017/S0963548311000253},
	abstract = {An old result by Shearer relates the LovÃ¡sz local lemma with the independent set polynomial on graphs, and consequently, as observed by Scott and Sokal, with the partition function of the hard-core lattice gas on graphs is used. An old result by Shearer relates the LovÃ¡sz local lemma with the independent set polynomial on graphs, and consequently, as observed by Scott and Sokal, with the partition function of the hard-core lattice gas on graphs. We use this connection and a recent result on the analyticity of the logarithm of the partition function of the abstract polymer gas to get an improved version of the LovÃ¡sz local lemma. As an application we obtain tighter bounds on conditions for the existence of Latin transversal matrices.},
	journal = {Combinatorics, Probability and Computing},
	author = {Bissacot, R. and FernÃ¡ndez, R. and Procacci, A. and Scoppola, B.},
	year = {2011},
}

@article{kirousis_interactive_2017,
	title = {An interactive version of the {LovÃ¡sz} local lemma: {Arthur} and {Merlin} implement {Moser}'s algorithm},
	shorttitle = {An interactive version of {Lov}{\textbackslash}'asz local lemma},
	abstract = {Assume we are given (finitely many) mutually independent variables and (finitely many) "undesirable" events each depending on a subset of the variables of at most \$k\$ elements, known as the scope of the event. Assume that the probability of an individual variable belonging to the scope of an occurring event is bounded by \$q\$. We prove that if \$ekq {\textbackslash}leq 1\$ then there exists at least one assignment to the variables for which none of the events occurs. This result is stronger than the classical version of the Lov{\textbackslash}'asz local lemma, which is expressed in terms of a bound \$p\$ of the probabilities of the individual events, and of \$d\$, a bound on the degree of the dependency graph. The proof is through a public coin, interactive implementation of the algorithm by Moser. The original implementation, which yields the classical result, finds efficiently, but probabilistically, an assignment to the events that avoids all undesirable events. Interestingly, the interactive implementation given in this work does not constitute an efficient, even if probabilistic, algorithm to find an assignment as desired under the weaker assumption \$ekq {\textbackslash}leq 1\$. We can only conclude that under this hypothesis, the interactive protocol will produce an assignment as desired within \$n\$ rounds, with probability high with respect to \$n\$; however, the provers' (Merlin's) choices remain non-deterministic. Plausibly finding such an assignment is inherently hard, as the situation is reminiscent, in a probabilistic framework, of problems complete for syntactic subclasses of TFNP.},
	author = {Kirousis, Lefteris and Livieratos, John and Psaromiligkos, Kostas},
	month = aug,
	year = {2017},
}

@article{shearer_problem_1985,
	title = {On a problem of spencer},
	doi = {10.1007/BF02579368},
	abstract = {A sharp bound for Ï± is found in terms of the Ï±i andG for events in a probability space if Ï±1, ..., Ï±nâ¦x andG has maximum degree â¦d then Ï±{\textgreater}0. AbstractLetX1, ...,Xn be events in a probability space. Let Ï±i be the probabilityXi occurs. Let Ï± be the probability that none of theXi occur. LetG be a graph on [n] so that for 1 â¦iâ¦n Xi is independent of âXjâ(i, j)âGâ. Letf(d) be the sup of thosex such that if Ï±1, ..., Ï±nâ¦x andG has maximum degree â¦d then Ï±{\textgreater}0. We showf(1)=1/2,f(d)=(dâ1)dâ1dâd fordâ§2. Hence
\$\${\textbackslash}mathop \{{\textbackslash}lim \}{\textbackslash}limits\_\{d {\textbackslash}to {\textbackslash}infty \} \$\$
df(d)=1/e. This answers a question posed by Spencer in [2]. We also find a sharp bound for Ï± in terms of the Ï±i andG.},
	journal = {Comb.},
	author = {Shearer, J.},
	year = {1985},
}

@inproceedings{achlioptas_simple_2020,
	address = {Virtual Event USA},
	title = {Simple {Local} {Computation} {Algorithms} for the {General} {LovÃ¡sz} {Local} {Lemma}},
	isbn = {978-1-4503-6935-0},
	url = {https://dl.acm.org/doi/10.1145/3350755.3400250},
	doi = {10.1145/3350755.3400250},
	abstract = {We consider the task of designing Local Computation Algorithms (LCA) for applications of the LovÃ¡sz Local Lemma (LLL). LCA is a class of sublinear algorithms proposed by Rubinfeld et al. [38] that have received a lot of attention in recent years. The LLL is an existential, sufficient condition for a collection of sets to have non-empty intersection (in applications, often, each set comprises all objects having a certain property). The ground-breaking algorithm of Moser and Tardos [34] made the LLL fully constructive, following earlier results by Beck [7] and Alon [5] giving algorithms under significantly stronger LLL-like conditions. LCAs under those stronger conditions were given in [38], where it was asked if the Moser-Tardos algorithm can be used to design LCAs under the standard LLL condition. The main contribution of this paper is to answer this question affirmatively. In fact, our techniques yield LCAs for settings beyond the standard LLL condition.},
	language = {en},
	urldate = {2022-04-17},
	booktitle = {Proceedings of the 32nd {ACM} {Symposium} on {Parallelism} in {Algorithms} and {Architectures}},
	publisher = {ACM},
	author = {Achlioptas, Dimitris and Gouleakis, Themis and Iliopoulos, Fotis},
	month = jul,
	year = {2020},
	pages = {1--10},
}

@article{spencer_asymptotic_1977-1,
	title = {Asymptotic lower bounds for {Ramsey} functions},
	volume = {20},
	issn = {0012-365X},
	url = {https://www.sciencedirect.com/science/article/pii/0012365X77900449},
	doi = {10.1016/0012-365X(77)90044-9},
	abstract = {A probability theorem, due to Lovasz, is used to derive lower bounds for various Ramsey functions. A short proof of the known result R(3, t) â©¾ ct2(ln t)2 is given.},
	language = {en},
	urldate = {2022-04-17},
	journal = {Discrete Mathematics},
	author = {Spencer, Joel},
	month = jan,
	year = {1977},
	pages = {69--76},
}

@article{moser_constructive_2009,
	title = {A constructive proof of the general {Lovasz} {Local} {Lemma}},
	url = {http://arxiv.org/abs/0903.0544},
	abstract = {The LovÂ´asz Local Lemma [EL75] is a powerful tool to non-constructively prove the existence of combinatorial objects meeting a prescribed collection of criteria. In his breakthrough paper [Bec91], Beck demonstrated that a constructive variant can be given under certain more restrictive conditions. Simpliï¬cations of his procedure and relaxations of its restrictions were subsequently exhibited in several publications [Alo91, MR98, CS00, Mos06, Sri08, Mos08]. In [Mos09], a constructive proof was presented that works under negligible restrictions, formulated in terms of the Bounded Occurrence Satisï¬ability problem. In the present paper, we reformulate and improve upon these ï¬ndings so as to directly apply to almost all known applications of the general Local Lemma.},
	language = {en},
	urldate = {2022-04-17},
	journal = {arXiv:0903.0544 [cs]},
	author = {Moser, Robin A. and Tardos, GÃ¡bor},
	month = may,
	year = {2009},
	note = {arXiv: 0903.0544},
	keywords = {Computer Science - Computational Complexity, Computer Science - Data Structures and Algorithms, Computer Science - Discrete Mathematics},
}

@article{erdos_problems_1974,
	title = {Problems and results on 3-chromatic {Hypergraphs} and some related questions},
	volume = {10},
	journal = {Coll Math Soc J Bolyai},
	author = {ErdÅs, Paul and LÃ¡szlÃ³, LovÃ¡sz},
	month = jan,
	year = {1974},
}

@misc{chita_goprize_nodate,
	title = {The {GÃ¶del} {Prize} 2020 - {Laudation}},
	url = {https://eatcs.org/index.php/component/content/article/1-news/2850-2020-03-31-12-11-16},
	abstract = {European Association for Theoretical Computer Science},
	language = {en-gb},
	urldate = {2022-04-17},
	journal = {EATCS},
	author = {Chita, Efi},
}

@article{harvey_algorithmic_2015,
	title = {An {Algorithmic} {Proof} of the {Lovasz} {Local} {Lemma} via {Resampling} {Oracles}},
	url = {http://arxiv.org/abs/1504.02044},
	abstract = {The LovaÂ´sz Local Lemma is a seminal result in probabilistic combinatorics. It gives a sufï¬cient condition on a probability space and a collection of events for the existence of an outcome that simultaneously avoids all of those events. Finding such an outcome by an efï¬cient algorithm has been an active research topic for decades. Breakthrough work of Moser and Tardos (2009) presented an efï¬cient algorithm for a general setting primarily characterized by a product structure on the probability space.},
	language = {en},
	urldate = {2022-04-17},
	journal = {arXiv:1504.02044 [cs, math]},
	author = {Harvey, Nicholas and Vondrak, Jan},
	month = nov,
	year = {2015},
	note = {arXiv: 1504.02044},
	keywords = {Computer Science - Data Structures and Algorithms, Mathematics - Combinatorics},
}

@article{moser_constructive_2008,
	title = {A constructive proof of the {Lovasz} {Local} {Lemma}},
	url = {http://arxiv.org/abs/0810.4812},
	abstract = {The LovÂ´asz Local Lemma [EL75] is a powerful tool to prove the existence of combinatorial objects meeting a prescribed collection of criteria. The technique can directly be applied to the satisï¬ability problem, yielding that a k-CNF formula in which each clause has common variables with at most 2kâ2 other clauses is always satisï¬able. All hitherto known proofs of the Local Lemma are non-constructive and do thus not provide a recipe as to how a satisfying assignment to such a formula can be eï¬ciently found. In his breakthrough paper [Bec91], Beck demonstrated that if the neighbourhood of each clause be restricted to O(2k/48), a polynomial time algorithm for the search problem exists. Alon simpliï¬ed and randomized his procedure and improved the bound to O(2k/8) [Alo91]. Srinivasan presented in [Sri08] a variant that achieves a bound of essentially O(2k/4). In [Mos08], we improved this to O(2k/2). In the present paper, we give a randomized algorithm that ï¬nds a satisfying assignment to every k-CNF formula in which each clause has a neighbourhood of at most the asymptotic optimum of 2kâ5 â 1 other clauses and that runs in expected time polynomial in the size of the formula, irrespective of k. If k is considered a constant, we can also give a deterministic variant. In contrast to all previous approaches, our analysis does not anymore invoke the standard non-constructive versions of the Local Lemma and can therefore be considered an alternative, constructive proof of it.},
	language = {en},
	urldate = {2022-04-17},
	journal = {arXiv:0810.4812 [cs]},
	author = {Moser, Robin A.},
	month = oct,
	year = {2008},
	note = {arXiv: 0810.4812},
	keywords = {Computer Science - Data Structures and Algorithms, F.2, G.2},
}

@article{fronczak_average_2004-1,
	title = {Average path length in random networks},
	volume = {70},
	issn = {1539-3755, 1550-2376},
	url = {http://arxiv.org/abs/cond-mat/0212230},
	doi = {10.1103/PhysRevE.70.056110},
	abstract = {Analytic solution for the average path length in a large class of random graphs is found. We apply the approach to classical random graphs of Erd{\textbackslash}"\{o\}s and R{\textbackslash}'\{e\}nyi (ER) and to scale-free networks of Barab{\textbackslash}'\{a\}si and Albert (BA). In both cases our results confirm previous observations: small world behavior in classical random graphs \$l\_\{ER\} {\textbackslash}sim {\textbackslash}ln N\$ and ultra small world effect characterizing scale-free BA networks \$l\_\{BA\} {\textbackslash}sim {\textbackslash}ln N/{\textbackslash}ln{\textbackslash}ln N\$. In the case of scale-free random graphs with power law degree distributions we observed the saturation of the average path length in the limit of \$N{\textbackslash}to{\textbackslash}infty\$ for systems with the scaling exponent \$2{\textless} {\textbackslash}alpha {\textless}3\$ and the small-world behaviour for systems with \${\textbackslash}alpha{\textgreater}3\$.},
	language = {en},
	number = {5},
	urldate = {2022-04-17},
	journal = {Physical Review E},
	author = {Fronczak, Agata and Fronczak, Piotr and Holyst, Janusz A.},
	month = nov,
	year = {2004},
	note = {arXiv: cond-mat/0212230},
	keywords = {Condensed Matter - Disordered Systems and Neural Networks, Condensed Matter - Statistical Mechanics},
	pages = {056110},
}

@article{johansson_nsga-ii_nodate,
	title = {{NSGA}-{II} {DESIGN} {FOR} {FEATURE} {SELECTION} {IN} {EEG} {CLASSIFICATION} {RELATED} {TO} {MOTOR} {IMAGERY}},
	language = {en},
	author = {Johansson, Robin},
	pages = {68},
}

@article{zitzler_evolutionary_nodate,
	title = {An {Evolutionary} {Algorithm} for {Multiobjective} {Optimization}: {The} {Strength} {Pareto} {Approach}},
	language = {en},
	author = {Zitzler, Eckart and Thiele, Lothar},
	pages = {43},
}

@inproceedings{amuso_strength_2004,
	address = {Edinburgh},
	title = {A strength {Pareto} {Evolutionary} {Algorithm} ({SPEA}) for multi-mission radar waveform optimization},
	isbn = {978-1-5090-3177-1},
	url = {http://ieeexplore.ieee.org/document/8317523/},
	doi = {10.1109/IWDDC.2004.8317523},
	abstract = {This paper furthers the development of Evolutionary Computation, specifically Genetic Algorithms (GAâs) applied to the waveform design of simultaneously transmitted orthogonal waveforms. The determination of a suite of âoptimalâ waveforms in the Pareto sense is found a priori for a single platform radar system performing multiple radar missions simultaneously. The waveform suite is determined by utilizing a Strength Pareto Evolutionary Algorithm to find solutions to a Multi-Objective Optimization Problem (MOP). The objectives to be optimized are dictated by the particular missions of interest. The mapping of these objective functions to actual radar performance parameters is used in the SPEA to determine how best to simultaneously perform multiple radar missions such as GMTI, AMTI, and SAR using a single radar system. Most evolutionary algorithms, GAâs being no exception, include constraints. In practical problems such as waveform design, these constraints are based on physical limitations. By developing objective functions that are mapped to mission performance via waveform parameters (the solution space for our application) we have developed a method for searching a vast solution space to determine Pareto optimal waveform suites that can be used to simultaneously perform multiple radar missions. We have proposed using orthogonal waveforms as a constraint due to the natural separation of such signals in frequency space as well as practical state-of-the art hardware implementation feasibility.},
	language = {en},
	urldate = {2022-04-16},
	booktitle = {2004 {International} {Waveform} {Diversity} \& {Design} {Conference}},
	publisher = {IEEE},
	author = {Amuso, Vincent J. and Schneible, Richard S. and Antonik, Paul and Zhang, Yuhong},
	month = nov,
	year = {2004},
	pages = {1--7},
}

@article{deb_fast_2002,
	title = {A fast and elitist multiobjective genetic algorithm: {NSGA}-{II}},
	volume = {6},
	issn = {1089778X},
	shorttitle = {A fast and elitist multiobjective genetic algorithm},
	url = {http://ieeexplore.ieee.org/document/996017/},
	doi = {10.1109/4235.996017},
	language = {en},
	number = {2},
	urldate = {2022-04-16},
	journal = {IEEE Transactions on Evolutionary Computation},
	author = {Deb, K. and Pratap, A. and Agarwal, S. and Meyarivan, T.},
	month = apr,
	year = {2002},
	pages = {182--197},
}

@article{knowles_approximating_2000,
	title = {Approximating the {Nondominated} {Front} {Using} the {Pareto} {Archived} {Evolution} {Strategy}},
	volume = {8},
	issn = {1063-6560, 1530-9304},
	url = {https://direct.mit.edu/evco/article/8/2/149-172/866},
	doi = {10.1162/106365600568167},
	abstract = {We introduce a simple evolution scheme for multiobjective optimization problems, called the Pareto Archived Evolution Strategy (PAES). We argue that PAES may represent the simplest possible nontrivial algorithm capable of generating diverse solutions in the Pareto optimal set. The algorithm, in its simplest form, is a (1 + 1) evolution strategy employing local search but using a reference archive of previously found solutions in order to identify the approximate dominance ranking of the current and candidate solution vectors. (1 + 1)PAES is intended to be a baseline approach against which more involved methods may be compared. It may also serve well in some real-world applications when local search seems superior to or competitive with population-based methods. We introduce (1 + ) and ( + ) variants of PAES as extensions to the basic algorithm. Six variants of PAES are compared to variants of the Niched Pareto Genetic Algorithm and the Nondominated Sorting Genetic Algorithm over a diverse suite of six test functions. Results are analyzed and presented using techniques that reduce the attainment surfaces generated from several optimization runs into a set of univariate distributions. This allows standard statistical analysis to be carried out for comparative purposes. Our results provide strong evidence that PAES performs consistently well on a range of multiobjective optimization tasks.},
	language = {en},
	number = {2},
	urldate = {2022-04-16},
	journal = {Evolutionary Computation},
	author = {Knowles, Joshua D. and Corne, David W.},
	month = jun,
	year = {2000},
	pages = {149--172},
}

@article{thompson_theory_1935,
	title = {On the {Theory} of {Apportionment}},
	volume = {57},
	issn = {00029327},
	url = {https://www.jstor.org/stable/2371219?origin=crossref},
	doi = {10.2307/2371219},
	language = {en},
	number = {2},
	urldate = {2022-04-13},
	journal = {American Journal of Mathematics},
	author = {Thompson, William R.},
	month = apr,
	year = {1935},
	pages = {450},
}

@article{thompson_likelihood_1933,
	title = {On the {Likelihood} that {One} {Unknown} {Probability} {Exceeds} {Another} in {View} of the {Evidence} of {Two} {Samples}},
	volume = {25},
	issn = {00063444},
	url = {https://www.jstor.org/stable/2332286?origin=crossref},
	doi = {10.2307/2332286},
	language = {en},
	number = {3/4},
	urldate = {2022-04-13},
	journal = {Biometrika},
	author = {Thompson, William R.},
	month = dec,
	year = {1933},
	pages = {285},
}

@article{russo_tutorial_nodate,
	title = {A {Tutorial} on {Thompson} {Sampling}},
	abstract = {Thompson sampling is an algorithm for online decision problems where actions are taken sequentially in a manner that must balance between exploiting what is known to maximize immediate performance and investing to accumulate new information that may improve future performance. The algorithm addresses a broad range of problems in a computationally eï¬cient manner and is therefore enjoying wide use. This tutorial covers the algorithm and its application, illustrating concepts through a range of examples, including Bernoulli bandit problems, shortest path problems, product recommendation, assortment, active learning with neural networks, and reinforcement learning in Markov decision processes. Most of these problems involve complex information structures, where information revealed by taking an action informs beliefs about other actions. We will also discuss when and why Thompson sampling is or is not eï¬ective and relations to alternative algorithms.},
	language = {en},
	author = {Russo, Daniel J and Roy, Benjamin Van and Kazerouni, Abbas and Wen, Zheng},
	pages = {96},
}

@article{auer_using_nodate,
	title = {Using {Conï¬dence} {Bounds} for {Exploitation}-{Exploration} {Trade}-oï¬s},
	abstract = {We show how a standard tool from statistics â namely conï¬dence bounds â can be used to elegantly deal with situations which exhibit an exploitation-exploration trade-oï¬. Our technique for designing and analyzing algorithms for such situations is general and can be applied when an algorithm has to make exploitation-versus-exploration decisions based on uncertain information provided by a random process.},
	language = {en},
	author = {Auer, Peter},
	pages = {26},
}

@article{ortega_minimum_2010,
	title = {A {Minimum} {Relative} {Entropy} {Principle} for {Learning} and {Acting}},
	volume = {38},
	issn = {1076-9757},
	url = {https://jair.org/index.php/jair/article/view/10659},
	doi = {10.1613/jair.3062},
	abstract = {This paper proposes a method to construct an adaptive agent that is universal with respect to a given class of experts, where each expert is designed speciï¬cally for a particular environment. This adaptive control problem is formalized as the problem of minimizing the relative entropy of the adaptive agent from the expert that is most suitable for the unknown environment. If the agent is a passive observer, then the optimal solution is the well-known Bayesian predictor. However, if the agent is active, then its past actions need to be treated as causal interventions on the I/O stream rather than normal probability conditions. Here it is shown that the solution to this new variational problem is given by a stochastic controller called the Bayesian control rule, which implements adaptive behavior as a mixture of experts. Furthermore, it is shown that under mild assumptions, the Bayesian control rule converges to the control law of the most suitable expert.},
	language = {en},
	urldate = {2022-04-13},
	journal = {Journal of Artificial Intelligence Research},
	author = {Ortega, P. A. and Braun, D. A.},
	month = aug,
	year = {2010},
	pages = {475--511},
}

@book{lattimore_bandit_2020,
	edition = {1},
	title = {Bandit {Algorithms}},
	isbn = {978-1-108-57140-1 978-1-108-48682-8},
	url = {https://www.cambridge.org/core/product/identifier/9781108571401/type/book},
	language = {en},
	urldate = {2022-04-06},
	publisher = {Cambridge University Press},
	author = {Lattimore, Tor and SzepesvÃ¡ri, Csaba},
	month = jul,
	year = {2020},
	doi = {10.1017/9781108571401},
}

@misc{noauthor_student_nodate,
	title = {Student {Perspectives}: {Multi}-agent sequential decision making â {Compass} {Blog}},
	url = {https://compass.blogs.bristol.ac.uk/2022/03/23/student-perspectives-multi-agent-sequential-decision-making/},
	urldate = {2022-04-06},
}

@article{robbins_aspects_nodate,
	title = {{SOME} {ASPECTS} {OF} {THE} {SEQUENTIAL} {DESIGN} {OF} {EXPERIMENTS}},
	language = {en},
	author = {Robbins, Herbert},
	pages = {9},
}

@article{gittins_bandit_1979,
	title = {Bandit {Processes} and {Dynamic} {Allocation} {Indices}},
	volume = {41},
	issn = {00359246},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.2517-6161.1979.tb01068.x},
	doi = {10.1111/j.2517-6161.1979.tb01068.x},
	abstract = {The paper aims to give a unified account of the central concepts in recent work on bandit processes and dynamic allocation indices; to show how these reduce some previously intractable problems to the problem of calculating such indices; and to describe how these calculations may be carried out. Applications to stochastic scheduling, sequential clinical trials and a class of search problems are discussed.},
	language = {en},
	number = {2},
	urldate = {2022-03-07},
	journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
	author = {Gittins, J. C.},
	month = jan,
	year = {1979},
	pages = {148--164},
}

@article{ryvkin_three_2006,
	title = {Three {Prominent} {Tournament} {Formats}: {Predictive} {Power} and {Costs}},
	issn = {1556-5068},
	shorttitle = {Three {Prominent} {Tournament} {Formats}},
	url = {http://www.ssrn.com/abstract=1132408},
	doi = {10.2139/ssrn.1132408},
	abstract = {We analyze tournaments of heterogeneous players from an organizerâs perspective. Using a simple model of a noisy tournament, we demonstrate how the likelihood of selecting the best player, here termed the âpredictive powerâ of a tournament, depends on the tournament format, the distribution of playersâ types, and the overall noise level. We formalize the organizerâs decision problem for varying time and measurement costs and compare the predictive power of three widely used tournament formats â contests, binary elimination tournaments, and round-robin tournaments. We show which formats are preferred in the various scenarios and ï¬nd that for certain parameter constellations, certain formats are not viable.},
	language = {en},
	urldate = {2022-03-07},
	journal = {SSRN Electronic Journal},
	author = {Ryvkin, Dmitry and Ortmann, Andreas},
	year = {2006},
}

@misc{aerion_english_2013,
	title = {English:  {Example} of a 10-entry double elimination bracket},
	copyright = {Public domain},
	shorttitle = {English},
	url = {https://commons.wikimedia.org/wiki/File:NSB-doubleelim-draw-2004.svg},
	urldate = {2022-03-07},
	author = {Aerion},
	month = jul,
	year = {2013},
}

@misc{creditor_full_nodate,
	title = {Full {Bracket}, {Last}-16 {Matchups} for {Euro} 2020},
	url = {https://www.si.com/soccer/2021/06/23/euro-2020-knockout-bracket-matchups-last-16-times-dates},
	abstract = {Find out which teams are paired together in the knockout stage and how the road to the title at Euro 2020 will unfold.},
	language = {en-us},
	urldate = {2022-03-07},
	journal = {Sports Illustrated},
	author = {Creditor, Avi},
}

@misc{georg_skorobohatyj_mp-testdata_1995,
	title = {{MP}-{TESTDATA} - {The} {TSPLIB} {Symmetric} {Traveling} {Salesman} {Problem} {Instances}},
	url = {http://elib.zib.de/pub/mp-testdata/tsp/tsplib/tsp/},
	urldate = {2022-03-07},
	journal = {MP-TESTDATA - The TSPLIB Symmetric Traveling Salesman Problem Instances},
	author = {{Georg Skorobohatyj}},
	month = jun,
	year = {1995},
}

@article{hussain_genetic_2017,
	title = {Genetic {Algorithm} for {Traveling} {Salesman} {Problem} with {Modified} {Cycle} {Crossover} {Operator}},
	volume = {2017},
	issn = {1687-5265, 1687-5273},
	url = {https://www.hindawi.com/journals/cin/2017/7430125/},
	doi = {10.1155/2017/7430125},
	abstract = {Genetic algorithms are evolutionary techniques used for optimization purposes according to survival of the fittest idea. These methods do not ensure optimal solutions; however, they give good approximation usually in time. The genetic algorithms are useful for NP-hard problems, especially the traveling salesman problem. The genetic algorithm depends on selection criteria, crossover, and mutation operators. To tackle the traveling salesman problem using genetic algorithms, there are various representations such as binary, path, adjacency, ordinal, and matrix representations. In this article, we propose a new crossover operator for traveling salesman problem to minimize the total distance. This approach has been linked with path representation, which is the most natural way to represent a legal tour. Computational results are also reported with some traditional path representation methods like partially mapped and order crossovers along with new cycle crossover operator for some benchmark TSPLIB instances and found improvements.},
	language = {en},
	urldate = {2022-03-06},
	journal = {Computational Intelligence and Neuroscience},
	author = {Hussain, Abid and Muhammad, Yousaf Shad and Nauman Sajid, M. and Hussain, Ijaz and Mohamd Shoukry, Alaa and Gani, Showkat},
	year = {2017},
	pages = {1--7},
}

@article{brady_optimization_1985,
	title = {Optimization strategies gleaned from biological evolution},
	volume = {317},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/317804a0},
	doi = {10.1038/317804a0},
	language = {en},
	number = {6040},
	urldate = {2022-03-06},
	journal = {Nature},
	author = {Brady, R. M.},
	month = oct,
	year = {1985},
	pages = {804--806},
}

@article{fogel_evolutionary_1988,
	title = {An evolutionary approach to the traveling salesman problem},
	volume = {60},
	issn = {0340-1200, 1432-0770},
	url = {http://link.springer.com/10.1007/BF00202901},
	doi = {10.1007/BF00202901},
	abstract = {Evolutionary optimization has been proposed as a method to generate machine learning through automated discovery. A simulation of natural evolution is conducted using the traveling salesman problem as an artificial environment. For an exact solution of a traveling salesman problem, the only known algorithms require the number of steps to grow at least exponentially with the number of elements in the problem. Three adaptive techniques are described and analyzed. Evolutionary adaptation is demonstrated to be worthwhile in a variety of contexts. Local stagnation is prevented by allowing for the probabilistic survival of the simulated organisms. In complex problems, the final routing is estimated to be better than 99.99999999999\% of all possible tours, even though only a small fraction (8.58 x 10-151) of the total number of tours are examined.},
	language = {en},
	number = {2},
	urldate = {2022-03-06},
	journal = {Biological Cybernetics},
	author = {Fogel, D. B.},
	month = dec,
	year = {1988},
	pages = {139--144},
}

@article{whitley_scheduling_1989,
	title = {Scheduling {Problems} and {Traveling} {Salesmen}: {The} {Genetic} {Edge} {Recombination} {Operator}},
	abstract = {This paper outlines a new approach to generating solutions to Traveling Salesman Problems. A new operator is introduced which recombines the edges (or links) between cities from two parents to create a single new oï¬spring. This operator is diï¬erent from past operators that emphasized edges in that 95compose the oï¬spring are inherited from one of the two parents. The recombination operator does not use information about the distance between cities or any heuristic operators. This operator has found known solutions and matched âbest knownâ solutions for every problem on which it has been tested. The functionality of this new operator can be explained using a variation on Hollandâs original schema theorem. Because this operator uses no information about the actual cost associated with edges and requires the use of no additional heuristics, it can also be used on sequencing problems where there are no actual distances to measure, but rather only some overall evaluation of the total sequence. An example is given of how this operator has been used to generate job shop schedules.},
	language = {en},
	journal = {Proceedings on the Third International Conference on Genetic Algorithms},
	author = {Whitley, Darrell and Starkweather, Tim and Fuquay, DâAnn},
	year = {1989},
	pages = {133--140},
}

@incollection{whitley_traveling_1991,
	title = {The {Traveling} {Salesman} and {Sequence} {Scheduling}: {Quality} {Solutions} {Using} {Genetic} {Edge} {Recombination}},
	language = {en},
	booktitle = {Handbook of {Genetic} {Algorithms}},
	publisher = {Van Nostrand Reinhold, New York},
	author = {Whitley, Darrell and Starkweather, Timothy and Shaner, Daniel},
	year = {1991},
	pages = {350--372},
}

@article{metropolis_equation_1953,
	title = {Equation of {State} {Calculations} by {Fast} {Computing} {Machines}},
	volume = {21},
	issn = {0021-9606, 1089-7690},
	url = {http://aip.scitation.org/doi/10.1063/1.1699114},
	doi = {10.1063/1.1699114},
	language = {en},
	number = {6},
	urldate = {2022-03-06},
	journal = {The Journal of Chemical Physics},
	author = {Metropolis, Nicholas and Rosenbluth, Arianna W. and Rosenbluth, Marshall N. and Teller, Augusta H. and Teller, Edward},
	month = jun,
	year = {1953},
	pages = {1087--1092},
}

@article{basu_tabu_2012,
	title = {Tabu {Search} {Implementation} on {Traveling} {Salesman} {Problem} and {Its} {Variations}: {A} {Literature} {Survey}},
	volume = {02},
	issn = {2160-8830, 2160-8849},
	shorttitle = {Tabu {Search} {Implementation} on {Traveling} {Salesman} {Problem} and {Its} {Variations}},
	url = {http://www.scirp.org/journal/doi.aspx?DOI=10.4236/ajor.2012.22019},
	doi = {10.4236/ajor.2012.22019},
	abstract = {The Traveling Salesman Problem (TSP) and its allied problems like Vehicle Routing Problem (VRP) are one of the most widely studied problems in combinatorial optimization. It has long been known to be NP-hard and hence research on developing algorithms for the TSP has focused on approximate methods in addition to exact methods. Tabu search is one of the most widely applied metaheuristic for solving the TSP. In this paper, we review the tabu search literature on the TSP and its variations, point out trends in it, and bring out some interesting research gaps in this literature.},
	language = {en},
	number = {02},
	urldate = {2022-03-03},
	journal = {American Journal of Operations Research},
	author = {Basu, Sumanta},
	year = {2012},
	pages = {163--173},
}

@misc{noauthor_student_nodate-1,
	title = {Student perspectives: {The} {Elo} {Rating} {System} â {From} {Chess} to {Education} â {Compass} {Blog}},
	url = {https://compass.blogs.bristol.ac.uk/2020/12/17/the-elo-rating-system-from-chess-to-education/},
	urldate = {2022-03-02},
}

@article{malek_serial_1989,
	title = {Serial and parallel simulated annealing and tabu search algorithms for the traveling salesman problem},
	doi = {10.1007/BF02022093},
	abstract = {Results indicate that tabu search consistently outperforms simulated annealing with respect to computation time while giving comparable solutions to traveling salesman problem problems. This paper describes serial and parallel implementations of two different search techniques applied to the traveling salesman problem. A novel approach has been taken to parallelize simulated annealing and the results are compared with the traditional annealing algorithm. This approach uses abbreviated cooling schedule and achieves a superlinear speedup. Also a new search technique, called tabu search, has been adapted to execute in a parallel computing environment. Comparison between simulated annealing and tabu search indicate that tabu search consistently outperforms simulated annealing with respect to computation time while giving comparable solutions. Examples include 25, 33, 42, 50, 57, 75 and 100 city problems.},
	author = {Malek, M. and Guruswamy, M. and Pandya, Mihir and Owens, Howard},
	year = {1989},
}

@article{osman_meta-strategy_1993,
	title = {Meta-strategy simulated annealing and {Tabu} search algorithms for the vehicle routine problem},
	volume = {41},
	doi = {10.1007/BF02023004},
	abstract = {The vehicle routing problem (VRP) under capacity and distance restrictions involves the design of a set of minimum cost delivery routes, originating and terminating at a central depot, which services a set of customers. Each customer must be supplied exactly once by one vehicle route. The total demand of any vehicle must not exceed the vehicle capacity. The total length of any route must not exceed a pre-specified bound. Approximate methods based on descent, hybrid simulated annealing/tabu search, and tabu search algorithms are developed and different search strategies are investigated. A special data structure for the tabu search algorithm is implemented which has reduced notably the computational time by more than 50\%. An estimate for the tabu list size is statistically derived. Computational results are reported on a sample of seventeen bench-mark test problems from the literature and nine randomly generated problems. The new methods improve significantly both the number of vehicles used and the total distances travelled on all results reported in the literature.},
	journal = {Annals of Operations Research},
	author = {Osman, Ibrahim},
	month = dec,
	year = {1993},
	pages = {421--451},
}

@article{larranaga_genetic_1999,
	title = {Genetic {Algorithms} for the {Travelling} {Salesman} {Problem}: {A} {Review} of {Representations} and {Operators}},
	volume = {13},
	shorttitle = {Genetic {Algorithms} for the {Travelling} {Salesman} {Problem}},
	doi = {10.1023/A:1006529012972},
	abstract = {This paper is the result of a literature study carried out by the authors. It is a review of the different attempts made to solve the Travelling Salesman Problem with Genetic Algorithms. We present crossover and mutation operators, developed to tackle the Travelling Salesman Problem with Genetic Algorithms with different representations such as: binary representation, path representation, adjacency representation, ordinal representation and matrix representation. Likewise, we show the experimental results obtained with different standard examples using combination of crossover and mutation operators in relation with path representation.},
	journal = {Artificial Intelligence Review},
	author = {Larranaga, Pedro and Kuijpers, Cindy and Murga, R. and Inza, I. and Dizdarevic, S.},
	month = jan,
	year = {1999},
	pages = {129--170},
}

@article{alobaidi_rule_2017,
	title = {A {Rule} {Based} {Evolutionary} {Optimization} {Approach} for the {Traveling} {Salesman} {Problem}},
	volume = {9},
	copyright = {http://creativecommons.org/licenses/by/4.0/},
	url = {http://www.scirp.org/Journal/Paperabs.aspx?paperid=77661},
	doi = {10.4236/iim.2017.94006},
	abstract = {The traveling salesman problem has long been regarded as a challenging application for existing optimization methods as well as a benchmark application for the development of new optimization methods. As with many existing algorithms, a traditional genetic algorithm will have limited success with this problem class, particularly as the problem size increases. A rule based genetic algorithm is proposed and demonstrated on sets of traveling salesman problems of increasing size. The solution character as well as the solution efficiency is compared against a simulated annealing technique as well as a standard genetic algorithm. The rule based genetic algorithm is shown to provide superior performance for all problem sizes considered. Furthermore, a post optimal analysis provides insight into which rules were successfully applied during the solution process which allows for rule modification to further enhance performance.},
	language = {en},
	number = {4},
	urldate = {2022-03-02},
	journal = {Intelligent Information Management},
	author = {Alobaidi, Wissam M. and Webb, David J. and Sandgren, Eric},
	month = jul,
	year = {2017},
	note = {Number: 4
Publisher: Scientific Research Publishing},
	pages = {115--132},
}

@article{kirkpatrick_optimization_1983,
	title = {Optimization by {Simulated} {Annealing}},
	volume = {220},
	url = {https://www.science.org/doi/10.1126/science.220.4598.671},
	doi = {10.1126/science.220.4598.671},
	number = {4598},
	urldate = {2022-03-02},
	journal = {Science},
	author = {Kirkpatrick, S. and Gelatt, C. D. and Vecchi, M. P.},
	month = may,
	year = {1983},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {671--680},
}

@article{ben-ameur_computing_2004,
	title = {Computing the {Initial} {Temperature} of {Simulated} {Annealing}},
	volume = {29},
	doi = {10.1023/B:COAP.0000044187.23143.bd},
	abstract = {The classical version of simulated annealing is based on a cooling schedule. Generally, the initial temperature is set such that the acceptance ratio of bad moves is equal to a certain value 0. In this paper, we first propose a simple algorithm to compute a temperature which is compatible with a given acceptance ratio. Then, we study the properties of the acceptance probability. It is shown that this function is convex for low temperatures and concave for high temperatures. We also provide a lower bound for the number of plateaux of a simulated annealing based on a geometric cooling schedule. Finally, many numerical experiments are reported.},
	journal = {Computational Optimization and Applications},
	author = {Ben-Ameur, Walid},
	month = dec,
	year = {2004},
	pages = {369--385},
}

@misc{noauthor_simulated_nodate,
	title = {Simulated {Annealing}: {The} {Travelling} {Salesman} {Problem}},
	url = {https://www.fourmilab.ch/documents/travelling/anneal/},
	urldate = {2022-03-02},
}

@article{zhan_list-based_2016,
	title = {List-{Based} {Simulated} {Annealing} {Algorithm} for {Traveling} {Salesman} {Problem}},
	volume = {2016},
	issn = {1687-5265},
	url = {https://www.hindawi.com/journals/cin/2016/1712630/},
	doi = {10.1155/2016/1712630},
	abstract = {Simulated annealing (SA) algorithm is a popular intelligent optimization algorithm which has been successfully applied in many fields. Parametersâ setting is a key factor for its performance, but it is also a tedious work. To simplify parameters setting, we present a list-based simulated annealing (LBSA) algorithm to solve traveling salesman problem (TSP). LBSA algorithm uses a novel list-based cooling schedule to control the decrease of temperature. Specifically, a list of temperatures is created first, and then the maximum temperature in list is used by Metropolis acceptance criterion to decide whether to accept a candidate solution. The temperature list is adapted iteratively according to the topology of the solution space of the problem. The effectiveness and the parameter sensitivity of the list-based cooling schedule are illustrated through benchmark TSP problems. The LBSA algorithm, whose performance is robust on a wide range of parameter values, shows competitive performance compared with some other state-of-the-art algorithms.},
	language = {en},
	urldate = {2022-03-02},
	journal = {Computational Intelligence and Neuroscience},
	author = {Zhan, Shi-hua and Lin, Juan and Zhang, Ze-jun and Zhong, Yi-wen},
	month = mar,
	year = {2016},
	note = {Publisher: Hindawi},
	pages = {e1712630},
}

@article{biro_designing_nodate,
	title = {Designing chess pairing mechanisms},
	abstract = {The Swiss system is the most popular chess tournament system that is recognised and regulated by the World Chess Federation (FIDE). Chess pairings in each round of a Swiss tournament are conducted by sophisticated matching algorithms. The matching mechanisms are precisely deï¬ned in the FIDE guidebook [3], currently four diï¬erent variants are allowed. The descriptions of the matching procedures are such that every arbiter should be able to conduct the pairings, even without computer assistance. However, many parts of these procedures are very ineï¬cient, as they may terminate in highly exponential time in the number of players due to their exhaustive search nature. We demonstrate how the main priority rules of the Dutch variant can be replaced by eï¬cient matching algorithms. These eï¬cient algorithms can serve as the base of software tools used for pairings.},
	language = {en},
	author = {Biro, Peter and Fleiner, Tamas and Palincza, Richard},
	pages = {10},
}

@misc{noauthor_c_nodate,
	title = {C. {General} {Rules} and {Technical} {Recommendations} for {Tournaments} / 04. {FIDE} {Swiss} {Rules} / {C}.04.3 {FIDE} ({Dutch}) {System} / {FIDE} {Handbook}},
	url = {https://handbook.fide.com/chapter/C0403},
	abstract = {C. General Rules and Technical Recommendations for Tournaments / 04. FIDE Swiss Rules / C.04.3 FIDE (Dutch) System /},
	language = {en},
	urldate = {2021-12-16},
	journal = {International Chess Federation (FIDE)},
}

@article{wooley_new_1995,
	title = {New {Estimates} for {Smooth} {Weyl} {Sums}},
	volume = {51},
	url = {https://www.semanticscholar.org/paper/New-Estimates-for-Smooth-Weyl-Sums-Wooley/336fddeb695c6c48e9794b340294ea1ff3e7d716},
	doi = {10.1112/JLMS/51.1.1},
	abstract = {Since the early part of this century, estimates for Weyl sums (or generalisations thereof) have been central to the treatment of many problems in the additive theory of numbers. For over forty years, the strongest such estimates have stemmed from a method due to Vinogradov [8], the argument having been somewhat simplified recently by the use of the large sieve (see [4, Lemma 5.4]). During this period, improvements in estimates for generalisations of Weyl sums have arisen from improved bounds on mean values of such sums, very recently with the arrival of Vaughan's new iterative method (see [5, Theorems 1.5 and 1.8]). In contrast, this paper will be devoted to improvements at the core of this circle of ideas, within Vinogradov's method itself. Our ideas, which here we shall investigate in the context of smooth Weyl sums, would seem to be applicable elsewhere, and this is a matter which we intend to pursue in the future. We now describe our conclusions in some detail. Let k be a natural number, and P be a large real number. When 2 {\textless} R {\textless} P, we define the set of /{\textasciicircum}-smooth numbers, s\#(P, R), by},
	number = {1},
	journal = {Journal of The London Mathematical Society},
	author = {Wooley, T.},
	year = {1995},
	pages = {1--13},
}

@misc{noauthor_lower_2012,
	title = {A lower bound for {Waring}'s {Problem} for sufficiently large numbers},
	shorttitle = {A lower bound for {Waring}'s {Problem} for sufficiently large numbers},
	url = {https://math.stackexchange.com/questions/219954/a-lower-bound-for-warings-problem-for-sufficiently-large-numbers-gk-ge-k1},
	urldate = {2021-12-15},
	journal = {Mathematics Stack Exchange},
	month = oct,
	year = {2012},
}

@article{niven_unsolved_1944,
	title = {An {Unsolved} {Case} of the {Waring} {Problem}},
	volume = {66},
	issn = {0002-9327},
	url = {https://www.jstor.org/stable/2371901},
	doi = {10.2307/2371901},
	number = {1},
	urldate = {2021-12-14},
	journal = {American Journal of Mathematics},
	author = {Niven, Ivan},
	year = {1944},
	note = {Publisher: Johns Hopkins University Press},
	pages = {137--143},
}

@article{davenport_warings_1939,
	title = {On {Waring}'s {Problem} for {Fourth} {Powers}},
	volume = {40},
	issn = {0003-486X},
	url = {https://www.jstor.org/stable/1968889},
	doi = {10.2307/1968889},
	number = {4},
	urldate = {2021-12-14},
	journal = {Annals of Mathematics},
	author = {Davenport, H.},
	year = {1939},
	pages = {731--747},
}

@article{sidokhine_algorithmic_2013,
	title = {An algorithmic proof of {Bachet}'s conjecture and the {Lagrange}-{Euler} method},
	url = {http://arxiv.org/abs/1310.5632},
	abstract = {The goal of this notice is to present a proof of Bachet's conjecture based exclusively on the fundamental theorem of arithmetic. The novelty of this proof consists in its introduction of a partial order on rational integers through the unique factorization property. In general, the proofs of Bachet's conjecture by Lagrange - Euler's method assume necessary the use of infinite descent. In the proposed proof we do not assume the existence of a "minimal solution", but rather we show the existence of the desired solution through an algorithmic method.},
	urldate = {2021-12-14},
	journal = {arXiv:1310.5632 [math]},
	author = {Sidokhine, Felix},
	month = oct,
	year = {2013},
	keywords = {Mathematics - Number Theory},
}

@article{srinivasa_ramanujan_expression_1917,
	title = {On the expression of a number in the form ax2 + by2 + cz2 + dw2},
	volume = {19},
	issn = {0008-1981},
	url = {https://www.biodiversitylibrary.org/item/95836},
	journal = {Proceedings - Cambridge Philosophical Society.},
	author = {{Srinivasa Ramanujan}},
	year = {1917},
	pages = {11--21},
}

@article{mahler_fractional_1957,
	title = {On the fractional parts of the powers of a rational number ({II})},
	volume = {4},
	issn = {2041-7942},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1112/S0025579300001170},
	doi = {10.1112/S0025579300001170},
	language = {en},
	number = {2},
	urldate = {2021-12-14},
	journal = {Mathematika},
	author = {Mahler, K.},
	year = {1957},
	pages = {122--124},
}

@book{diophantus_diophanti_1670,
	address = {Tolosae},
	title = {Diophanti {Alexandrini} {Arithmeticorum} libri sex et de numeris multangulis liber unus: cum commentariis {C}. {G}. {Bacheti} ... \& observationibus .. de {Fermat} ...; accessit doctrinae analyticae inventum novum collectum ex varijs eiusdem ... de {Fermat} epistolis},
	shorttitle = {Diophanti {Alexandrini} {Arithmeticorum} libri sex et de numeris multangulis liber unus},
	url = {https://www.e-rara.ch/zut/doi/10.3931/e-rara-9423},
	language = {lat},
	publisher = {excudebat Bernardi Bosc},
	author = {{Diophantus} and Fermat, Pierre de and Bachet, Claude Gaspar and {CollÃ¨ge de Plessis-Sorbonne} and {SacrÃ©-Coeur Conflan} and {Courcier}},
	year = {1670},
	doi = {10.3931/e-rara-9423},
	note = {Book Title: Diophanti Alexandrini Arithmeticorum libri sex et de numeris multangulis liber unus cum commentariis C. G. Bacheti ... \& observationibus .. de Fermat ...; accessit doctrinae analyticae inventum novum collectum ex varijs eiusdem ... de Fermat epistolis},
	keywords = {DIOPHANTISCHE GLEICHUNGEN (ZAHLENTHEORIE), DRITTES JAHRHUNDERT N. CHR, EBENE GEOMETRIE, ELEMENTARE ALGEBRA, ELEMENTARE ZAHLENTHEORIE},
}

@book{diophantus_diophanti_1670-1,
	title = {Diophanti {Alexandrini} {Arithmeticorum} libri sex et de numeris multangulis liber unus : cum commentariis {C}. {G}. {Bacheti} ... \& observationibus .. de {Fermat} ...; accessit doctrinae analyticae inventum novum collectum ex varijs eiusdem ... de {Fermat} epistolis},
	shorttitle = {Diophanti {Alexandrini} {Arithmeticorum} libri sex et de numeris multangulis liber unus},
	url = {http://www.e-rara.ch/zut/2790606},
	abstract = {ETH-Bibliothek (NEBIS). Diophanti Alexandrini Arithmeticorum libri sex et de numeris multangulis liber unus : cum commentariis C. G. Bacheti ... \& observationibus .. de Fermat ...; accessit doctrinae analyticae inventum novum collectum ex varijs eiusdem ... de [...]. Tolosae : excudebat Bernardi Bosc ..., 1670},
	language = {en},
	urldate = {2021-12-14},
	publisher = {TEST},
	author = {{Diophantus} and {Claude Gaspar Bachet} and {Pierre de Fermat}},
	year = {1670},
}

@article{hilbert_beweis_1909,
	title = {Beweis fÃ¼r die {Darstellbarkeit} der ganzen {Zahlen} durch eine feste {Anzahln} ter {Potenzen} ({Waringsches} {Problem}): {Dem} {Andenken}},
	volume = {67},
	shorttitle = {Beweis fÃ¼r die {Darstellbarkeit} der ganzen {Zahlen} durch eine feste {Anzahln} ter {Potenzen} ({Waringsches} {Problem})},
	url = {https://zenodo.org/record/1428266},
	doi = {10.1007/bf01450405},
	abstract = {n/a},
	urldate = {2021-12-14},
	journal = {Mathematische Annalen},
	author = {Hilbert, David},
	month = sep,
	year = {1909},
	pages = {281--300},
}

@book{waring_meditationes_1770,
	address = {Cantabrigiae},
	title = {Meditationes algebraicae},
	language = {Latin},
	publisher = {Typis Academicis excudebat J. Archdeacon},
	author = {Waring, Edward},
	year = {1770},
	note = {OCLC: 187480384},
}

@article{stokesbary_proof_2007,
	title = {A {Proof} of {Lagrange}'s {Four} {Square} {Theorem} {Using} {Quaternion} {Algebras}},
	issn = {1556-5068},
	url = {https://www.ssrn.com/abstract=2396123},
	doi = {10.2139/ssrn.2396123},
	abstract = {Many prime numbers can be expressed as a sum of the squares of two other numbers. This paper explores which numbers can be written as a sum of the squares of four numbers. This question is deeply related to a number system known as quaternion algebra, which will be developed in this paper to describe what numbers can be written as the sum of four squares.},
	language = {en},
	urldate = {2021-12-14},
	journal = {SSRN Electronic Journal},
	author = {Stokesbary, Andrew},
	year = {2007},
	doi = {10.2139/ssrn.2396123},
}

@article{nathanson_short_1987,
	title = {A {Short} {Proof} of {Cauchy}'s {Polygonal} {Number} {Theorem}},
	volume = {99},
	issn = {0002-9939},
	url = {https://www.jstor.org/stable/2046263},
	doi = {10.2307/2046263},
	abstract = {This paper presents a simple proof that every nonnegative integer is the sum of m + 2 polygonal numbers of order m + 2.},
	number = {1},
	urldate = {2021-12-14},
	journal = {Proceedings of the American Mathematical Society},
	author = {Nathanson, Melvyn B.},
	year = {1987},
	note = {Publisher: American Mathematical Society},
	pages = {22--24},
}

@book{cambridge_philosophical_society_proceedings_1917,
	address = {Cambridge [etc.]},
	title = {Proceedings - {Cambridge} {Philosophical} {Society}.},
	volume = {19},
	url = {https://www.biodiversitylibrary.org/item/95836},
	publisher = {Cambridge Philosophical Society [etc.]},
	author = {Cambridge Philosophical Society. and Society, Cambridge Philosophical and Bohr, Niels Henrik David},
	year = {1917},
	note = {ISSN: 0008-1981
Pages: 1-926},
}

@article{hua_results_1938,
	title = {{SOME} {RESULTS} {IN} {THE} {ADDITIVE} {PRIME}-{NUMBER} {THEORY}},
	volume = {os-9},
	issn = {0033-5606},
	url = {https://doi.org/10.1093/qmath/os-9.1.68},
	doi = {10.1093/qmath/os-9.1.68},
	number = {1},
	urldate = {2021-12-14},
	journal = {The Quarterly Journal of Mathematics},
	author = {HUA, LOO-KENG},
	month = jan,
	year = {1938},
	pages = {68--80},
}

@article{hardy_problems_1922,
	title = {Some problems of â{Partitio} numerorumâ ({VI}): {Further} researches in {Waring}'s {Problem}},
	volume = {12},
	issn = {1432-1823},
	shorttitle = {Some problems of â{Partitio} numerorumâ ({VI})},
	url = {https://eudml.org/doc/167675},
	doi = {10.1007/BF01506218},
	language = {en},
	urldate = {2021-12-14},
	journal = {Mathematische Zeitschrift},
	author = {Hardy, G. H. and Littlewood, J. E.},
	month = jan,
	year = {1922},
	pages = {161--188},
}

@article{vaughan_warings_2002,
	title = {Waring's {Problem}: {A} {Survey}},
	shorttitle = {Waring's {Problem}},
	abstract = {this paper (section 7.2) there is also a brief discussion about the representation of a natural number as the sum of a fixed number of squares of integers, and there seems little doubt that it is the methods described therein which inspired the later work of Hardy and Littlewood},
	author = {Vaughan, R. and Wooley, Trevor},
	month = may,
	year = {2002},
}

@incollection{euler_leonhardi_1862,
	title = {Leonhardi {Euleri} {Opera} postuma mathematica et physica},
	url = {http://archive.org/details/leonhardieuleri00petrgoog},
	abstract = {Book digitized by Google from the library of Harvard University and uploaded to the Internet Archive by user tpb.; 2 volumes : 25 cm; Microopaque. New York : Readex Microprint, 16 cards ; 23 x 15 cm; s 1969 n},
	language = {lat},
	urldate = {2021-12-14},
	booktitle = {Leonhardi {Euleri} {Opera} postuma mathematica et physica : anno {MDCCCXLIV} detecta},
	publisher = {Petropoli : Eggers},
	author = {Euler, Leonhard},
	collaborator = {{Harvard University}},
	year = {1862},
	keywords = {Mathematics},
	pages = {203--204},
}

@book{bennett_number_2002,
	address = {Natick, Mass},
	title = {Number theory for the millennium},
	isbn = {978-1-56881-126-0 978-1-56881-146-8 978-1-56881-152-9},
	publisher = {A.K. Peters},
	editor = {Bennett, M. A.},
	year = {2002},
	note = {Meeting Name: Millennial Conference on Number Theory},
	keywords = {Congresses, Number theory},
}

@article{w_j_ellison_warings_1971,
	title = {Waring's {Problem}},
	volume = {78},
	url = {https://www.maa.org/programs/maa-awards/writing-awards/warings-problem},
	urldate = {2021-12-14},
	journal = {The American Mathematical Monthly},
	author = {{W. J. Ellison}},
	year = {1971},
}

@article{kumar_ensemble_2017,
	title = {An {Ensemble} of {Fine}-{Tuned} {Convolutional} {Neural} {Networks} for {Medical} {Image} {Classification}},
	volume = {21},
	issn = {2168-2208},
	doi = {10.1109/JBHI.2016.2635663},
	abstract = {The availability of medical imaging data from clinical archives, research literature, and clinical manuals, coupled with recent advances in computer vision offer the opportunity for image-based diagnosis, teaching, and biomedical research. However, the content and semantics of an image can vary depending on its modality and as such the identification of image modality is an important preliminary step. The key challenge for automatically classifying the modality of a medical image is due to the visual characteristics of different modalities: some are visually distinct while others may have only subtle differences. This challenge is compounded by variations in the appearance of images based on the diseases depicted and a lack of sufficient training data for some modalities. In this paper, we introduce a new method for classifying medical images that uses an ensemble of different convolutional neural network (CNN) architectures. CNNs are a state-of-the-art image classification technique that learns the optimal image features for a given classification task. We hypothesise that different CNN architectures learn different levels of semantic image representation and thus an ensemble of CNNs will enable higher quality features to be extracted. Our method develops a new feature extractor by fine-tuning CNNs that have been initialized on a large dataset of natural images. The fine-tuning process leverages the generic image features from natural images that are fundamental for all images and optimizes them for the variety of medical imaging modalities. These features are used to train numerous multiclass classifiers whose posterior probabilities are fused to predict the modalities of unseen images. Our experiments on the ImageCLEF 2016 medical image public dataset (30 modalities; 6776 training images, and 4166 test images) show that our ensemble of fine-tuned CNNs achieves a higher accuracy than established CNNs. Our ensemble also achieves a higher accuracy than methods in the literature evaluated on the same benchmark dataset and is only overtaken by those methods that source additional training data.},
	number = {1},
	journal = {IEEE Journal of Biomedical and Health Informatics},
	author = {Kumar, Ashnil and Kim, Jinman and Lyndon, David and Fulham, Michael and Feng, Dagan},
	month = jan,
	year = {2017},
	note = {Conference Name: IEEE Journal of Biomedical and Health Informatics},
	keywords = {Biomedical imaging, Computer architecture, Convolutional neural network (CNN), Feature extraction, Informatics, Neural networks, Training, Training data, deep learning, ensembles, fine-tuning, image classification},
	pages = {31--40},
}

@misc{tsang_review_2019,
	title = {Review: {SegNet} ({Semantic} {Segmentation})},
	shorttitle = {Review},
	url = {https://towardsdatascience.com/review-segnet-semantic-segmentation-e66f2e30fb96},
	abstract = {Encoder Decoder Architecture, Using Max Pooling Indices to Upsample, Outperforms FCN, DeepLabv1, DeconvNet},
	language = {en},
	urldate = {2021-11-27},
	journal = {Medium},
	author = {Tsang, Sik-Ho},
	month = apr,
	year = {2019},
}

@article{piramanayagam_supervised_2018,
	title = {Supervised {Classification} of {Multisensor} {Remotely} {Sensed} {Images} {Using} a {Deep} {Learning} {Framework}},
	volume = {10},
	doi = {10.3390/rs10091429},
	abstract = {In this paper, we present a convolutional neural network (CNN)-based method to efficiently combine information from multisensor remotely sensed images for pixel-wise semantic classification. The CNN features obtained from multiple spectral bands are fused at the initial layers of deep neural networks as opposed to final layers. The early fusion architecture has fewer parameters and thereby reduces the computational time and GPU memory during training and inference. We also propose a composite fusion architecture that fuses features throughout the network. The methods were validated on four different datasets: ISPRS Potsdam, Vaihingen, IEEE Zeebruges and Sentinel-1, Sentinel-2 dataset. For the Sentinel-1,-2 datasets, we obtain the ground truth labels for three classes from OpenStreetMap. Results on all the images show early fusion, specifically after layer three of the network, achieves results similar to or better than a decision level fusion mechanism. The performance of the proposed architecture is also on par with the state-of-the-art results.},
	journal = {Remote Sensing},
	author = {Piramanayagam, Sankaranarayanan and Saber, Eli and Schwartzkopf, Wade and Koehler, Frederick},
	month = sep,
	year = {2018},
	pages = {1429},
}

@misc{noauthor_medical_2020,
	title = {Medical {Image} {Segmentation} {Using} a {U}-{Net} type of {Architecture}},
	url = {https://deepai.org/publication/medical-image-segmentation-using-a-u-net-type-of-architecture},
	abstract = {05/11/20 - Deep convolutional neural networks have been proven to be very effective in
image related analysis and tasks, such as image segmen...},
	urldate = {2021-11-27},
	journal = {DeepAI},
	month = may,
	year = {2020},
}

@article{liu_survey_2020,
	title = {A survey on {U}-shaped networks in medical image segmentations},
	volume = {409},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231220309218},
	doi = {10.1016/j.neucom.2020.05.070},
	abstract = {The U-shaped network is one of the end-to-end convolutional neural networks (CNNs). In electron microscope segmentation of ISBI challenge 2012, the concise architecture and outstanding performance of the U-shaped network are impressive. Then, a variety of segmentation models based on this architecture have been proposed for medical image segmentations. We present a comprehensive literature review of U-shaped networks applied to medical image segmentation tasks, focusing on the architectures, extended mechanisms and application areas in these studies. The aim of this survey is twofold. First, we report the different extended U-shaped networks, discuss main state-of-the-art extended mechanisms, including residual mechanism, dense mechanism, dilated mechanism, attention mechanism, multi-module mechanism, and ensemble mechanism, analyze their pros and cons. Second, this survey provides the overview of studies in main application areas of U-shaped networks, including brain tumor, stroke, white matter hyperintensities (WMHs), eye, cardiac, liver, musculoskeletal, skin cancer, and neuronal pathology. Finally, we summarize the current U-shaped networks, point out the open challenges and directions for future research.},
	language = {en},
	urldate = {2021-11-27},
	journal = {Neurocomputing},
	author = {Liu, Liangliang and Cheng, Jianhong and Quan, Quan and Wu, Fang-Xiang and Wang, Yu-Ping and Wang, Jianxin},
	month = oct,
	year = {2020},
	keywords = {Convolutional neural networks, Extended mechanism, Medical image segmentation, U-shaped network},
	pages = {244--258},
}

@article{jacob_reinhold_how_2020,
	title = {How to {Choose} a {Neural} {Net} {Architecture} for {Medical} {Image} {Segmentation}},
	url = {https://innolitics.com/articles/medical-image-segmentation-overview/#:~:text=The%20U-Net%20is%20a%20simple-to-implement%20DNN%20architecture%20that,paper%20at%20the%20prestigious%20medical%20imaging%20conference%20MICCAI.},
	journal = {Innolitics.com},
	author = {{Jacob Reinhold} and {Yujan Shrestha}},
	month = jul,
	year = {2020},
}

@article{long_fully_2015,
	title = {Fully {Convolutional} {Networks} for {Semantic} {Segmentation}},
	url = {http://arxiv.org/abs/1411.4038},
	abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20\% relative improvement to 62.2\% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.},
	urldate = {2021-11-27},
	journal = {arXiv:1411.4038 [cs]},
	author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
	month = mar,
	year = {2015},
	note = {arXiv: 1411.4038},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{badrinarayanan_segnet_2016,
	title = {{SegNet}: {A} {Deep} {Convolutional} {Encoder}-{Decoder} {Architecture} for {Image} {Segmentation}},
	shorttitle = {{SegNet}},
	url = {http://arxiv.org/abs/1511.00561},
	abstract = {We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN and also with the well known DeepLab-LargeFOV, DeconvNet architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. We show that SegNet provides good performance with competitive inference time and more efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.},
	urldate = {2021-11-27},
	journal = {arXiv:1511.00561 [cs]},
	author = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
	month = oct,
	year = {2016},
	note = {arXiv: 1511.00561},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{ronneberger_u-net_2015,
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	shorttitle = {U-{Net}},
	url = {http://arxiv.org/abs/1505.04597},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
	urldate = {2021-11-27},
	journal = {arXiv:1505.04597 [cs]},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	month = may,
	year = {2015},
	note = {arXiv: 1505.04597},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{joseph_malkevitch_who_2010,
	title = {Who {Won}!},
	shorttitle = {American {Mathematical} {Society}},
	url = {http://www.ams.org/publicoutreach/feature-column/fcarc-scores},
	abstract = {Advancing research. Creating connections.},
	language = {en},
	urldate = {2021-11-11},
	journal = {American Mathematical Society},
	author = {{Joseph Malkevitch}},
	year = {2010},
}

@book{sutton_reinforcement_2014,
	edition = {2},
	title = {Reinforcement {Learning}: {An} {Introduction}},
	language = {en},
	publisher = {MIT Press},
	author = {Sutton, Richard S and Barto, Andrew G},
	year = {2014},
}

@article{aldous_elo_2017,
	title = {Elo {Ratings} and the {Sports} {Model}: {A} {Neglected} {Topic} in {Applied} {Probability}?},
	volume = {32},
	issn = {0883-4237},
	shorttitle = {Elo {Ratings} and the {Sports} {Model}},
	url = {https://projecteuclid.org/journals/statistical-science/volume-32/issue-4/Elo-Ratings-and-the-Sports-Model--A-Neglected-Topic/10.1214/17-STS628.full},
	doi = {10.1214/17-STS628},
	abstract = {In a simple model for sports, the probability A beats B is a speciï¬ed function of their difference in strength. One might think this would be a staple topic in Applied Probability textbooks (like the GaltonâWatson branching process model, for instance) but it is curiously absent. Our ï¬rst purpose is to point out that the model suggests a wide range of questions, suitable for âundergraduate researchâ via simulation but also challenging as professional research. Our second, more speciï¬c, purpose concerns Elo-type rating algorithms for tracking changing strengths. There has been little foundational research on their accuracy, despite a much-copied â30 matches sufï¬ceâ claim, which our simulation study casts doubt upon.},
	language = {en},
	number = {4},
	urldate = {2021-11-11},
	journal = {Statistical Science},
	author = {Aldous, David},
	month = nov,
	year = {2017},
}

@article{litvakov_tournament_1986,
	title = {Tournament methods in choice theory},
	volume = {39},
	issn = {00200255},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0020025586900526},
	doi = {10.1016/0020-0255(86)90052-6},
	language = {en},
	number = {1},
	urldate = {2021-11-11},
	journal = {Information Sciences},
	author = {Litvakov, Boris M. and Vol'skiy, Vladimir I.},
	month = aug,
	year = {1986},
	pages = {7--40},
}

@book{richard_s_sutton_reinforcement_2014,
	edition = {2},
	title = {Reinforcement {Learning}: {An} {Introduction}},
	abstract = {Andrew G. Barto},
	publisher = {MIT Press},
	author = {Richard S. Sutton},
	year = {2014},
}

@misc{arunachalam_y_tournament_2002,
	title = {Tournament {Scheduling}},
	url = {https://nrich.maths.org/1443},
	abstract = {Scheduling games is a little more challenging than one might desire. Here are some tournament formats that sport schedulers use.},
	urldate = {2021-11-11},
	author = {{Arunachalam Y}},
	year = {2002},
}

@phdthesis{christopher_todd_edwards_combinatorial_1991,
	title = {The combinatorial theory of single-elimination tournaments},
	url = {https://scholarworks.montana.edu/xmlui/bitstream/handle/1/6870/31762100987518.pdf;sequence=1},
	school = {Montana State University},
	author = {{Christopher Todd Edwards}},
	year = {1991},
}

@book{amy_n_langville_whos_2012,
	title = {Who's \#1? {The} {Science} of {Rating} and {Ranking}},
	isbn = {978-0-691-15422-0},
	shorttitle = {Who's \#1?},
	publisher = {Princeton University Press},
	author = {{Amy N. Langville} and {Carl D. Meyer}},
	year = {2012},
}

@article{lewis_exploring_nodate,
	title = {Exploring {Tournament} {Graphs} and {Their} {Win} {Sequences}},
	language = {en},
	author = {Lewis, Sadiki O},
	pages = {56},
}

@misc{noauthor_mathematical_nodate,
	title = {mathematical modeling - {How} does a tournament's structure affect the likelihood that the best player will win?},
	url = {https://mathoverflow.net/questions/42461/how-does-a-tournaments-structure-affect-the-likelihood-that-the-best-player-wil},
	urldate = {2021-11-11},
	journal = {MathOverflow},
}

@article{jech_ranking_1983,
	title = {The {Ranking} of {Incomplete} {Tournaments}: {A} {Mathematician}'s {Guide} to {Popular} {Sports}},
	volume = {90},
	issn = {0002-9890},
	shorttitle = {The {Ranking} of {Incomplete} {Tournaments}},
	url = {https://www.jstor.org/stable/2975756},
	doi = {10.2307/2975756},
	number = {4},
	urldate = {2021-11-11},
	journal = {The American Mathematical Monthly},
	author = {Jech, Thomas},
	year = {1983},
	note = {Publisher: Mathematical Association of America},
	pages = {246--266},
}

@misc{noauthor_python_nodate,
	title = {python - {How} to specify legend position in matplotlib in graph coordinates},
	url = {https://stackoverflow.com/questions/44413020/how-to-specify-legend-position-in-matplotlib-in-graph-coordinates},
	urldate = {2021-05-04},
	journal = {Stack Overflow},
}

@misc{mark_petersen_mathematical_2001,
	title = {Mathematical {Harmonies}},
	url = {https://amath.colorado.edu/pub/matlab/music/MathMusic.pdf},
	urldate = {2021-05-04},
	author = {Mark Petersen},
	month = jul,
	year = {2001},
}

@misc{noauthor_history_nodate,
	title = {The history of the {CD} - {Technology} - {Research}},
	url = {https://www.philips.com/a-w/research/technologies/cd/technology.html},
	abstract = {The technology of the Compact Disc is very complex, but in general it can be divided into 'digital data processing', 'optics' and âmechanics. These disciplines allowed a system to be developed more than twenty years ago that was superior to any other concept, and that since then has grown into an enormous family which could never have been imagined at the time of the introduction.},
	language = {en},
	urldate = {2021-05-03},
	journal = {Philips},
}

@misc{recording_academy_technical_2010,
	title = {Technical {GRAMMY} {Award}},
	url = {https://www.grammy.com/grammys/awards/technical-awards},
	abstract = {This Special Merit Award is presented by vote of the Recording Academy's National Trustees to individuals and/or companies who have made contributions of outstanding technical significance to the recording field.  Previous TechnicalÂ GRAMMY Recipients},
	language = {en},
	urldate = {2021-04-29},
	journal = {GRAMMY.com},
	author = {{Recording Academy}},
	month = oct,
	year = {2010},
}

@article{simon_reynolds_how_2018,
	title = {How {Auto}-{Tune} {Revolutionized} the {Sound} of {Popular} {Music}},
	url = {https://pitchfork.com/features/article/how-auto-tune-revolutionized-the-sound-of-popular-music/},
	abstract = {An in-depth history of the most important pop innovation of the last 20 years, from Cherâs âBelieveâ to Kanye West to Migos},
	language = {en},
	urldate = {2021-04-29},
	journal = {Pitchfork},
	author = {{Simon Reynolds}},
	month = sep,
	year = {2018},
}

@article{blackman_measurement_1958,
	title = {The measurement of power spectra from the point of view of communications engineering â {Part} {I}},
	volume = {37},
	issn = {0005-8580},
	doi = {10.1002/j.1538-7305.1958.tb03874.x},
	abstract = {The measurement of power spectra is a problem of steadily increasing importance which appears to some to be primarily a problem in statistical estimation. Others may see it as a problem of instrumentation, recording and analysis which vitally involves the ideas of transmission theory. Actually, ideas and techniques from both fields are needed. When they are combined, they provide a basis for developing the insight necessary (i) to plan both the acquisition of adequate data and sound procedures for its reduction to meaningful estimates and (ii) to interpret these estimates correctly and usefully. This account attempts to provide and relate the necessary ideas and techniques in reasonable detail. Part II of this article wilt appear in the March issue of THE JOURNAL.},
	number = {1},
	journal = {The Bell System Technical Journal},
	author = {Blackman, R. B. and Tukey, J. W.},
	month = jan,
	year = {1958},
	note = {Conference Name: The Bell System Technical Journal},
	pages = {200--201},
}

@article{kursell_experiments_2013,
	title = {Experiments on {Tone} {Color} in {Music} and {Acoustics}: {Helmholtz}, {Schoenberg}, and {Klangfarbenmelodie}},
	volume = {28},
	issn = {0369-7827},
	shorttitle = {Experiments on {Tone} {Color} in {Music} and {Acoustics}},
	url = {https://www.jstor.org/stable/10.1086/671377},
	doi = {10.1086/671377},
	abstract = {AbstractIn the mid-nineteenth century, Hermann von Helmholtz developed a new, mathematically formalized representation of the quality of tones, which he termed musikalische Klangfarbe. He did so at the price of excluding change from this representation and from the sounds he experimented with. Later researchers and composers discovered the cognitive and aesthetic side effects of this new concept. Experimental psychologist Carl Stumpf found that stable tones veil their source; their recognition strongly depends on their characteristic beginnings and endings. Arnold Schoenberg in turn used this effect to merge the sounds of musical instruments into new orchestral colors. On the basis of a three-part case study, I argue that nineteenth-century research in perception has deeply affected twentieth-century concepts of music, bringing to the fore the aesthetic quality of experimental situations.},
	number = {1},
	urldate = {2021-04-29},
	journal = {Osiris},
	author = {Kursell, Julia},
	year = {2013},
	note = {Publisher: [Saint Catherines Press, The University of Chicago Press, The History of Science Society]},
	pages = {191--211},
}

@article{chialvo_how_2003,
	title = {How we hear what is not there: {A} neural mechanism for the missing fundamental illusion},
	volume = {13},
	issn = {1054-1500, 1089-7682},
	shorttitle = {How we hear what is not there},
	url = {http://aip.scitation.org/doi/10.1063/1.1617771},
	doi = {10.1063/1.1617771},
	language = {en},
	number = {4},
	urldate = {2021-04-29},
	journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
	author = {Chialvo, Dante R.},
	month = dec,
	year = {2003},
	pages = {1226--1230},
}

@article{good_interaction_1958,
	title = {The {Interaction} {Algorithm} and {Practical} {Fourier} {Analysis}},
	volume = {20},
	issn = {0035-9246},
	url = {https://www.jstor.org/stable/2983896},
	abstract = {The interactions in a complete factorial experiment are linear functions of the observations. They can be calculated by means of a stream-lined algorithm, of which the first example was the adding-and-subtracting algorithm of Yates (1937). The relationship between the two methods of calculating the interactions is given a precise and succinct form, which surprisingly does not seem to have been given in the existing literature. The algorithm is logically simplest for the t$^{\textrm{n}}$ experiment. The inverse algorithm is specified and also the method of obtaining the divisors for the analysis of variance. Finally some analogous short cuts in practical Fourier analysis are described.},
	number = {2},
	urldate = {2021-04-29},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Good, I. J.},
	year = {1958},
	note = {Publisher: [Royal Statistical Society, Wiley]},
	pages = {361--372},
}

@misc{kevin_russell_acoustic_nodate,
	title = {Acoustic {Phonetics}: {Formants}},
	url = {https://home.cc.umanitoba.ca/~krussll/phonetics/acoustic/formants.html},
	urldate = {2021-04-28},
	author = {{Kevin Russell}},
}

@book{lindblom_explaining_1990,
	title = {Explaining {Phonetic} {Variation}: {A} {Sketch} of the {H}\&{H} {Theory}},
	volume = {55},
	isbn = {978-94-010-7414-8},
	shorttitle = {Explaining {Phonetic} {Variation}},
	abstract = {The H\&H theory is developed from evidence showing that speaking and listening are shaped by biologically general processes. Speech production is adaptive. Speakers can, and typically do, tune their performance according to communicative and situational demands, controlling the interplay between production-oriented factors on the one hand, and output-oriented constraints on the other. For the ideal speaker, H\&H claims that such adaptations reflect his tacit awareness of the listenerâs access to sources of information independent of the signal and his judgement of the short-term demands for explicit signal information. Hence speakers are expected to vary their output along a continuum of hyper- and hypospeech. The theory suggests that the lack of invariance that speech signals commonly exhibit (Perkell and Klatt 1986) is a direct consequence of this adaptive organization (cf MacNeilage 1970). Accordingly, in the H\&H program the quest for phonetic invariance is replaced by another research task: Explicating the notion of sufficient discriminability and defining the class of speech signals that meet that criterion.},
	author = {Lindblom, BjÃ¶rn and Hardcastle, W. and Marchal, Alain},
	month = jan,
	year = {1990},
	doi = {10.1007/978-94-009-2037-8_16},
	note = {Journal Abbreviation: Speech Production and Speech Modelling
Publication Title: Speech Production and Speech Modelling},
}

@article{scherer_self-reported_2015,
	title = {Self-{Reported} {Symptoms} of {Depression} and {PTSD} {Are} {Associated} with {Reduced} {Vowel} {Space} in {Screening} {Interviews}},
	volume = {7},
	doi = {10.1109/TAFFC.2015.2440264},
	abstract = {Reduced frequency range in vowel production is a well documented speech characteristic of individuals with psychological and neurological disorders. Affective disorders such as depression and post-traumatic stress disorder (PTSD) are known to influence motor control and in particular speech production. The assessment and documentation of reduced vowel space and reduced expressivity often either rely on subjective assessments or on analysis of speech under constrained laboratory conditions (e.g. sustained vowel production, reading tasks). These constraints render the analysis of such measures expensive and impractical. Within this work, we investigate an automatic unsupervised machine learning based approach to assess a speakerâs vowel space. Our experiments are based on recordings of 253 individuals. Symptoms of depression and PTSD are assessed using standard self-assessment questionnaires and their cut-off scores. The experiments show a significantly reduced vowel space in subjects that scored positively on the questionnaires. We show the measureâs statistical robustness against varying demographics of individuals and articulation rate. The reduced vowel space for subjects with symptoms of depression can be explained by the common condition of psychomotor retardation influencing articulation and motor control. These findings could potentially support treatment of affective disorders, like depression and PTSD in the future.},
	journal = {IEEE Transactions on Affective Computing},
	author = {Scherer, Stefan and Lucas, Gale and Gratch, Jonathan and Rizzo, Albert and Morency, Louis-Philippe},
	month = jan,
	year = {2015},
	pages = {1--1},
}

@article{mousa_voice_2011,
	title = {Voice {Conversion} using {Pitch} {Shifting} {Algorithm} by {Time} {Stretchingwith} {PSOLA} and {Re}-{Sampling}},
	volume = {61},
	doi = {10.2478/v10187-010-0008-5},
	abstract = {Voice Conversion Using Pitch Shifting Algorithm by Time Stretching with PSOLA and Re-Sampling
Voice changing has many applications in the industry and commercial filed. This paper emphasizes voice conversion using a pitch shifting method which depends on detecting the pitch of the signal (fundamental frequency) using Simplified Inverse Filter Tracking (SIFT) and changing it according to the target pitch period using time stretching with Pitch Synchronous Over Lap Add Algorithm (PSOLA), then resampling the signal in order to have the same play rate. The same study was performed to see the effect of voice conversion when some Arabic speech signal is considered. Treatment of certain Arabic voiced vowels and the conversion between male and female speech has shown some expansion or compression in the resulting speech. Comparison in terms of pitch shifting is presented here. Analysis was performed for a single frame and a full segmentation of speech.},
	journal = {Journal of Electrical Engineering},
	author = {Mousa, Allam},
	month = jun,
	year = {2011},
	pages = {2011},
}

@inproceedings{roucos_high_1985,
	title = {High quality time-scale modification for speech},
	volume = {10},
	doi = {10.1109/ICASSP.1985.1168381},
	abstract = {We present a new and simple method for speech rate modification that yields high quality rate-modified speech. Earlier algorithms either required a significant amount of computation for good quality output speech or resulted in poor quality rate-modified speech. The algorithm we describe allows arbitrary linear or nonlinear scaling of the time axis. The algorithm operates in the time domain using a modified overlap-and-add (OLA) procedure on the waveform. It requires moderate computation and could be easily implemented in real time on currently available hardware. The algorithm works equally well on single voice speech, multiple-voice speech, and speech in noise. In this paper, we discuss an earlier algorithm for time-scale modification (TSM), and present both objective and informal subjective results for the new and previous TSM methods.},
	booktitle = {{ICASSP} '85. {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	author = {Roucos, S. and Wilgus, A.},
	month = apr,
	year = {1985},
	keywords = {Convergence, Fasteners, Fourier transforms, Iterative algorithms, Speech analysis, Speech coding, Speech enhancement, Speech processing, Speech synthesis, Time measurement},
	pages = {493--496},
}

@article{allen_unified_1977,
	title = {A unified approach to short-time {Fourier} analysis and synthesis},
	volume = {65},
	issn = {0018-9219},
	url = {http://ieeexplore.ieee.org/document/1455039/},
	doi = {10.1109/PROC.1977.10770},
	language = {en},
	number = {11},
	urldate = {2021-04-28},
	journal = {Proceedings of the IEEE},
	author = {Allen, J.B. and Rabiner, L.R.},
	year = {1977},
	pages = {1558--1564},
}

@inproceedings{charpentier_diphone_1986,
	title = {Diphone synthesis using an overlap-add technique for speech waveforms concatenation},
	volume = {11},
	doi = {10.1109/ICASSP.1986.1168657},
	abstract = {A new method is presented for text-to-speech synthesis using diphones. The diphone database consists of the diphone waveforms labeled with pitch-marks indicating the pitch-periods. At synthesis time, the diphone waveforms are processed through a new analysis-synthesis system, providing an independent control of all prosodic parameters, while retaining a good degree of naturalness. This system is based on a representation of the speech signal by its short-time Fourier transform (STFT) at a pitch-synchronous sampling rate. The synthesis part of the system works by overlap-adding the modified short-term signals and it ensures a smooth concatenation of the diphone waveforms. The synthetic speech obtained by this method sounds more natural than with the conventional LPC method.},
	booktitle = {{ICASSP} '86. {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}},
	author = {Charpentier, F. and Stella, M.},
	month = apr,
	year = {1986},
	keywords = {Control system synthesis, Databases, Filter bank, Linear predictive coding, Signal analysis, Signal synthesis, Speech analysis, Speech coding, Speech synthesis, Telecommunication control},
	pages = {2015--2018},
}

@article{drugman_traditional_2018,
	title = {Traditional {Machine} {Learning} for {Pitch} {Detection}},
	volume = {25},
	issn = {1070-9908, 1558-2361},
	url = {http://arxiv.org/abs/1903.01290},
	doi = {10.1109/LSP.2018.2874155},
	abstract = {Pitch detection is a fundamental problem in speech processing as F0 is used in a large number of applications. Recent articles have proposed deep learning for robust pitch tracking. In this paper, we consider voicing detection as a classification problem and F0 contour estimation as a regression problem. For both tasks, acoustic features from multiple domains and traditional machine learning methods are used. The discrimination power of existing and proposed features is assessed through mutual information. Multiple supervised and unsupervised approaches are compared. A significant relative reduction of voicing errors over the best baseline is obtained: 20\% with the best clustering method (K-means) and 45\% with a Multi-Layer Perceptron. For F0 contour estimation, the benefits of regression techniques are limited though. We investigate whether those objective gains translate in a parametric synthesis task. Clear perceptual preferences are observed for the proposed approach over two widely-used baselines (RAPT and DIO).},
	number = {11},
	urldate = {2021-04-28},
	journal = {IEEE Signal Processing Letters},
	author = {Drugman, Thomas and Huybrechts, Goeric and Klimkov, Viacheslav and Moinet, Alexis},
	month = nov,
	year = {2018},
	note = {arXiv: 1903.01290},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	pages = {1745--1749},
}

@article{gfeller_spice_2020,
	title = {{SPICE}: {Self}-supervised {Pitch} {Estimation}},
	volume = {28},
	issn = {2329-9290, 2329-9304},
	shorttitle = {{SPICE}},
	url = {http://arxiv.org/abs/1910.11664},
	doi = {10.1109/TASLP.2020.2982285},
	abstract = {We propose a model to estimate the fundamental frequency in monophonic audio, often referred to as pitch estimation. We acknowledge the fact that obtaining ground truth annotations at the required temporal and frequency resolution is a particularly daunting task. Therefore, we propose to adopt a self-supervised learning technique, which is able to estimate pitch without any form of supervision. The key observation is that pitch shift maps to a simple translation when the audio signal is analysed through the lens of the constant-Q transform (CQT). We design a self-supervised task by feeding two shifted slices of the CQT to the same convolutional encoder, and require that the difference in the outputs is proportional to the corresponding difference in pitch. In addition, we introduce a small model head on top of the encoder, which is able to determine the confidence of the pitch estimate, so as to distinguish between voiced and unvoiced audio. Our results show that the proposed method is able to estimate pitch at a level of accuracy comparable to fully supervised models, both on clean and noisy audio samples, although it does not require access to large labeled datasets.},
	urldate = {2021-04-28},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Gfeller, Beat and Frank, Christian and Roblek, Dominik and Sharifi, Matt and Tagliasacchi, Marco and VelimiroviÄ, Mihajlo},
	year = {2020},
	note = {arXiv: 1910.11664},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	pages = {1118--1128},
}

@article{elowsson_polyphonic_2019,
	title = {Polyphonic {Pitch} {Tracking} with {Deep} {Layered} {Learning}},
	url = {http://arxiv.org/abs/1804.02918},
	abstract = {This paper presents a polyphonic pitch tracking system able to extract both framewise and note-based estimates from audio. The system uses several artificial neural networks in a deep layered learning setup. First, cascading networks are applied to a spectrogram for framewise fundamental frequency (f0) estimation. A sparse receptive field is learned by the first network and then used as a filter kernel for parameter sharing throughout the system. The f0 activations are connected across time to extract pitch contours. These contours define a framework within which subsequent networks perform onset and offset detection, operating across both time and smaller pitch fluctuations at the same time. As input, the networks use, e.g., variations of latent representations from the f0 estimation network. Finally, incorrect tentative notes are removed one by one in an iterative procedure that allows a network to classify notes within an accurate context. The system was evaluated on four public test sets: MAPS, Bach10, TRIOS, and the MIREX Woodwind quintet, and performed state-of-the-art results for all four datasets. It performs well across all subtasks: f0, pitched onset, and pitched offset tracking.},
	urldate = {2021-04-28},
	journal = {arXiv:1804.02918 [cs, eess]},
	author = {Elowsson, Anders},
	month = mar,
	year = {2019},
	note = {arXiv: 1804.02918},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@article{kaur_analysis_2020,
	title = {Analysis of {Performance} of {Pitch} {Estimation} {Techniques}},
	volume = {7},
	abstract = {Fundamental frequency or pitch estimation is important problem in speech recognition to extract the features of speech signal. It gives information about the speakers emotion and the gender information. Pitch vary within a speaker and can sometimes drop or rise significantly. The Presented work focuses especially on estimation of pitch period, pitch frequency with autocorrelation and cepstum pitch estimation techniques in clean and noisy environment. For Pitch Estimation input signal is recorded from people having different age and sex. Pitch contour by autocorrelation and cepstrum method is evaluated. The random values in the pitch contours corresponds to the unvoiced and silence regions and the continuous segments correspond to the voiced regions. Fine Pitch Error(FPE) is computed for speakers of different age, sex to evaluate performance. The focus here is to study and implement various pitch estimation techniques on both clean and noisy speech and then evaluating the performance of these techniques.},
	language = {en},
	number = {17},
	journal = {JOURNAL OF CRITICAL REVIEWS},
	author = {Kaur, Manpreet and Kaur, Gagandeep and Sood, Priyanka},
	year = {2020},
	pages = {13},
}

@article{chander_short_2017,
	title = {Short {Time} {Cepstrum} {Analysis} {Method} for {Pitch} {Estimation} of an {Arbitrary} {Speech} {Signal} {Using} {MATLAB}},
	volume = {5},
	abstract = {In this paper we present the short time cepstrum analysis method for pitch estimation of an arbitrary speech signal. We use MATLAB simulating software for our analysis purpose. The speech signal whose pitch is to be estimated is imported into the MATLAB SPTool box in .wav format. The sampled speech signal is further sliced to reduce its duration for pitch estimation. Pitch estimation of speech signal is useful for speech modeling, speech enhancement, speech synthesis, speech coding and various other application of speech signal processing.},
	language = {en},
	number = {3},
	author = {Chander, Harish and Singh, Balwinder and Khanna, Ravinder},
	year = {2017},
	pages = {4},
}

@article{kumar_efficient_2015,
	title = {Efficient {Time} {Domain} {Fundamental} {Frequency} {Estimation} ({Pitch} {Estimation})},
	volume = {6},
	abstract = {Fundamental frequency (F0) or pich estimatin is important problem in speech recognition to extract the features of speech signal. It gives information about the speakers emotion and the gender specific analysis. Here an algorithm presented to estimate pitch of a speech signal using improved Auto-Correlation Function(ACF) method. The frequency present in the speech signal is harmonically related to the fundamental frequency and is the integr multiples of F0. Here energy present in the signal is calculated to find the positions where the pitch value is high or less. The fundamental frequency of speech can vary from 40 Hz for low-pitched male voices to 600 Hz for children or high-pitched female voices. This algorithm gives very accurate analysis for the estimation of pitch variations according to voiced or unvoiced signal.},
	language = {en},
	number = {3},
	author = {Kumar, Sunil and Jangra, Manisha},
	year = {2015},
	pages = {4},
}

@article{dubnowski_real-time_1976,
	title = {Real-time digital hardware pitch detector},
	volume = {24},
	issn = {0096-3518},
	url = {http://ieeexplore.ieee.org/document/1162765/},
	doi = {10.1109/TASSP.1976.1162765},
	abstract = {A high-quality pitch detector has been built in digital hardware and operates in real time at a 10 kl-Iz sampling rate. The hardware is capable of providing energy as well as pitch-period esthnates. The pitch and energy computations are performed 100 times/s (i.e., once per 10 ms interval). The algorithm to estimate the pitch period uses center clipping, infinite peak clipping, and a simplified autocorrelation analysis. The analysis is performed on a 300 sample section of speech which is both center clipped and infinite peak clipped, yielding a threelevel speech signal where the levels are â1, 0, and +1 depending on the relation of the original speech sample to the clipping threshold. Thus computation of the autocorrelation function of the clipped speech is easily implemented in digital hardware using simple combinatorial logic, i.e., an up-down counter can be used to compute each correlation point. The pitch detector has been interfaced to the NOVA computer facility of the Acoustics Research Department at Bell Laboratories.},
	language = {en},
	number = {1},
	urldate = {2021-04-28},
	journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	author = {Dubnowski, J. and Schafer, R. and Rabiner, L.},
	month = feb,
	year = {1976},
	pages = {2--8},
}

@article{shimamura_weighted_2001,
	title = {Weighted autocorrelation for pitch extraction of noisy speech},
	volume = {9},
	issn = {10636676},
	url = {http://ieeexplore.ieee.org/document/952490/},
	doi = {10.1109/89.952490},
	abstract = {In this paper, we propose a modified version of the autocorrelation pitch extraction method well known to be robust against noise. Utilizing that the average magnitude difference function (AMDF) has similar characteristics with the autocorrelation function, the autocorrelation function is weighted by the reciprocal of the AMDF. By simulation experiments, it is shown that the proposed pitch extraction method is useful in noisy environments.},
	language = {en},
	number = {7},
	urldate = {2021-04-28},
	journal = {IEEE Transactions on Speech and Audio Processing},
	author = {Shimamura, T. and Kobayashi, H.},
	month = oct,
	year = {2001},
	pages = {727--730},
}

@misc{cockos_incoporated_reaper_nodate,
	title = {Reaper},
	url = {https://www.reaper.fm/},
	publisher = {Cockos Incroporated},
	author = {{Cockos Incoporated}},
}

@misc{tamara_smyth_harmonic_2019,
	title = {Harmonic {Product} {Spectrum} ({HPS})},
	url = {http://musicweb.ucsd.edu/~trsmyth/analysis/Harmonic_Product_Spectrum.html},
	urldate = {2021-04-25},
	author = {{Tamara Smyth}},
	month = dec,
	year = {2019},
}

@misc{bastian_bechtold_soundfile_nodate,
	title = {{SoundFile} â {PySoundFile} 0.10.3},
	shorttitle = {{SoundFile}},
	url = {https://pysoundfile.readthedocs.io/en/latest/#},
	urldate = {2021-04-25},
	author = {{Bastian Bechtold} and {Matthias Geier}},
}

@misc{oli_larkin_iplug2_nodate,
	title = {{iPlug2} {C}++ {Audio} {Plug}-in {Framework}},
	url = {https://iplug2.github.io/},
	abstract = {iPlug2 website},
	urldate = {2021-04-25},
	author = {{Oli Larkin}},
}

@misc{cockos_incoporated_wdl_nodate,
	title = {{WDL}},
	shorttitle = {{WDL}},
	url = {https://www.cockos.com/wdl/},
	urldate = {2021-04-25},
	author = {{Cockos Incoporated}},
}

@misc{the_midi_manufacturers_association_midi_1996,
	title = {{MIDI} 1.0 {Detailed} {Specification}},
	url = {https://www.midi.org/specifications/midi1-specifications/m1-v4-2-1-midi-1-0-detailed-specification-96-1-4},
	urldate = {2021-04-27},
	publisher = {The MIDI Manufacturers Association},
	author = {{The MIDI Manufacturers Association}},
	year = {1996},
}

@misc{robert_bristow-johnson_music-dsp_2004,
	title = {[music-dsp] {Re}: {Pitch} detection algorithm},
	shorttitle = {[music-dsp] {Re}},
	url = {https://music.columbia.edu/pipermail/music-dsp/2004-April/059917.html},
	urldate = {2021-04-25},
	author = {Robert Bristow-Johnson},
	month = apr,
	year = {2004},
}

@misc{robert_bristow-johnson_rbj_2005,
	title = {{RBJ} {Audio}-{EQ}-{Cookbook}},
	shorttitle = {{RBJ} {Audio} {EQ} {Cookbook}},
	url = {https://www.musicdsp.org/en/latest/Filters/197-rbj-audio-eq-cookbook.html},
	urldate = {2021-04-25},
	author = {Robert Bristow-Johnson},
	month = may,
	year = {2005},
}

@article{mcleod_smarter_2005,
	title = {A {Smarter} {Way} {To} {Find} {Pitch}},
	abstract = {The âTartiniâ project at the University of Otago aims to use the computer as a practical tool for singers and instrumentalists. Sound played into the system is analysed fast enough to create useful feedback for teaching or, at a higher level, for practising musicians to reï¬ne their technique. Central to this analysis is the accurate determination of musical pitch.},
	language = {en},
	author = {McLeod, Philip and Wyvill, Geoff},
	month = jan,
	year = {2005},
	pages = {4},
}

@article{de_cheveigne_yin_2002,
	title = {{YIN}, a fundamental frequency estimator for speech and musica)},
	volume = {111},
	language = {en},
	number = {4},
	journal = {J. Acoust. Soc. Am.},
	author = {de Cheveigne, Alain and Kawahara, Hideki},
	year = {2002},
	pages = {14},
}

@misc{noauthor_numpy_nodate,
	title = {{NumPy}},
	url = {https://numpy.org/},
	urldate = {2021-04-25},
}

@inproceedings{gentleman_fast_1966,
	address = {New York, NY, USA},
	series = {{AFIPS} '66 ({Fall})},
	title = {Fast {Fourier} {Transforms}: for fun and profit},
	isbn = {978-1-4503-7893-2},
	shorttitle = {Fast {Fourier} {Transforms}},
	url = {https://doi.org/10.1145/1464291.1464352},
	doi = {10.1145/1464291.1464352},
	abstract = {The "Fast Fourier Transform" has now been widely known for about a year. During that time it has had a major effect on several areas of computing, the most striking example being techniques of numerical convolution, which have been completely revolutionized. What exactly is the "Fast Fourier Transform"?},
	urldate = {2021-04-25},
	booktitle = {Proceedings of the {November} 7-10, 1966, fall joint computer conference},
	publisher = {Association for Computing Machinery},
	author = {Gentleman, W. M. and Sande, G.},
	month = nov,
	year = {1966},
	pages = {563--578},
}

@incollection{william_arthur_sethares_transforms_2007,
	title = {Transforms},
	isbn = {978-1-84628-639-1},
	url = {https://sethares.engr.wisc.edu/vocoders/Transforms.pdf},
	booktitle = {Rhythm and {Transforms}},
	publisher = {Springer},
	author = {William Arthur Sethares},
	month = jul,
	year = {2007},
	pages = {109--121},
}

@article{heideman_gauss_1985,
	title = {Gauss and the history of the fast {Fourier} transform},
	volume = {34},
	doi = {10.1007/BF00348431},
	abstract = {An investigation into history of Fast Fourier Transform (FFT) algorithm is considered. It deals mostly with work of Carl Friedrick Gauss, an eminent German mathematician who apparently described an algorithm similar to FFT for the computation of the coefficient of a finite Fourier series. Historical reference and evidences of Gausses algorithm are presented. It is also shown that various FFT-type algorithms were used in Great Britain and elsewhere in the nineteenth century, but were unrelated to the work of Gauss and were, in fact, not as general or well-formulated as Gauss' work. Almost one-hundred years passed between the publication of Gauss' algorithm and the modern rediscovery of this approach.},
	journal = {Archive for History of Exact Sciences},
	author = {Heideman, Michael and Johnson, Don and Burrus, Charles},
	month = jan,
	year = {1985},
	pages = {265--277},
}

@article{cooley_algorithm_1965,
	title = {An {Algorithm} for the {Machine} {Calculation} of {Complex} {Fourier} {Series}},
	language = {en},
	author = {Cooley, James W and Tukey, John W},
	year = {1965},
	pages = {5},
}

@article{a_michael_noll_pitch_1969,
	title = {Pitch {Determination} of {Human} {Speech} by the {Harmonic} {Product} {Spectrum}, the {Harmonic} {Sum} {Spectrum}, and a {Maximum} {Likelihood} {Estimate}},
	abstract = {This paper describes three pitch determination methods which were especially designed for noisy speech. The harmonic sum spectrum and harmonic product spectrum developed from intuitive considerations of the harmonic structure of the spectrum of a periodic signal while the maximum likelihood estimate was the result of a purely mathematical treatment of estimating the period of a periodic signal.
The three methods were individually simulated on a digital computer, and the results were compared with the results of a computer simulation of cepstrum pitch determination. All of the new methods performed somewhat better than cepstrum for very noisy signals, but cepstrum gave more "accurate" pitch data for non-noisy speech.},
	journal = {Symposium on Computer Processing in Communications},
	author = {{A. Michael Noll}},
	month = apr,
	year = {1969},
}

@article{bruce_p_bogert_quefrency_1963,
	title = {The quefrency alanysis of time series for echoes : cepstrum, pseudo-autocovariance, cross-cepstrum and saphe cracking},
	volume = {15},
	journal = {Proceedings of the Symposium on Time Series Analysis},
	author = {{Bruce P. Bogert} and {M. J. R. Healy} and {John W. Tukey}},
	month = jun,
	year = {1963},
	pages = {209--243},
}

@misc{microsoft_corporation_multimedia_1991,
	title = {Multimedia {Programming} {Interface} and {Data} {Specifications} 1.0},
	url = {https://www.aelius.com/njh/wavemetatools/doc/riffmci.pdf},
	urldate = {2021-04-25},
	author = {{Microsoft Corporation} and {IBM Corporation}},
	month = aug,
	year = {1991},
}

@article{rabiner_comparative_1976,
	title = {A comparative performance study of several pitch detection algorithms},
	volume = {24},
	issn = {0096-3518},
	doi = {10.1109/TASSP.1976.1162846},
	abstract = {A comparative performance study of seven pitch detection algorithms was conducted. A speech data base, consisting of eight utterances spoken by three males, three females, and one child was constructed. Telephone, close talking microphone, and wideband recordings were made of each of the utterances. For each of the utterances in the data base; a "standard" pitch contour was semiautomatically measured using a highly sophisticated interactive pitch detection program. The "standard" pitch contour was then compared with the pitch contour that was obtained from each of the seven programmed pitch detectors. The algorithms used in this study were 1) a center clipping, infinite-peak clipping, modified autocorrelation method (AUTOC), 2) the cepstral method (CEP), 3) the simplified inverse filtering technique (SIFT) method, 4) the parallel processing time-domain method (PPROC), 5) the data reduction method (DARD), 6) a spectral flattening linear predictive coding (LPC) method, and 7) the average magnitude difference function (AMDF) method. A set of measurements was made on the pitch contours to quantify the various types of errors which occur in each of the above methods. Included among the error measurements were the average and standard deviation of the error in pitch period during voiced regions, the number of gross errors in the pitch period, and the average number of voiced-unvoiced classification errors. For each of the error measurements, the individual pitch detectors could be rank ordered as a measure of their relative performance as a function of recording condition, and pitch range of the various speakers. Performance scores are presented for each of the seven pitch detectors based on each of the categories of error.},
	number = {5},
	journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	author = {Rabiner, L. and Cheng, M. and Rosenberg, A. and McGonegal, C.},
	month = oct,
	year = {1976},
	note = {Conference Name: IEEE Transactions on Acoustics, Speech, and Signal Processing},
	keywords = {Autocorrelation, Detection algorithms, Detectors, Filtering algorithms, Linear predictive coding, Measurement standards, Microphones, Speech, Telephony, Wideband},
	pages = {399--418},
}

@article{chakraborty_pitch_2008,
	title = {Pitch {Tracking} of {Acoustic} {Signals} based on {Average} {Squared} {Mean} {Difference} {Function}},
	url = {http://arxiv.org/abs/cs/0701177},
	abstract = {In this paper, a method of pitch tracking based on variance minimization of locally periodic subsamples of an acoustic signal is presented. Replicates along the length of the periodically sampled data of the signal vector are taken and locally averaged sample variances are minimized to estimate the fundamental frequency. Using this method, pitch tracking of any text independent voiced signal is possible for different speakers.},
	urldate = {2021-04-25},
	journal = {arXiv:cs/0701177},
	author = {Chakraborty, Roudra and Sengupta, Debapriya and Sinha, Sagnik},
	month = aug,
	year = {2008},
	note = {arXiv: cs/0701177},
	keywords = {Computer Science - Sound},
}

@misc{noauthor_autocorrelation_nodate,
	title = {Autocorrelation},
	url = {https://www.ee.columbia.edu/~dpwe/classes/e6820-2001-01/matlab/MAD/auto/auto.htm},
	urldate = {2021-04-25},
}

@article{miller_measurement_1956,
	title = {Measurement of the {Fundamental} {Period} of {Speech} {Using} a {Delay} {Line}},
	volume = {28},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.1905017},
	doi = {10.1121/1.1905017},
	language = {en},
	number = {4},
	urldate = {2021-04-25},
	journal = {The Journal of the Acoustical Society of America},
	author = {Miller, R. L. and Weibel, E. S.},
	month = jul,
	year = {1956},
	pages = {761--761},
}

@inproceedings{cuadra_efficient_2001,
	title = {Efficient pitch detection techniques for interactive music},
	abstract = {Several pitch detection algorithms are examined for use in interactive computer-music performance. We define criteria necessary for successful pitch tracking in real-time and survey},
	booktitle = {In {Proceedings} of the 2001 {International} {Computer} {Music} {Conference}, {La} {Habana}},
	author = {Cuadra, Patricio De La and Master, Aaron},
	year = {2001},
}

@article{portnoff_implementation_1976,
	title = {Implementation of the digital phase vocoder using the fast {Fourier} transform},
	volume = {24},
	issn = {0096-3518},
	url = {http://ieeexplore.ieee.org/document/1162810/},
	doi = {10.1109/TASSP.1976.1162810},
	abstract = {This paper discusses a digital formulation of the phase vocoder, an analysis-synthesis system providing a parametric representation of a speech waveform by its short-time Fourier transform. Such a system is of interest both for data-rate reduction and for manipulating basic speech parameters. The system is designed to be an identity system in the absenceofanyparametermodifications.Computational efficiency is achieved by employing the fast Fourier transform (FFT) algorithm to perform the bulk of the computation in both the analysis and synthesis procedures, therebymaking the formulation attractive for implementation on a minicomputer.},
	language = {en},
	number = {3},
	urldate = {2021-04-25},
	journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	author = {Portnoff, M.},
	month = jun,
	year = {1976},
	pages = {243--248},
}

@article{driedger_review_2016,
	title = {A {Review} of {Time}-{Scale} {Modification} of {Music} {Signals}},
	abstract = {Time-scale modiï¬cation (TSM) is the task of speeding up or slowing down an audio signalâs playback speed without changing its pitch. In digital music production, TSM has become an indispensable tool, which is nowadays integrated in a wide range of music production software. Music signals are diverseâthey comprise harmonic, percussive, and transient components, among others. Because of this wide range of acoustic and musical characteristics, there is no single TSM method that can cope with all kinds of audio signals equally well. Our main objective is to foster a better understanding of the capabilities and limitations of TSM procedures. To this end, we review fundamental TSM methods, discuss typical challenges, and indicate potential solutions that combine different strategies. In particular, we discuss a fusion approach that involves recent techniques for harmonic-percussive separation along with time-domain and frequency-domain TSM procedures.},
	language = {en},
	author = {Driedger, Jonathan and MÃ¼ller, Meinard},
	year = {2016},
	pages = {26},
}

@article{andre_harmonic_nodate,
	title = {Harmonic {Product} {Spectrum} revisited and adapted for rotating machine monitoring based on {IAS}.},
	abstract = {A few years ago, Instantaneous Angular Speed (IAS) signal analysis has been proven able to detect natural bearing faults. This major experimental demonstration shows that mechanical faults can be detected through the reading of the torsional shaft movement rather than the transverse vibration of the equipment housing. Amongst the different techniques that can be used to get IAS signal, Elapse Time seems to bring the best results and is under consideration in this paper. However, since the lack of advance processing tool limits the development of this technology, this paper proposes an alternative technique to extract dry impacts components from IAS spectrum and therefore, greatly enhance its capacity to detect bearing fault. The efï¬ciency of the proposed technique is shown on real measurements issued from a wind turbine main bearing fault, and compared to classical IAS analysis tools.},
	language = {en},
	author = {Andre, Hugo and Khelf, Ilyes and Leclere, Quentin},
	pages = {8},
}

@misc{noauthor_cepstral_nodate,
	title = {Cepstral analysis},
	url = {http://www.phon.ox.ac.uk/jcoleman/new_SLP/Lecture_7/Cepstral_analysis.html},
	urldate = {2021-04-25},
}

@article{nercessian_lightweight_2021,
	title = {Lightweight and interpretable neural modeling of an audio distortion effect using hyperconditioned differentiable biquads},
	url = {http://arxiv.org/abs/2103.08709},
	abstract = {In this work, we propose using differentiable cascaded biquads to model an audio distortion effect. We extend trainable infinite impulse response (IIR) filters to the hyperconditioned case, in which a transformation is learned to directly map external parameters of the distortion effect to its internal filter and gain parameters, along with activations necessary to ensure filter stability. We propose a novel, efficient training scheme of IIR filters by means of a Fourier transform. Our models have significantly fewer parameters and reduced complexity relative to more traditional black-box neural audio effect modeling methodologies using finite impulse response filters. Our smallest, best-performing model adequately models a BOSS MT-2 pedal at 44.1 kHz, using a total of 40 biquads and only 210 parameters. Its model parameters are interpretable, can be related back to the original analog audio circuit, and can even be intuitively altered by machine learning non-specialists after model training. Quantitative and qualitative results illustrate the effectiveness of the proposed method.},
	urldate = {2021-04-25},
	journal = {arXiv:2103.08709 [cs, eess]},
	author = {Nercessian, Shahan and Sarroff, Andy and Werner, Kurt James},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.08709},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Electrical Engineering and Systems Science - Signal Processing},
}

@misc{roche_basic_nodate,
	title = {Basic {Audio} {EQs}},
	url = {http://blog.bjornroche.com/2012/08/basic-audio-eqs.html},
	abstract = {In my last post , I looked at why it's usually better to do EQ (or filtering) in the time domain than the frequency domain as far as audio i...},
	urldate = {2021-04-25},
	author = {Roche, Bjorn},
}

@misc{costello_dsp_2009,
	title = {{DSP} {Hero}: {Robert} {Bristow}-{Johnson}},
	shorttitle = {{DSP} {Hero}},
	url = {https://valhalladsp.com/2009/11/05/dsp-hero-robert-bristow-johnson/},
	abstract = {Super nerdy DSP content warning. If you program audio DSP effects, you have undoubtedly heard of Robert Bristow-Johnson. If you haven't heard of him, get},
	language = {en-US},
	urldate = {2021-04-25},
	journal = {Valhalla DSP},
	author = {Costello, Sean},
	month = nov,
	year = {2009},
}

@book{fourier_theorie_1822,
	address = {Paris},
	title = {ThÃ©orie analytique de la chaleur},
	language = {French},
	publisher = {F. Didot},
	author = {Fourier, Jean-Baptiste-Joseph},
	year = {1822},
	note = {OCLC: 2688081},
}

@article{dominguez_highlights_2016,
	title = {Highlights in the {History} of the {Fourier} {Transform} [{Retrospectroscope}]},
	volume = {7},
	issn = {2154-2317},
	doi = {10.1109/MPUL.2015.2498500},
	abstract = {Discusses the creation of the Fourier Transform and presents an historical look at its creator, Jean Baptiste Joseph Fourier.},
	number = {1},
	journal = {IEEE Pulse},
	author = {DomÃ­nguez, Alejandro},
	month = jan,
	year = {2016},
	note = {Conference Name: IEEE Pulse},
	keywords = {Biographies, Eigenvalues and eigenfunctions, Fourier series, Fourier, Jean Baptiste Joseph, History, Transforms},
	pages = {53--61},
}

@misc{noauthor_python_nodate-1,
	title = {Python, {Pitch} shifting, and the {Pianoputer} - \_\_del\_\_( self )},
	url = {http://zulko.github.io/blog/2014/03/29/soundstretching-and-pitch-shifting-in-python/},
	urldate = {2021-04-25},
}

@article{laroche_improved_1999,
	title = {Improved phase vocoder time-scale modification of audio},
	doi = {10.1109/89.759041},
	abstract = {The phase vocoder is a well established tool for time scaling and pitch shifting speech and audio signals via modification of their short-time Fourier transforms (STFTs). In contrast to time-domain time-scaling and pitch-shifting techniques, the phase vocoder is generally considered to yield high quality results, especially for large modification factors and/or polyphonic signals. However, the phase vocoder is also known for introducing a characteristic perceptual artifact, often described as "phasiness", "reverberation", or "loss of presence". This paper examines the problem of phasiness in the context of time-scale modification and provides new insights into its causes. Two extensions to the standard phase vocoder algorithm are introduced, and the resulting sound quality is shown to be significantly improved. Moreover, the modified phase vocoder is shown to provide a factor-of-two decrease in computational cost.},
	journal = {IEEE Trans. Speech Audio Process.},
	author = {Laroche, J. and Dolson, M.},
	year = {1999},
}

@inproceedings{laroche_phase-vocoder_1997,
	address = {New Paltz, NY, USA},
	title = {Phase-vocoder: about this phasiness business},
	isbn = {978-0-7803-3908-8},
	shorttitle = {Phase-vocoder},
	url = {http://ieeexplore.ieee.org/document/625603/},
	doi = {10.1109/ASPAA.1997.625603},
	abstract = {The phase-vocoder is a well-known tool for the frequency domain processing of speech or audio signals, with applications such as time compression or expansion, pitch-scale modiï¬cation, noise reduction, etc. In the context of time-scale or pitch-scale modiï¬cation, the phase-vocoder is usually considered to yield high quality results, especially when large modiï¬cation factors are used on polyphonic or non-pitched signals. However, the phase-vocoder is also known for an artifact that plagues its output, and has been described in the literature as either âphasinessâ, âreverberationâ, or âloss of presenceâ. Recent research has been devoted to understanding and reducing this artifact, and solutions have been proposed which either signiï¬cantly improve the quality of the output at the cost of a very high additional computation time, or are inexpensive but only marginally eï¬ective. This paper examines the problem of phasiness in the context of time-scale modiï¬cation of signals, and presents two new phase synchronization schemes which are shown to both signiï¬cantly improve the sound quality, and reduce the computational cost of such modiï¬cations.},
	language = {en},
	urldate = {2021-04-25},
	booktitle = {Proceedings of 1997 {Workshop} on {Applications} of {Signal} {Processing} to {Audio} and {Acoustics}},
	publisher = {IEEE},
	author = {Laroche, J. and Dolson, M.},
	year = {1997},
	pages = {4},
}

@article{roebel_new_2003,
	title = {A new approach to transient processing in the phase vocoder},
	abstract = {In this paper we propose a new method to reduce phase vocoder artifacts during attack transients. In contrast to all transient preservation algorithms that have been proposed up to now the new approach does not impose any constraints on the time dilation parameter for processing transient segments. By means of an investigation into the spectral properties of attack transients of simple sinusoids we provide new insights into the causes of phase vocoder artifacts and propose a new method for transient preservation as well as a new criterion and a new algorithm for transient detection. Both, the transient detection and the transient processing algorithms are designed to operate on the level of spectral bins which reduces possible artifacts in stationary signal components that are close to the spectral peaks classiï¬ed as transient. The transient detection criterion has a close relation to the transient position and allows us to ï¬nd an optimal position for reinitializing the phase spectrum. The evaluation of the transient detector by means of a hand labeled data base demonstrates its superior performance compared to a previously published algorithm. Attack transients in sound signals transformed with the new algorithm achieves high quality even if strong dilation is applied to polyphonic signals.},
	language = {en},
	author = {Roebel, Axel},
	year = {2003},
	pages = {6},
}

@misc{noauthor_pitch_nodate,
	title = {Pitch {Shifting} {Using} {The} {Fourier} {Transform} {\textbar} {Stephan} {Bernsee}'s {Blog}},
	url = {http://blogs.zynaptiq.com/bernsee/pitch-shifting-using-the-ft/},
	language = {en-US},
	urldate = {2021-04-25},
}

@article{flanagan_phase_1966,
	title = {Phase vocoder},
	volume = {45},
	issn = {0005-8580},
	doi = {10.1002/j.1538-7305.1966.tb01706.x},
	abstract = {A vocoder technique is described in which speech signals are represented by their short-time phase and amplitude spectra. A complete transmission system utilizing this approach is simulated on a digital computer. The encoding method leads to an economy in transmission bandwidth and to a means for time compression and expansion of speech signals.},
	number = {9},
	journal = {The Bell System Technical Journal},
	author = {Flanagan, J. L. and Golden, R. M.},
	month = nov,
	year = {1966},
	note = {Conference Name: The Bell System Technical Journal},
	pages = {1493--1509},
}

@inproceedings{petkov_improving_2007,
	title = {Improving the phase vocoder approach to pitch-shifting},
	abstract = {A class of methods known as phase vocoders allows for implementing pitch shifting in the spectral domain. We extend the approach of shifting the isolated harmonies of the spectrum by introducing a ...},
	booktitle = {{INTERSPEECH}},
	author = {Petkov, Petko N. and Kleijn, W.},
	year = {2007},
}

@misc{noauthor_spitfire_nodate,
	title = {Spitfire {Audio} â {Sample} {Libraries} and {Virtual} {Instruments}},
	url = {https://www.spitfireaudio.com/},
	urldate = {2021-04-25},
}

@misc{noauthor_end_nodate,
	title = {End {User} {License} {Agreement} for {Spitfire} {Audio}},
	url = {https://www.spitfireaudio.com/info/eula/},
	urldate = {2021-04-25},
}

@article{dolson_phase_1986,
	title = {The {Phase} {Vocoder}: {A} {Tutorial}},
	volume = {10},
	issn = {01489267},
	shorttitle = {The {Phase} {Vocoder}},
	url = {https://www.jstor.org/stable/3680093?origin=crossref},
	doi = {10.2307/3680093},
	language = {en},
	number = {4},
	urldate = {2021-04-25},
	journal = {Computer Music Journal},
	author = {Dolson, Mark},
	year = {1986},
	pages = {14},
}

@misc{noauthor_phase_nodate,
	title = {phase vocoder in {Matlab}},
	url = {https://sethares.engr.wisc.edu/vocoders/phasevocoder.html},
	urldate = {2021-04-25},
}

@inproceedings{rai_analysis_2019,
	title = {Analysis of three pitch-shifting algorithms for different musical instruments},
	doi = {10.1109/LISAT.2019.8817334},
	abstract = {Pitch-shifting is a process where the original pitch of the sound is increased or decreased without affecting the length of the sound clip being recorded. Pitch shifters are being embedded in most of the audio processors. They are used to generate desired audio effects while recording sounds and music. Some of the notable uses of pitch shifters are in cartoons and entertainment industry for producing very distinct and unique voice, electronic music to generate melodies, metallic music to generate unnatural sound effects, and so on. However, one of the major problems of the pitch shifting algorithms is that there is not a sole algorithm, which can deal efficiently with all types of music and sounds. Here, we are evaluating the performance of three different pitch-shifting algorithms: pitch shifting via phase vocoder using phase locking, PitchshiftOcean, and pitch shifting via TSM using WSOLA for five musical instruments including guitar, piano, violin, drum, and flute. The objective of this work is to define the changes in some of the spectral and time-domain characteristics of musical instruments because of the pitch-shifting process.},
	booktitle = {2019 {IEEE} {Long} {Island} {Systems}, {Applications} and {Technology} {Conference} ({LISAT})},
	author = {Rai, Anil and Barkana, Buket D},
	month = may,
	year = {2019},
	note = {ISSN: 2642-8873},
	keywords = {Phase-Vocoder, Short-time Fourier transform (STFT), Time scale modification (TSM), analysis window, hop size, phase vocoder, pitch, synthesis window},
	pages = {1--6},
}

@misc{noauthor_note_nodate,
	title = {Note names, {MIDI} numbers and frequencies},
	url = {https://newt.phys.unsw.edu.au/jw/notes.html},
	urldate = {2021-04-25},
}

@misc{noauthor_pitch_nodate-1,
	title = {Pitch {Detection} {Algorithms}},
	url = {https://cnx.org/contents/i5AAkZCP@2/Pitch-Detection-Algorithms},
	urldate = {2021-04-25},
}

@misc{guzman_fast_nodate,
	title = {Fast and {Efficient} {Pitch} {Detection}},
	url = {https://www.cycfi.com/2017/10/fast-and-efficient-pitch-detection/},
	abstract = {I needed to implement real-time, multichannel pitch detection in software using a small ARM Cortex-M4 microcontroller (MCU). My all-time favorite is the STM32F4 family from STMicroelectronics. It hasÂ DSP and single precision FPU instructions andÂ can reachÂ up to 225 DMIPS/608 CoreMark at up to 180},
	language = {en-US},
	urldate = {2021-04-25},
	journal = {Cycfi Research},
	author = {Guzman, Joel de},
}

@article{noll_shorttime_1964,
	title = {Shortâ{Time} {Spectrum} and â{Cepstrum}â {Techniques} for {Vocal}â{Pitch} {Detection}},
	volume = {36},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.1918949},
	doi = {10.1121/1.1918949},
	language = {en},
	number = {2},
	urldate = {2021-04-25},
	journal = {The Journal of the Acoustical Society of America},
	author = {Noll, A. Michael},
	month = feb,
	year = {1964},
	pages = {296--302},
}

@misc{noauthor_short_nodate,
	title = {A {Short} {Tutorial} on {Cepstral} {Analysis} for {Pitch}-tracking {\textbar} {Frolian}'s blog},
	url = {http://flothesof.github.io/cepstrum-pitch-tracking.html},
	urldate = {2021-04-25},
}

@misc{noauthor_fft_nodate,
	title = {{FFT} {Implementation} of the {Phase} {Vocoder}},
	url = {https://ccrma.stanford.edu/~jos/sasp/FFT_Implementation_Phase_Vocoder.html},
	urldate = {2021-04-25},
}

@misc{noauthor_short-time_nodate,
	title = {The {Short}-{Time} {Fourier} {Transform}},
	url = {https://ccrma.stanford.edu/~jos/sasp/Short_Time_Fourier_Transform.html},
	urldate = {2021-04-25},
}

@misc{noauthor_phase_nodate-1,
	title = {Phase {Vocoder}},
	url = {https://ccrma.stanford.edu/~jos/sasp/Phase_Vocoder.html},
	urldate = {2021-04-25},
}

@misc{noauthor_mathematical_nodate-1,
	title = {The {Mathematical} {Genius} of {Auto}-{Tune}},
	url = {http://priceonomics.com/the-inventor-of-auto-tune/},
	abstract = {Auto-Tune â one of modern historyâs most reviled inventions â was an act of mathematical genius.},
	language = {en},
	urldate = {2021-04-25},
	journal = {Priceonomics},
}

@misc{noauthor_jens_nodate,
	title = {â«{Jens} {Johansson} Â· {The} {Phase} {Vocoder}: {A} {Tutorial}},
	url = {http://www.panix.com/~jens/pvoc-dolson.par},
	urldate = {2021-04-25},
}

@book{goodman_geometric_2020,
	title = {A {Geometric} {Framework} for {Pitch} {Estimation} on {Acoustic} {Musical} {Signals} - {A} {Preprint}},
	abstract = {This paper presents a geometric approach to pitch estimation (PE)-an important problem in Music Information Retrieval (MIR), and a precursor to a variety of other problems in the field. Though there exist a number of highly-accurate methods, both mono-pitch estimation and multi-pitch estimation (particularly with unspecified polyphonic timbre) prove computationally and conceptually challenging. A number of current techniques, whilst incredibly effective, are not targeted towards eliciting the underlying mathematical structures that underpin the complex musical patterns exhibited by acoustic musical signals. Tackling the approach from both a theoretical and experimental perspective, we present a novel framework, a basis for further work in the area, and results that (whilst not state of the art) demonstrate relative efficacy. The framework presented in this paper opens up a completely new way to tackle PE problems, and may have uses both in traditional analytical approaches, as well as in the emerging machine learning (ML) methods that currently dominate the literature.},
	author = {Goodman, Tom and van Gemst, Karoline and Tino, Peter},
	month = nov,
	year = {2020},
	doi = {10.13140/RG.2.2.22366.05444},
}

@misc{noauthor_pitch_nodate-2,
	title = {Pitch {Detection} {Methods}},
	url = {https://sound.eti.pg.gda.pl/student/eim/synteza/leszczyna/index_ang.htm},
	urldate = {2021-04-25},
}

@misc{noauthor_guitar_nodate,
	title = {Guitar {Pitch} {Shifter} - {Introduction}},
	url = {http://www.guitarpitchshifter.com/},
	urldate = {2021-04-25},
}

@misc{noauthor_time_nodate,
	title = {Time {Stretching} {And} {Pitch} {Shifting} of {Audio} {Signals} â {An} {Overview} {\textbar} {Stephan} {Bernsee}'s {Blog}},
	url = {http://blogs.zynaptiq.com/bernsee/time-pitch-overview/},
	language = {en-US},
	urldate = {2021-04-25},
}

@inproceedings{laroche_new_1999,
	address = {New Paltz, NY, USA},
	title = {New phase-vocoder techniques for pitch-shifting, harmonizing and other exotic effects},
	isbn = {978-0-7803-5612-2},
	url = {http://ieeexplore.ieee.org/document/810857/},
	doi = {10.1109/ASPAA.1999.810857},
	abstract = {The phase-vocoder is usually presented as a high-quality solution for time-scale modiï¬cation of signals, pitch-scale modiï¬cations usually being implemented as a combination of timescaling and sampling rate conversion [1]. In this paper, we present two new phase-vocoder-based techniques which allow direct manipulation of the signal in the frequency-domain, enabling such applications as pitch-shifting, chorusing, harmonizing, partial stretching and other exotic modiï¬cations which cannot be achieved by the standard time-scale samplingrate conversion scheme. The new techniques are based on a very simple peak-detection stage, followed by a peak-shifting stage. The very simplest one allows for 50\% overlap but restricts the precision of the modiï¬cations, while the most ï¬exible techniques requires a more expensive 75\% overlap.},
	language = {en},
	urldate = {2020-12-01},
	booktitle = {Proceedings of the 1999 {IEEE} {Workshop} on {Applications} of {Signal} {Processing} to {Audio} and {Acoustics}. {WASPAA}'99 ({Cat}. {No}.{99TH8452})},
	publisher = {IEEE},
	author = {Laroche, J. and Dolson, M.},
	year = {1999},
	pages = {91--94},
}

@inproceedings{goodman_real-time_2018,
	address = {Louisville, KY, USA},
	title = {Real-{Time} {Polyphonic} {Pitch} {Detection} on {Acoustic} {Musical} {Signals}},
	isbn = {978-1-5386-7568-7},
	url = {https://ieeexplore.ieee.org/document/8642626/},
	doi = {10.1109/ISSPIT.2018.8642626},
	abstract = {This paper presents an algorithm for fundamental frequency detection on polyphonic acoustic musical signals, based on a new ârakingâ method over the frequency-domain spectra. The algorithm is evaluated as a classiï¬er, and boasts a good accuracy (83.20\%) compared to other such methods, as well as the ability to function effectively in real-time, with a runningspeed below 140ms per window evaluated. This proves to be real-time for the use-case, as the latency between an auditory stimulus and its perception by a person has been shown to be longer than this. The algorithm itself runs in linear-time, but is thus slowed by the O(nlog(n)) Fast Fourier Transform during preprocessing. Though the algorithm fails to account for certain edge-cases with overlapping harmonics as well as certain instruments, future work and improvements are also presented, paving the way for further research.},
	language = {en},
	urldate = {2020-10-22},
	booktitle = {2018 {IEEE} {International} {Symposium} on {Signal} {Processing} and {Information} {Technology} ({ISSPIT})},
	publisher = {IEEE},
	author = {Goodman, Thomas A. and Batten, Ian},
	month = dec,
	year = {2018},
	pages = {1--6},
}
